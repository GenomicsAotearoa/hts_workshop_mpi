{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"![image](./theme_images/mpi_logo.png){width=\"400\"} # High Throughput Sequencing <p>Level 1 - Beginner</p> <p>Introduction to NeSI</p> <ol> <li>Connecting to NeSI</li> <li>Navigation on the command line</li> <li>Working with files on the command line</li> </ol> <p>Quality filtering Illumina data</p> <ol> <li>Inspecting reads</li> <li>Trimming paired-end reads</li> </ol> <p>Quality filtering Nanopore data</p> <ol> <li>Inspecting reads</li> <li>Trimming long-read Nanopore data</li> </ol> <p>Annotating sequences with BLAST</p> <ol> <li>Submitting a BLAST job to NeSI</li> <li>Introduction to slurm</li> <li>Interpreting BLAST outputs</li> </ol> <p>Level 2 - Advanced</p> <p>Working in the shell</p> <ol> <li>Working with files</li> <li>Variables and loops</li> <li>Redirection</li> <li>Understanding the NeSI environment</li> </ol> <p>De novo assembly of sequencing data</p> <ol> <li>Overview</li> <li>Short read assembly with SPAdes</li> <li>Long read assembly with Flye</li> <li>Assessing the quality of a genome</li> <li>Polishing of genome assemblies</li> </ol> <p>Mapping reads to a reference</p> <ol> <li>Overview</li> <li>Illumina mapping with bowtie2</li> <li>Nanopore mapping with minimap2</li> <li>Filtering and sorting mapping files</li> <li>Summarising mapping statistics</li> </ol> <p>Performing gene prediction and classification</p> <ol> <li>Overview</li> <li>Prediction with <code>prodigal</code></li> <li>Prediction with <code>AUGUSTUS</code></li> <li>Protein annotation with <code>BLAST</code>-like tools</li> <li>Classification with <code>kraken2</code></li> </ol> <p>Proficiency testing</p> <ol> <li>Primary test materials (2023)</li> <li>Alternate test materials (2023)</li> </ol> <p>Data used in training</p> <p>This workshop provides a basic introduction to working with the slurm scheduling system, and begins working with Illumina MiSeq and Oxford Nanopore Technology sequence data. The data used in this workshop is mostly using simulated reads, produced using InSilicoSeq[^2] from the Mycoplasma bovis 8790 reference genome NZ_LAUS01000004.1. We also make use of publicly available sequencing data from the studies PRJNA813586, PRJEB38441, and PRJEB38523.</p> <p>Additional teaching materials were sourced from:</p> <ul> <li>Genomics Aoteoroa Metagenomic Summer School workshop2.</li> <li>Long-Read, long reach Bioinformatics Tutorial3.</li> <li>Galaxy Training! sequence analysis resources4.</li> </ul> <p>Citations</p> <ol> <li> <p>Erin Alison Becker, Anita Sch\u00fcrch, Tracy Teal, Sheldon John McKay, Jessica Elizabeth Mizzi, Fran\u00e7ois Michonneau, et al. (2019, June). datacarpentry/shell-genomics: Data Carpentry: Introduction to the shell for genomics data, June 2019 (Version v2019.06.1). Zenodo. http://doi.org/10.5281/zenodo.3260560.</p> </li> <li> <p>Hadrien Gourl\u00e9, Oskar Karlsson-Lindsj\u00f6, Juliette Hayer, Erik Bongcam-Rudloff (2019). Simulating Illumina metagenomic data with InSilicoSeq. Bioinformatics 35(3), 521-522.</p> </li> <li> <p>Jian Sheng Boey, Dinindu Senanayake, Michael Hoggard et al. (2022). Metagenomics Summer School https://github.com/GenomicsAotearoa/metagenomics_summer_school.</p> </li> <li> <p>Tim Kahlke (2021). Long-Read Data Analysis https://timkahlke.github.io/LongRead_tutorials/.</p> </li> <li> <p>Joachim Wolff, B\u00e9r\u00e9nice Batut, Helena Rasche (2023). Sequence Analysis (revision 96e0180). https://training.galaxyproject.org/training-material/topics/sequence-analysis/.</p> </li> </ol>"},{"location":"level1/11_nesi_connection/","title":"1.1 - Connecting to NeSI","text":""},{"location":"level1/11_nesi_connection/#11-connecting-to-nesi","title":"1.1 - Connecting to NeSI","text":""},{"location":"level1/11_nesi_connection/#overview","title":"Overview","text":"<p>time</p> <ul> <li>Teaching: 10 minutes</li> <li>Exercises: 5 minutes</li> </ul> <p>Learning objectives</p> <p>Objectives</p> <ul> <li>Understand how to connect to NeSI using the OnDemand web portal.</li> </ul> <p>Key points</p> <ul> <li>You can connect to NeSI from any computer with internet access using the OnDemand portal.</li> <li>This allocates a small server instance from which you can perform simple operations, or prepare larger analysis scripts for deployment.</li> </ul>"},{"location":"level1/11_nesi_connection/#how-to-access-nesi","title":"How to access NeSI","text":"<p>To access the shell on your own computer requires platform-specific software. For all work in this training we will be connecting to the NeSI platform and using their web-based shell environment. We will access this through your web browser, by navigating to NeSI's OnDemand portal. </p> <p>When you arrive at this website, you will be prompted to provide your access details.</p> <p>You need to provide your specific training account user name and password. These have already been emailed to you.</p> <p>Once these details are submitted, you will be redirected to a loading page. Scroll through the training options until you find the button for the MPI training environment.</p> <p>MPI Training</p> <p></p><p></p> <p>Once you have selected this option you will be asked to launch a server for the workshop. The minimum number of hours you can set is 4, so just use this and click the 'Launch' button.</p> <p>Launching your NeSI session.</p> <p></p><p></p> <p>It may take a moment for the server to come online. You will know that it is ready when the interface displays the 'Connect to MPI Shell app' option:</p> <p>After logging in, you will see a screen showing something like this: </p> <p>Loading a shell environment.</p> <p></p> <p>Clicking on this button will transfer you through to the following view:</p> <p>The landing page.</p> <p></p> <p>For now, we will just launch a basic terminal environment, so click on the Terminal icon. This will lauch the command line interface that we will be using for the remainder of this workshop.</p> <p>Loading a terminal.</p> <p></p> <p>We will spend most of our time learning about the basics of the shell by manipulating some experimental data. Some of the data we're going to be working with is quite large, so a copy of the data has been placed in your working directory for this workshop. We will now begin to explore the working directory.</p>"},{"location":"level1/11_nesi_connection/#uses-of-ondemand","title":"Uses of OnDemand","text":"<p>Using the OnDemand portal has two main functions. Primarily, this is a simple and easy to follow method for connecting to the NeSI HPC cluster without using specialised software. As long as you have a supported web browser you will be able to connect to NeSI from any location with internet access.</p> <p>The portal also provides you with a dedicated slice of NeSI resources so you can run small and medium sized commands directly on this terminal without worrying about the NeSI usage policy (which is a consideration if connecting directly). When you log in you requested the time you want these resources for and these are allocated directly to you and cannot be touched by other users. Commands you run here take place in real time and as long as they complete before your time allocation expires there are no limitations to what you can do.</p> <p>If you are planning to run a job that takes significantly longer than requested, the OnDemand portal still acts as a good staging ground to log into NeSI, set up your <code>slurm</code> batch file and then submit the job. This is covered in different tutorials, but in these cases it is better to request a minimal number of cores and hours as all you really need to do is set up some directories, and write a few text files.</p>"},{"location":"level1/11_nesi_connection/#what-is-a-shell-and-why-should-i-care","title":"What is a shell and why should I care?","text":"<p>A shell is a computer program that presents a command line interface which allows you to control your computer using commands entered with a keyboard instead of controlling graphical user interfaces (GUIs) with a mouse/keyboard combination.</p> <p>Command line interfaces are very common when working in bioinformatics, as creating a GUI is platform-dependant. To create a tool with a GUI that works on Windows, OS X, and Linux requires at least three different versions of the tool to be written. In contrast, the command line is common to all three platforms.</p> <p>Apart from this practical issue, there are many reasons to learn about the shell:</p> <p>The shell makes your work less boring</p> <p>In bioinformatics you often need to do the same set of tasks with a large number of files.</p> <p>Learning the shell will allow you to automate those repetitive tasks and leave you free to do more exciting things.</p> <p>The shell makes your work less error-prone</p> <p>When humans do the same thing a hundred different times (or even ten times), they're likely to make a mistake.</p> <p>Your computer can do the same thing a thousand times with no mistakes.</p> <p>The shell makes your work more reproducible</p> <p>When you carry out your work in the command-line your computer keeps a record of every step that you've carried out, which you can use to re-do your work when you need to.</p> <p>It also gives you a way to communicate unambiguously what you've done, so that others can check your work or apply your process to new data.</p> <p>Many bioinformatic tasks require large amounts of computing power and can't realistically be run on your own machine</p> <p>These tasks are best performed using remote computers or cloud computing.</p> <p>These devices require a remote connection which is most stably provided through a command line interface.</p> <p>In the following lessons you will learn how to use the command line interface to move around in your file system.</p>"},{"location":"level1/12_shell_navigation/","title":"1.2 - Introducing the shell","text":""},{"location":"level1/12_shell_navigation/#12-introducing-the-shell","title":"1.2 - Introducing the shell","text":""},{"location":"level1/12_shell_navigation/#overview","title":"Overview","text":"<p>time</p> <ul> <li>Teaching: 15 minutes</li> <li>Exercises: 20 minutes</li> </ul> <p>Learning objectives</p> <p>Objectives</p> <ul> <li>Understand how to navigate your file system using the command line.</li> <li>Perform basic file operations using the command line.</li> <li>Demonstrate the use of tab completion, and understand its advantages.</li> </ul> <p>Key points </p> <ul> <li>The shell gives you the ability to work more efficiently by using keyboard commands rather than a GUI.</li> <li>Useful commands for navigating your file system include: <code>ls</code>, <code>pwd</code>, and <code>cd</code>.</li> <li>Tab completion can reduce errors from mistyping and make work more efficient in the shell.</li> </ul>"},{"location":"level1/12_shell_navigation/#navigating-your-file-system","title":"Navigating your file system","text":"<p>The part of the operating system responsible for managing files and directories is called the file system. It organizes our data into files, which hold information, and directories (also called \"folders\"), which hold files or other directories. This is exactly the same as what you will be used to using the <code>File Explorer</code> on your home and work computers, except that we do not have visual prompts to tell us where we are in the file system.</p> <p>To the left hand side of your terminal cursor is a dollar sign character (<code>$</code>). The dollar sign is a prompt, which shows us that the shell is waiting for input; your shell may use a different character as a prompt and may add information before the prompt.</p> <p>Let's find out where we are by running a command called <code>pwd</code> (which stands for \"print working directory\"). At any moment, our current working directory is our default directory, i.e. the directory that the computer assumes we want to run commands in, unless we explicitly specify something else.</p> <p>code</p> <pre><code>pwd\n</code></pre> Output <pre><code>/home/shared/&lt;username&gt;\n</code></pre> <p>This is your home directory. It is private to you, and has limited file storage space. When working on NeSI properly we typically want to leave our home directory and navigate to a shared project directory. However, for the training today we will be working from this location.</p> <p>Let's now look at how our file system is organised. We can see what files and subdirectories are in this directory by running <code>ls</code>, which stands for \"listing\":</p> <p>code</p> <pre><code>ls\n</code></pre> Output <pre><code>level1  ondemand\n</code></pre> <p><code>ls</code> prints the names of the files and directories in the current directory in alphabetical order, arranged neatly into columns. We can navigate into one of these directories using the <code>cd</code> command.</p> <p>Once inside, use <code>ls</code> command one final time to view the contents.</p> <p>code</p> <pre><code>cd level1/\nls\n</code></pre> Output <pre><code>blast_annotation  quality_illumina  quality_nanopore  shell_data\n</code></pre> <p>We will be starting with the <code>shell_data</code> subdirectory. If we want to now navigate into the <code>shell_data</code> folder we must once again call the <code>cd</code> command:</p> <p>code</p> <pre><code>cd shell_data/\n</code></pre> <p>Note</p> <p>It's easy to get lost in a text-based file system. If you ever get stuck and do not know how to get out of your current location, calling either of:</p> <p>code</p> <pre><code>cd ~\n</code></pre> <p>or</p> <p>code</p> <pre><code>cd\n</code></pre> <p>Will return you do your home directory.</p> <p>Let's look at what is in the <code>shell_data</code> directory:</p> <p>code</p> <pre><code>ls\n</code></pre> Output <pre><code>SRR097977.fastq  SRR098026.fastq\n</code></pre>"},{"location":"level1/12_shell_navigation/#full-versus-relative-paths","title":"Full versus relative paths","text":"<p>As we have previously seen, the <code>cd</code> command takes an directory name which you provide, and moves you to that location on the computer file system. Up until this point, we have been specifying our directory changes one folder at a time, but this is not necessary.</p> <p>Directories can be specified using either a relative path or a full absolute path. The directories on the computer are arranged into a hierarchy. The full path tells you where a directory is in that hierarchy. Dependending on where you currently are in the file system when you enter the <code>pwd</code> command you will see something like:</p> <p>code</p> <pre><code>pwd\n</code></pre> Output <pre><code>/home/shared/&lt;username&gt;/level1/shell_data\n</code></pre> <p>This is the full name of your current directory. Assuming you saw the first output, this tells you that you are in a directory called <code>&lt;username&gt;/</code>, which sits inside a directory called <code>shared/</code>, which in turn sits inside a directory <code>home/</code>. At the very top of the hierarchy is a directory called <code>/</code> which is usually referred to as the root directory.</p> <p>You can navigate to your current location using the following command:</p> <p>code</p> <pre><code>cd /home/shared/&lt;username&gt;/level1/shell_data/\n</code></pre> <p>Now return to your home directory again, and navigate back using the following commands:</p> <p>code</p> <pre><code>cd /\ncd home/\ncd shared/\ncd &lt;username&gt;/\ncd level1/\ncd shell_data/\n</code></pre> <p>These two commands have the same effect and take us to the same location. However, the first uses the absolute path, giving the full address from the top of the file system. The second uses a series of relative paths, with the directory specified in each command contingent on the current working directory.</p> <p>A relative path is like getting directions from someone on the street. They tell you to \"go right at the stop sign, and then turn left onto Queen Street\". That works great if you're standing there together, but not so well if you're trying to tell someone how to get there from another country. A full path is like GPS coordinates. It tells you exactly where something is no matter where you are right now.</p> <p>You can use either a full path or a relative path depending on what is most convenient. If we are in the home directory nd want to move into the training folder it is more convenient to enter the full path. If we are already in the this location but want to move to another nearby folder, it may be more convenient to enter the relative path since it involves less typing.</p> <p>Over time, it will become easier for you to keep a mental note of the structure of the directories that you are using and how to quickly navigate amongst them.</p> <p>A full path always starts with a <code>/</code> (the root directory). A relative path does not. This is a helpful difference to remember so that you always know which type of path you are working with.</p> <p>When working with relative paths, there is one other thing which is critical to know - how to move up out of a directory. This can be achieved using a special path <code>../</code> which means \"move one level higher than the current directory. To see this in action, run the follow commands and note output each time to run <code>pwd</code> and <code>ls</code>.</p> <p>code</p> <pre><code>cd /home/shared/&lt;username&gt;/\npwd\nls\n</code></pre> <pre><code>cd ../\npwd\nls\n</code></pre> <pre><code>cd &lt;username&gt;/\npwd\nls\n</code></pre> <p>Exercise</p> <p>Using the filesystem diagram below, if <code>pwd</code> displays <code>/Users/thing</code>, what would be displayed after entering the following commands?</p> <pre><code>cd ../\nls backup/\n</code></pre> <ol> <li><code>../backup: No such file or directory</code></li> <li><code>2012-12-01 2013-01-08 2013-01-27</code></li> <li><code>2012-12-01/ 2013-01-08/ 2013-01-27/</code></li> <li><code>original pnas_final pnas_sub</code></li> </ol> <p></p> Solution <ol> <li>No: there is a directory <code>backup/</code> in <code>/Users</code>.</li> <li>No: this is the content of <code>Users/thing/backup</code>, but with <code>..</code> we asked for one level further up.</li> <li>No: see previous explanation. Also, we did not specify <code>-F</code> to display <code>/</code> at the end of the directory names.</li> <li>Yes: <code>../backup</code> refers to <code>/Users/backup</code>.</li> </ol>"},{"location":"level1/12_shell_navigation/#navigational-shortcuts","title":"Navigational shortcuts","text":"<p>The root directory is the highest level directory in your file system and contains files that are important for your computer to perform its daily work. While you will be using the root (<code>/</code>) at the beginning of your absolute paths, it is important that you avoid working with data in these higher-level directories, as your commands can permanently alter files that the operating system needs to function.</p> <p>In many cases, including when working on NeSI, trying to run commands in root directories will require special permissions which are not available to you as a regular user.</p> <p>Dealing with the home directory is very common. The tilde character, <code>~</code>, is a shortcut for your home directory. On a Linux operating system either <code>cd</code> or <code>cd ~</code> will take you to your home directory, and <code>cd /</code> will take you to <code>/</code>.</p>"},{"location":"level1/12_shell_navigation/#speeding-up-commands-with-tab-completion","title":"Speeding up commands with tab completion","text":"<p>Typing out file or directory names can waste a lot of time and it's easy to make typing mistakes. Instead we can use tab complete as a shortcut. When you start typing out the name of a directory or file, then hit the Tab key, the shell will try to fill in the rest of the directory or file name.</p> <p>Return to your working directory:</p> <p>code</p> <pre><code>cd /home/shared/&lt;username&gt;/level1/\n</code></pre> <p>Then start a new <code>cd</code> command and provide it with the first few letters of the <code>shell_data/</code> name, then press Tab.</p> <p>code</p> <pre><code>cd she&lt;tab&gt;\n</code></pre> Output <p>The command will automatically expand to the following text:</p> <pre><code>cd shell_data/\n</code></pre> <p>Using tab complete can be very helpful. However, it will only autocomplete a file or directory name if you've typed enough characters to provide a unique identifier for the file or directory you are trying to access.</p> <p>For an example of this in action, move into the <code>shell_data/</code> directory and we will try to repeat this. Try to list the files which names start with <code>SR</code> by using tab complete:  </p> <p>code</p> <pre><code>ls SR&lt;tab&gt;\n</code></pre> Output <pre><code>cd SRR09\n</code></pre> <p>The shell auto-completes your command to <code>SRR09</code>, because all file names in the directory begin with this prefix but it does not have enough information to know exactly which file you are trying to identify. Hitting Tab twice in quick succession will prompt the the shell to list all possible choices.</p> <p>code</p> <pre><code>ls SR&lt;tab&gt;&lt;tab&gt;\n</code></pre> Output <pre><code>SRR097977.fastq  SRR098026.fastq\n</code></pre> <p>Tab completion can also fill in the names of programs, which can be useful if you remember only part  of a program name. For example, if you wished to display the name of every program that starts with <code>pw</code>:</p> <p>code</p> <pre><code>pw&lt;tab&gt;&lt;tab&gt;\n</code></pre> Output <pre><code>pwck      pwconv    pwd       pwdx      pwunconv\n</code></pre>"},{"location":"level1/13_shell_manipulation/","title":"1.3 - Manipulating files in the shell","text":""},{"location":"level1/13_shell_manipulation/#13-manipulating-files-in-the-shell","title":"1.3 - Manipulating files in the shell","text":"<p>time</p> <ul> <li>Teaching: 20 minutes</li> <li>Exercises: 20 minutes</li> </ul> <p>Learning objectives</p> <p>Objectives</p> <ul> <li>View the contents of basic text files.</li> <li>Copy, move, and rename files and create/remove directories.</li> <li>Make a file read only.</li> <li>Use the <code>history</code> command to view and repeat recently used commands.</li> </ul> <p>Key points</p> <ul> <li>You can view file contents using <code>less</code>, <code>cat</code>, <code>head</code> or <code>tail</code>.</li> <li>The commands <code>cp</code>, <code>mv</code>, and <code>mkdir</code> are useful for manipulating existing files and creating new directories.</li> <li>The <code>history</code> command and the up arrow on your keyboard can be used to repeat recently used commands.</li> </ul>"},{"location":"level1/13_shell_manipulation/#viewing-the-contents-of-files","title":"Viewing the contents of files","text":"<p>From the previous exercises we know how to move around the file system of NeSI, but how do we look at the contents of files? One way to examine a file is to print out all of the contents using the program <code>cat</code>.</p> <p>To look at a text file, navigate to the <code>shell_data/</code> directory in your training folder and try to run the following command:</p> <p>code</p> <pre><code>cat SRR097977.fastq\n</code></pre> <p>That wasn't very helpful. It printed the full file content to the screen. It's a very small file, as bioinformatic files go, but still far to much to scan by eye. For smaller files, <code>cat</code> is a terrific tool but when the file is really big, it can be annoying to use.</p> <p>Fortunately there is another handy tool to read large files in a more manageable way. The command <code>less</code> can be used to open a file and navigate through it line by line. Enter the following command:</p> <p>code</p> <pre><code>less SRR097977.fastq\n</code></pre> <p>This will load the content of the file into your terminal, but rather than print every line instantly it will only show those that can fit on one page. Since this is a tool designed to run from the command line only we generally need to navigate using the keyboard. Some of the commonly used navigation commands are:</p> Key Action \u2193 Go forward one line \u2191 Go back one line Space Go forward one page b Go back one page g Return to the beginning of the file G Jump to the end of the file q Quit <p>That said, you can also scroll forward and back theough the file using the mouse scroll wheel.</p> <p><code>less</code> can also be used to search through files. Use the / key to begin a search. Enter the word you would like to search for and press Enter. The screen will jump to the next location where that word is found. You can seach for the next word by pressing / repeatedly. Each time, <code>less</code> searches from the current location forward. If you need to go back one entry, use ?.</p> <p>Exercise</p> <p>As an example, let's search forward for the sequence <code>TTTTT</code> in our file What are the next three nucleotides (characters) after the first instance of this sequence?</p> Solution <p><code>CAC</code></p> <p>Sometimes we want to strike a balance between <code>cat</code> and <code>less</code>. We need to see a bit of the file, but we don't want to look at it line by line. This is uaually when we need to process a file in some way, and we just need to remind outselves how it's formatted.</p> <p>The commands <code>head</code> and <code>tail</code> are for this task. They let you look at the beginning and end of a file, respectively.</p> <p>code</p> <pre><code>head SRR098026.fastq\n</code></pre> Output <pre><code>@SRR098026.1 HWUSI-EAS1599_1:2:1:0:968 length=35\nNNNNNNNNNNNNNNNNCNNNNNNNNNNNNNNNNNN\n+SRR098026.1 HWUSI-EAS1599_1:2:1:0:968 length=35\n!!!!!!!!!!!!!!!!#!!!!!!!!!!!!!!!!!!\n@SRR098026.2 HWUSI-EAS1599_1:2:1:0:312 length=35\nNNNNNNNNNNNNNNNNANNNNNNNNNNNNNNNNNN\n+SRR098026.2 HWUSI-EAS1599_1:2:1:0:312 length=35\n!!!!!!!!!!!!!!!!#!!!!!!!!!!!!!!!!!!\n@SRR098026.3 HWUSI-EAS1599_1:2:1:0:570 length=35\nNNNNNNNNNNNNNNNNANNNNNNNNNNNNNNNNNN\n</code></pre> <p>code</p> <pre><code>tail SRR098026.fastq\n</code></pre> Output <pre><code>+SRR098026.247 HWUSI-EAS1599_1:2:1:2:1311 length=35\n#!##!#################!!!!!!!######\n@SRR098026.248 HWUSI-EAS1599_1:2:1:2:118 length=35\nGNTGNGGTCATCATACGCGCCCNNNNNNNGGCATG\n+SRR098026.248 HWUSI-EAS1599_1:2:1:2:118 length=35\nB!;?!A=5922:##########!!!!!!!######\n@SRR098026.249 HWUSI-EAS1599_1:2:1:2:1057 length=35\nCNCTNTATGCGTACGGCAGTGANNNNNNNGGAGAT\n+SRR098026.249 HWUSI-EAS1599_1:2:1:2:1057 length=35\nA!@B!BBB@ABAB#########!!!!!!!######\n</code></pre> <p>By default the first/last 10 lines are printed. This can be changed by adding the <code>-n</code> option to the command to change the number. </p> <p>code</p> <pre><code>head -n1 SRR098026.fastq\n</code></pre> Output <pre><code>@SRR098026.1 HWUSI-EAS1599_1:2:1:0:968 length=35\n</code></pre> <pre><code>tail -n1 SRR098026.fastq\n</code></pre> Output <pre><code>A!@B!BBB@ABAB#########!!!!!!!######\n</code></pre> <p>Learning what these symbols all mean</p> <p>If you want to learn more about the FASTQ file format, and what these symbols mean see the brief description document here</p>"},{"location":"level1/13_shell_manipulation/#basic-file-manipulation","title":"Basic file manipulation","text":"<p>We now know how to read files through the command line, but before we worry about any complex bioinformatic work the most basic thing we need to be able to do on the command line, other than navigating directories, is moving, copying, and deleting files and directories. These are operations which you probably perform daily on your desktop computer and these all exist on the command line as well.</p> <p>There are commands available on the command line which do all of these things. These typically have short names which are derived from the word that represent:</p> Command Action <code>mkdir</code> Make a new directory. <code>rmdir</code> Remove (delete) an empty directory. <code>cp</code> Copy a file, either to a new location or into a new file. <code>mv</code> Move a file from one location to another.Can also be used to rename files by 'moving' them to a file with a different name. <code>rm</code> Remove a file.This is how you permanently delete files."},{"location":"level1/13_shell_manipulation/#creating-and-removing-directories","title":"Creating and removing directories","text":"<p>The <code>mkdir</code> (\"make directory\") command is used to make a new directory. Enter <code>mkdir</code> followed by a space, then the directory name you want to create:</p> <p>code</p> <pre><code>mkdir backup/\n</code></pre> <p>If you want to create multiple directories at once, you can specify multiple names:</p> <p>code</p> <pre><code>mkdir other_backup/ another_backup/\n</code></pre> <p>As long as these directories are empty, they can be removed with the <code>rmdir</code> (\"remove directory\") command:</p> <p>code</p> <pre><code>rmdir another_backup/\n</code></pre> <p>If you try to remove a directory with files in it, you will receive an error and the directory will remain intact.</p>"},{"location":"level1/13_shell_manipulation/#copying-and-moving-files","title":"Copying and moving files","text":"<p>The <code>cp</code> (\"copy\") and <code>mv</code> (\"move\") commands are mostly identical in how they work. Each command can be used in one of two ways. We can either copy/move a file from one name to another, or we can copy/move them into a new directory without changing the file names.</p> <p>code</p> <pre><code>cp SRR097977.fastq SRR097977.fq_backup\nmv SRR097977.fq_backup SRR097977.fq_bkup\n</code></pre> <p>In this pair on commands, we first create a copy of the file <code>SRR097977.fastq</code> with the name <code>SRR097977.fq_backup</code>. These are identical in their content.</p> <p>We then move/rename the <code>SRR097977.fq_backup</code> file (effectively, moving the contents of the file into a new file) to a different file named <code>SRR097977.fq_bkup</code>.</p> <p>If you run the <code>ls</code> command you will see that <code>SRR097977.fastq</code> is still present, as it was only copied, but <code>SRR097977.fq_backup</code> no longer exists.</p> <p>code</p> <pre><code>ls\n</code></pre> Output <p><code>`bash SRR097977.fastq SRR097977.fq_bkup</code></p> <p>Note</p> <p>We can also use the <code>mv</code> command to rename directories. However, the <code>cp</code> command by default does not work for directories, it must be invoked with a specific parameter to copy directory and it's contents.</p> <p>code</p> <pre><code>cp shell_data/ shell_data_bkup/\n</code></pre> Failure <pre><code>cp: -r not specified; omitting directory 'shell_data/'\n</code></pre> <p>The other way we can use these commands is to copy/move one or more files into a different directory. This can be handy when creating backups of data we do not wish to risk losing (copy), or when we want to organise data into different folders to make navigation easier (move).</p> <p>code</p> <pre><code>cp SRR097977.fastq SRR098026.fastq backup/\nmv SRR097977.fq_bkup other_backup/\n</code></pre> <p>In these cases, we either create duplicates of the target files in a new location, or move an existing file into a new location.</p>"},{"location":"level1/13_shell_manipulation/#removing-files","title":"Removing files","text":"<p>To remove a file, it's super easy. Just use the <code>rm</code> (\"remove\") command:</p> <p>code</p> <pre><code>rm other_backup/SRR097977.fq_bkup\n</code></pre> <p>Boom. It's gone. There is no Recycle Bin on the command line and there is no way to get that file back...</p> <p>Note</p> <p>We need to be really careful with the <code>rm</code> command and be very sure of which files you are removing. To try and avoid unwanted loss of data, the <code>rmdir</code> command only removes empty directories and by default the <code>rm</code> command will not delete directories either. At this level, we will not expand upon this further.</p>"},{"location":"level1/13_shell_manipulation/#command-history","title":"Command history","text":"<p>We've done a lot on the command line, but how do we keep track of everything so far? What if we forget something we've done recently, and want to check exactly what we did?</p> <p>You can view previous commands using the up arrow on your keyboard to go back through your recent commands. Likewise, the down arrow takes you forward in the command history. If what you're looking for is only a few commands ago this is the best way to see the information.</p> <p>If you are looking for something from a long time ago, you can view a list of your last ~1,000 commands with the <code>history</code> command.</p> <p>code</p> <pre><code>history\n</code></pre> Output <pre><code>1053  2023-02-24 15:58:35 ll\n1054  2023-02-24 16:01:35 ls\n1055  2023-02-24 16:01:42 ls -sh\n1056  2023-02-24 16:01:45 history\n</code></pre> <p>You can reuse one of these commands directly by referring to the number of that command. For example, if your history looked like above you could repeat command #1055 by entering:</p> <p>code</p> <pre><code>!1055\n</code></pre> <p>This can be really useful when you are taking notes on a complicated set of commands you have run, or if you are trying to remember what you did last time you were logged into NeSI.</p>"},{"location":"level1/21_illumina_inspection/","title":"2.1 - Assesing Illumina data","text":""},{"location":"level1/21_illumina_inspection/#21-assesing-illumina-data","title":"2.1 - Assesing Illumina data","text":"<p>time</p> <ul> <li>Teaching: 20 minutes</li> <li>Exercises: 20 minutes</li> </ul> <p>Learning objectives</p> <p>Objectives</p> <ul> <li>Know how to assess the quality of Illumina sequence data using visualisation tools such as <code>FastQC</code>.</li> </ul> <p>Key points</p> <ul> <li>Raw sequencing data is usually not appropriate for immediate analysis.</li> <li><code>FastQC</code> is a powerful tool for quickly generating visual reports to summarise key aspects of Illumina data.</li> </ul>"},{"location":"level1/21_illumina_inspection/#assessing-sequence-quality","title":"Assessing sequence quality","text":"<p>When we obtain data from a sequencing facility, it is always important to check the overall quality of the sample, and to confirm whether or not sequencing constructs have been removed from the seqence data. In particular, we want to know:</p> <ol> <li>Are there adapter and/or barcode sequences attached to the reads</li> <li>Are there any obvious low-quality regions of sequence</li> <li>Is there a quality drop-off towards the end of read-pair sequence which might necessitate trimming</li> </ol> <p>A very useful tool for answering these questions is <code>FastQC</code>. This tool takes a set of fastq files as input and produces reports for each one to allow us to answer the questions above, as well as examine over features of the sequences such as compositional bias, k-mer frequency profiles, and sequence duplication levels.</p> <p>Navigate to your <code>level1/</code> folder and then enter the <code>quality_illumina/</code> sub-folder. Once you are in the correct location run <code>FastQC</code> on the sample <code>SRR097977</code> using the following commands:</p> <p>code</p> <pre><code>fastqc -o results/ reads/SRR097977.fq.gz\n</code></pre> Output <pre><code>Started analysis of SRR097977.fq.gz\nAnalysis complete for SRR097977.fq.gz\n</code></pre> <p><code>FastQC</code> generates output reports in <code>.html</code> files and a <code>.zip</code> file that contains the main display resources. These can be viewed in a standard web browser. Since we are connection to NeSI using the JupyterHub system, we can view these directly:</p> <p>Note</p> <p><code>FastQC</code> does not load the forward and reverse pairs of a library in the same window, as it works on individual fastq files and is not aware of any read pairing in your library. You need to be mindful of how your samples relate to each other and in which order you have opened them if you are trying to compar the forward and reverse complement of a sequencing library.</p> <p>Open .html</p> <ol> <li>Click on the folder icon in the top left to open the folder navigator pane (if not already open).</li> <li>Use the file browsing system to navigate through to your current location.</li> <li>Double click on the output <code>.fastqc.html</code> files to open them in a new tab.</li> </ol> <p>Let's now look at some of the outputs, starting with the summary for sample <code>SRR097977</code>.</p> <p>FastQC output</p> <p></p> <p>This is the basic view for <code>FastQC</code> output. At the left-hand side of the tab is a navigation menu, which can move you quickly through the pages of summary information. Alternatively, you can simply scroll down the page to find the section you are most interested in.</p> <p>From the summary view, the main point of interest is the 'Basic Statistics' table. This gives you some brief summary information for your input file, such as the name of the file read (this can be important if you are working with many files), the fastq encoding, the numbers of sequences and the average length. You should already have a rough expectation for these numbers based on correspondence with your sequencing provider.</p>"},{"location":"level1/21_illumina_inspection/#per-base-sequence-quality","title":"Per base sequence quality","text":"<p>Scrolling down (or clicking on the 'Per base sequence quality' link) will take us to the main piece of information we wish to know about the samples - the overall quality of the sequences.</p> <p>FastQC output</p> <p></p> <p>This view provides us with a nice graphical summary of the average sequence quality along the length of our reads. Fastq Q-scores are ranked on the y-axis and the nucleotide position in the read (or range of positions, for reads which are several hundred nucleotides in length) are plotted sequentially along the x-axis.</p> <p>The sample <code>SRR097977</code> shows a severe, but expected pattern of quality. Sequence quality decreases as the reading window moves towards the right-hand side of the sequence.</p> <p>This view provides us with two pieces of information - how strictly we need to trim our sequences, and what effect we could expect to see on our number of sequences and sequence length after quality filtering.</p>"},{"location":"level1/21_illumina_inspection/#per-base-sequence-content","title":"Per base sequence content","text":"<p>The next piece of information which can be helpful to check is the average distribution of nucleotides across the sequence length. When examining this feature of an Illumina run, we do not expect to see an even split between the four nucleotides at each position.</p> <p>This is because most genomes have a bias for a high or low proportion of G-C pairs compared with A-T pairs. For example, the human genome is on average 41% G and C nucleotides, and 59% A and T nucleotides.</p> <p>On average then, when we look to the nulcotide frequency distribution for a random genome we should not expect 25% of each nucleotide in each position, but we should see equal frequencies of A/T and G/C.</p> <p>FastQC output</p> <p></p> <p>This is not the case in the <code>SRR097977</code> data which we were looking at, but we already know that this is a reasonably low-quality library. For a more realistic result, let's take a look at some better data taken from the Sequence Read Archive.</p> <p>code</p> <pre><code>fastqc -o results/ reads/ERR4179828_*.fq.gz\n</code></pre> Output <pre><code>Started analysis of ERR4179828_1.fq.gz\nApprox 5% complete for ERR4179828_1.fq.gz\nApprox 10% complete for ERR4179828_1.fq.gz\nApprox 15% complete for ERR4179828_1.fq.gz\nApprox 20% complete for ERR4179828_1.fq.gz\nApprox 25% complete for ERR4179828_1.fq.gz\nApprox 30% complete for ERR4179828_1.fq.gz\nApprox 35% complete for ERR4179828_1.fq.gz\nApprox 40% complete for ERR4179828_1.fq.gz\nApprox 45% complete for ERR4179828_1.fq.gz\nApprox 50% complete for ERR4179828_1.fq.gz\nApprox 55% complete for ERR4179828_1.fq.gz\nApprox 60% complete for ERR4179828_1.fq.gz\nApprox 65% complete for ERR4179828_1.fq.gz\nApprox 70% complete for ERR4179828_1.fq.gz\nApprox 75% complete for ERR4179828_1.fq.gz\nApprox 80% complete for ERR4179828_1.fq.gz\nApprox 85% complete for ERR4179828_1.fq.gz\nApprox 90% complete for ERR4179828_1.fq.gz\nApprox 95% complete for ERR4179828_1.fq.gz\nAnalysis complete for ERR4179828_1.fq.gz\nStarted analysis of ERR4179828_2.fq.gz\nApprox 5% complete for ERR4179828_2.fq.gz\nApprox 10% complete for ERR4179828_2.fq.gz\nApprox 15% complete for ERR4179828_2.fq.gz\nApprox 20% complete for ERR4179828_2.fq.gz\nApprox 25% complete for ERR4179828_2.fq.gz\nApprox 30% complete for ERR4179828_2.fq.gz\nApprox 35% complete for ERR4179828_2.fq.gz\nApprox 40% complete for ERR4179828_2.fq.gz\nApprox 45% complete for ERR4179828_2.fq.gz\nApprox 50% complete for ERR4179828_2.fq.gz\nApprox 55% complete for ERR4179828_2.fq.gz\nApprox 60% complete for ERR4179828_2.fq.gz\nApprox 65% complete for ERR4179828_2.fq.gz\nApprox 70% complete for ERR4179828_2.fq.gz\nApprox 75% complete for ERR4179828_2.fq.gz\nApprox 80% complete for ERR4179828_2.fq.gz\nApprox 85% complete for ERR4179828_2.fq.gz\nApprox 90% complete for ERR4179828_2.fq.gz\nApprox 95% complete for ERR4179828_2.fq.gz\nAnalysis complete for ERR4179828_2.fq.gz\n</code></pre> <p>Take a look at the result of the <code>ERR4179828_1</code> file. The per base sequence content plot should look similar to this:</p> <p>FastQC output</p> <p></p> <p>Here we can see a much more realistic result for the sequencing output. These sequencing data are from an M. bovis genome (BioProject PRJEB38523. M. bovis has an average GC richness of ~29%, which is consistent with the results in this figure - both the <code>C%</code> and <code>G%</code> values are between 15% and 18% for the length of the sequence, and the <code>A%</code> and <code>T%</code> are slightly over 30%.</p> <p>This is true for most of the sequence, but at the starting nucleotides something is clearly different. A pattern such as this is quite a common occurance and is due to methodological biases inherent in library preparation strategies. This phenomenon has been demonstrated in the context of RNASeq libraries<sup>1, 2</sup> and has also been observed in DNA libraries prepared with transposon-based kits such as the Nextera preparation kits. If you follow the link above to ERR4179828 you can see in the metadata that these sequences are from an Illumina MiSeq run prepared with the Nextera XT system.</p> <p><sup>1</sup>Hansen KD, Brenner SE, Dudoit S (2010) Biases in Illumina transcriptome sequencing caused by random hexamer priming. Nucleic Acids Res. 38(12):e131. https://doi.org/10.1093/nar/gkq224</p> <p><sup>2</sup>van Gurp TP, McIntyre LM, Verhoeven KJF (2013) Consistent Errors in First Strand cDNA Due to Random Hexamer Mispriming. PLoS ONE 8(12): e85583. https://doi.org/10.1371/journal.pone.0085583</p>"},{"location":"level1/21_illumina_inspection/#adapter-content","title":"Adapter content","text":"<p>When sequencing is peformed we must add adapter sequences to our extracted DNA (or cDNA) to facilitate sequencing. Because these adapter sequences become part of the DNA sequence, they are read by the sequencer and reported as observed sequence.</p> <p>It is generally good practice for a sequencing facility to remove these adapter sequences from your data before returning it to you, and if you are using a multiplexed (barcoded) method of sequencing this almost certainly must be performed before your sequence is made available.</p> <p>However, this is never guaranteed and it is always good practice to confirm that there are no adapters or barcodes in your library before you attempt to analyse the data. Since adapters are attached to the start of the DNA fragment being sequenced, they will always appear in the starting positions of your reads if not removed.</p> <p>Exercise</p> <p>Think about what the per base sequence content of a library that has not had adapters removed would look like. How could you go about detecting whether or not adapters your present at the start of your sequences if;</p> <ol> <li>You know what adapters were used in your sequencing experiment? (i.e. the sequencing centre has told you what preparation method was used)</li> <li>You do not know what adapters were used in your sequencing experiment? (i.e. you know that some preparation was performed, but the sequencing centre has not revealed the information to you)</li> </ol> <p>Try to formulate your answers in terms only of tools used in this workshop, or the previous Level 1 training.</p> Solution <p>Note: These are not the only valid answers. If you have an answer that differs from the answers below, share it with the workshop attendees to discuss it.</p> <ol> <li>You could use a tool such as a search function in <code>less</code>, or <code>grep</code> to count the instances of the adapter sequence within your library.</li> <li>If you were to use the per base sequence content view from <code>FastQC</code>, the frequencies for the first 10 - 20 nucleotides would skew to 100% for the adapter sequence used.</li> </ol> <p>It is important to think carefully about the expected attributes of your sequence data as you work with it. Although there are plenty of tools (<code>FastQC</code> included) which can automatically detect the common library adapters, 'common' is a relative term and depends on how frequently updated the tool is compared with the latest sequencing technologies.</p> <p>Take a look at the adapter content view for the <code>ERR4179828_1</code> library. Are there any adapter or preparation sequences detected in these data? Are they where you would expect them?</p> <p>FastQC output</p> <p></p> <p>It appears that any Illumina sequencing adapters that were used in the library preparation has already been removed from the start of the sequences, but there are Nextera transposase sequences present in the tail end of some sequences.</p> <p>Exercise</p> <p>Considering what you know about the Illumina sequencing platform, how do you think it is possible to get adapters or sequencing constructs on the tail end (far end) of a sequence?</p> Solution <p>This is a phenomenon known as read-through. When the DNA fragment being sequenced is shorter than the read length. For example, in these data the read length is 150 bp. If a fragment of only 80 nucleotides in length is sequenced, the sequencing machine does not know that there are only 80 positions to read and it will not terminate after those 80 nucleotides are processed.</p> <p>Instead, the sequencer will continue to read the sequence it can find at the 5' end of the 80 nucleotides, which will be the reverse adapter.</p>"},{"location":"level1/22_illumina_filtering/","title":"2.2 - Filtering Illumina data","text":""},{"location":"level1/22_illumina_filtering/#22-filtering-illumina-data","title":"2.2 - Filtering Illumina data","text":"<p>time</p> <ul> <li>Teaching: 10 minutes</li> <li>Exercises: 20 minutes</li> </ul> <p>Learning objectives</p> <p>Objectives</p> <ul> <li>Be familiar with the consequences of paired-end sequencing data, compared with single-end sequencing.</li> <li>Be able to perform quality filtering to remove adapter and barcode regions, as well as low quality sequence regions.</li> </ul> <p>Key points</p> <ul> <li>When working with paired-end sequence data, maintaining the order of sequences between forward and reverse sequence files is critical.</li> <li>Be aware of how to account for singleton (orphan) sequences when working with paired-end sequencing data.</li> <li>Sequences can be filtered to remove low quality sequences and sequencing artefacts using <code>fastp</code>.</li> </ul>"},{"location":"level1/22_illumina_filtering/#considerations-when-working-with-paired-end-data","title":"Considerations when working with paired-end data","text":"<p>Once we have visualised our sequence quality, we need to make some decisions regarding whether we perform quality filering or not, how severely the data must be trimmed, and whether or not we need to remove adapter sequences.</p> <p>There are many tools available for trimming sequence data, and today we are only going to work with one - fastp. This tool was selected for its speed and performance, as well as having a good suite of quality-of-life features, but it there are many alternate tools out there which can perform the same job.</p> <p>Before we actually start to work on our data, there are some important factors of the nature of paired-end sequencing data that we need to consider.</p> <p>At it's most basic level, a tool like <code>fastp</code> simply takes one input sequence file, trims the sequences according to some specifications, then saves the output to a new file. Sequences from the input file are read and trimmed for regions that do or do not match filtering criteria.</p> <p>For example, adapter regions may be removed along with regions where the average sequence quality falls below a required threshold. This means that the entirity of a sequence falls below the selection criteria, the resulting sequence will not be written to the output file. Often, short sequences will also be rejected from the output.</p> <p>When working with Illumina data, each fragment sequenced is read from the forward and reverse direction so we have two corresponding paired sequences representing each piece of DNA in our library.</p> <p>When working with this paired data, it is critical to preserve the ordering between the forwrad and reverse sequences so that their relationship to each other is maintained.</p> The consequences of treating forwad and reverse sequence files as independent data <p>If our sequences consist of paired reads you cannot just run the trimming tool over each sequence file independently because the forward/reverse reads are ordered such that the first sequence in the reverse file is the partner to the first sequence in the forward file. If a sequence in one of these files is rejected by the filtering system but it's partner passes filtering, then there will be an uneven number of sequences written into the two output files.</p> <p>This is a massive problem, because when we perform tasks like assembly and read mapping, the tools will assume that the sequence order is identical between forward and reverse files and that there is a spatial relationship between these paired sequences. For example, if we have a pair of files where the sequences align like so:</p> <p>Observed input ordering</p> <pre><code>@Seq1-----------------@Seq1\n@Seq2-----------------@Seq2\n@Seq3-----------------@Seq3\n@Seq4-----------------@Seq4\n@Seq5-----------------@Seq5\n@Seq6-----------------@Seq6\n@Seq7-----------------@Seq7\n@Seq8-----------------@Seq8\n@Seq9-----------------@Seq9\n</code></pre> <p>And after filtering we might observe an output like the following, where <code>Seq4_R1</code> and <code>Seq7_R2</code> are removed.</p> <p>Expected output ordering</p> <pre><code>+ @Seq1-----------------@Seq1\n+ @Seq2-----------------@Seq2\n+ @Seq3-----------------@Seq3\n- ----------------------@Seq4\n+ @Seq5-----------------@Seq5\n+ @Seq6-----------------@Seq6\n- @Seq7----------------------\n+ @Seq8-----------------@Seq8\n+ @Seq9-----------------@Seq9\n</code></pre> <p>Any downstream analysis tools we use will not be aware of these gaps, and read the pairing information as:</p> <p>Observed output ordering</p> <pre><code>+ @Seq1-----------------@Seq1\n+ @Seq2-----------------@Seq2\n+ @Seq3-----------------@Seq3\n- @Seq5-----------------@Seq4\n- @Seq6-----------------@Seq5\n- @Seq7-----------------@Seq6\n+ @Seq8-----------------@Seq8\n+ @Seq9-----------------@Seq9\n</code></pre> <p>The consequence of these gaps should be obvious - <code>Seq5_R1</code> pairs with <code>Seq4_R2</code>, <code>Seq6_R1</code> with <code>Seq5_R2</code>, and <code>Seq7_R1</code> with <code>Seq6_R2</code>. The impact this has varies depending on what is being done with the next stage of analysis, but whether these errors cause chimeric assembly or simply a failure to successfully map a sequence pair to a reference genome, it is entirely an aretfact of our analysis procedure and not the biology of the sample.</p> <p>Any worthwhile trimming tool is aware of this problem, and has a built-in capacity for filtering these issues.</p>"},{"location":"level1/22_illumina_filtering/#performing-paired-end-sequence-trimming-with-fastp","title":"Performing paired-end sequence trimming with <code>fastp</code>","text":"<p>We are going to use the tool <code>fastp</code> to practice filtering some Illumina sequence data, making sure to preserve read order between the forward and reverse sequences. Begin by navitating to the <code>quality_illumina/</code> folder and loading the module for <code>fastp</code>.</p> <p>code</p> <pre><code>cd /nesi/shared/&lt;username&gt;/level1/quality_illumina/\n</code></pre> <p>We are going to build a command for filtering the paired files <code>reads/miseq_R1.fq.gz</code> and <code>reads/miseq_R2.fq.gz</code>. The most basic implementation of <code>fastp</code> is to take the input sequences, apply and trimming, and then save the paired sequences where both forward and reverse sequence passed the filtering criteria.</p> <p>code</p> <pre><code>fastp -i reads/miseq_R1.fq.gz -I reads/miseq_R2.fq.gz -o results/miseq_R1.fq.gz -O results/miseq_R2.fq.gz\n</code></pre> <p>Interpreting the command parameters</p> <p>We have just provided four parameters to <code>fastp</code>. There are two conventions you need to be aware of when interpreting these:</p> <ul> <li> <p>The <code>-i</code> and <code>-o</code> flags are used to specify _I_nput and _O_utput files. The filtered sequences read from the file provided with <code>-i</code> will be written to the destination specified with <code>-o</code>.</p> </li> <li> <p>The lowercase and uppercase convention is used to signify to <code>fastp</code> that these are paired-end files. The file specified with <code>-I</code> is understood to be the reverse partner of the file specified with <code>-i</code>, and its output is written to <code>-O</code>.</p> </li> </ul> <p>Exercise</p> <p>Why can we not just run <code>fastp</code> in the following way?</p> <p>code</p> <pre><code>fastp -i reads/miseq_R1.fq.gz -o results/miseq_R1.fq.gz\nfastp -i reads/miseq_R2.fq.gz -o results/miseq_R2.fq.gz\n</code></pre> Solution <p>The paired-end relationship between sequences in each file will not be maintained. If a sequence passes quality filtering in the forward file, but its partner in the reverse file fails the result ill still be written to the output file (and vice versa).</p> <p>The need to retain paired ordering is is a fairly common requirement, so filtering tools have an additional output type for these unpaired (also called 'orphan' or 'singleton') sequences which pass filtering when their partner do not. We can modify our <code>fastp</code> command to retain data when these cases happen, directing the sequences to new files:</p> <p>code</p> <pre><code>fastp -i reads/miseq_R1.fq.gz -I reads/miseq_R2.fq.gz -o results/miseq_R1.fq.gz -O results/miseq_R2.fq.gz --unpaired1 results/miseq_R1s.fq.gz --unpaired2 results/miseq_R2s.fq.gz\n</code></pre> Schematic of how the output files will be populated <p>Calling back to the description above of the hypothetical data set of 10 sequences, we would now expect our output files to look something like:</p> <p>miseq_R1.fq.gz / miseq_R2.fq.gz</p> <pre><code>+ @Seq1---------------------@Seq1\n+ @Seq2---------------------@Seq2\n+ @Seq3---------------------@Seq3\n+ @Seq5---------------------@Seq5\n+ @Seq6---------------------@Seq6\n+ @Seq8---------------------@Seq8\n+ @Seq9---------------------@Seq9\n</code></pre> <p> miseq_R1s.fq.gz</p> <pre><code>+ @Seq7_R1\n</code></pre> <p>miseq_R2s.fq.gz</p> <pre><code>+ @Seq4_R2\n</code></pre> <p>We have retained those individual reads, without compromising the pairing order.</p> <p><code>fastp</code> has one nice feature that we can exploit - if we don't set a value for the <code>--unpaired2</code> parameter, the sequences which would have gone to this file are instead sent to the same place as <code>--unpaired1</code>. This means that we can capture all of our unpaired reads in a single file with a more concise command:</p> <p>code</p> <pre><code>fastp -i reads/miseq_R1.fq.gz -I reads/miseq_R2.fq.gz -o results/miseq_R1.fq.gz -O results/miseq_R2.fq.gz --unpaired1 results/miseq_s.fq.gz\n</code></pre> Schematic of how the output files will be populated <p>miseq_R1.fq.gz / miseq_R2.fq.gz</p> <pre><code>+ @Seq1---------------------@Seq1\n+ @Seq2---------------------@Seq2\n+ @Seq3---------------------@Seq3\n+ @Seq5---------------------@Seq5\n+ @Seq6---------------------@Seq6\n+ @Seq8---------------------@Seq8\n+ @Seq9---------------------@Seq9\n</code></pre> <p>miseq_s.fq.gz</p> <pre><code>+ @Seq7_R1\n+ @Seq4_R2\n</code></pre>"},{"location":"level1/22_illumina_filtering/#customising-the-quailty-filtering-parameters","title":"Customising the quailty filtering parameters","text":"<p>We talked a lot above about how our reads were being filtered but not what this actually means. In general, there are two ways to screen a sequence for quality, once we have determined what we want our lower limit of acceptable quality to be.</p> <ol> <li>Remove a sequence with an average quality below this value.</li> <li>Cut the regions of a sequence with quality below this value.</li> </ol> <p>The later of these is definitely the better option, as a sequence with quite high average quality can still have large spans of low-quality sequence which will interfere with out analysis.</p> <p>A very popular method of achieving this is using a 'sliding window' approach. This consists of the sequence being read from one end to the other, reading a small number of bases (the window) and assessing the average quality over these nucleotides. The window progresses (slides) along the sequence, and at the first instance of average quality dropping below the threshold, the sequence is cut and the remainder discarded.</p> <p>In this kind of analysis, the default threshold is a Q-score of 20, and the window size is 4. These are fairly standard values, but can be changed if desired.</p> Changing the default trimming values <p>This method is enabled by the <code>--cut_right</code> parameter in <code>fastp</code>, as the sliding window moves from the left-hand side (start) of the sequence to the right-hand side (end), and everything discarded after the cut point is therefore the right half of the sequence.</p> <p>An example of how you could customise a sliding window approach in <code>fastp</code> is:</p> <p>code</p> <pre><code>fastp --cut_right --cut_window_size 8 --cut_mean_quality 15 -i ... -I ... -o ... -O ...\n</code></pre> <p>Which would assess an 8 nucleotide window size and trim at the first instance of the average quality falling below Q15.</p> <p>After any form of trimming, there is also the option to remove sequences which fall below a required minimum sequence length. Length filtering is appplied after trimming, so that the length of a sequence is evaluated once low quality regions are removed.</p>"},{"location":"level1/22_illumina_filtering/#other-useful-features-of-fastp","title":"Other useful features of <code>fastp</code>","text":"<p>There are just a few more features of <code>fastp</code> which can be useful to know.</p> <ul> <li><code>--thread</code></li> <li>Increase the number of computer processes (threads) used to process the sequences. Using more threads typically makes the job faster to complete. The default value is 2.</li> <li><code>--html</code>/<code>--json</code></li> <li>Change the names of the report files generated during a run.</li> <li>This is very useful when processing many samples, as the default output names (<code>fastp.html</code> and <code>fastp.json</code>) are recycled on each use and successive runs of <code>fastp</code> overwrite the output of previous runs.</li> <li><code>--length_required</code></li> <li>Discard sequences shorter than the specified length (after trimming) regardless of their quality.</li> <li><code>--n_base_limit</code></li> <li>Discard sequences with more than the specified limit of <code>N</code> characters, regardless of the sequence quality.</li> <li><code>--adapter_sequence</code></li> <li>Specify a sequencing adapter sequence to be identified and removed from the start and end of reads.</li> <li>Normally this is performed by the sequencing facility, but not always (as we saw in the <code>FastQC</code> exercises).</li> <li><code>--detect_adapter_for_pe</code></li> <li>As above, but with this parameter <code>fastp</code> will attempt to identify the adapter sequence by screening the input sequences and looking for conserved regions at the beginning of each read.</li> </ul>"},{"location":"level1/22_illumina_filtering/#example-of-manual-adapter-removal","title":"Example of manual adapter removal","text":"<p>code</p> <pre><code>fastp --adapter_sequence=AGATCGGAAGAGCACACGTCTGAACTCCAGTCA  -i ... -o ...\n</code></pre>"},{"location":"level1/22_illumina_filtering/#example-of-automatic-adapter-removal","title":"Example of automatic adapter removal","text":"<p>code</p> <pre><code>fastp --detect_adapter_for_pe  -i ... -o ...\n</code></pre>"},{"location":"level1/31_nanopore_inspection/","title":"3.2 - Assesing Nanopore data","text":""},{"location":"level1/31_nanopore_inspection/#32-assesing-nanopore-data","title":"3.2 - Assesing Nanopore data","text":"<p>time</p> <ul> <li>Teaching: 10 minutes</li> <li>Exercises: 10 minutes</li> </ul> <p>Learning objectives</p> <p>Objectives</p> <ul> <li>Know how to assess the quality of Oxford Nanopore data using visualization tools such as <code>pycoQC</code></li> </ul> <p>Key points</p> <ul> <li>Quality filtering can be performed natively by the <code>MinKNOW</code> software during sequencing</li> <li>It is important to use tools designed specifically for long read data when performing assessment and trimming of Nanopore reads</li> </ul>"},{"location":"level1/31_nanopore_inspection/#background","title":"Background","text":"<p>The process of translating the electrical current differentials generated by Nanopore sequencers into the nucleotide information is known as 'base calling'. Base calling is in most cases performed in real time as sequencing happens, so that when the sequencing run is finished we have our data ready for analysis.</p> <p>The tool which performs base calling during sequencing is <code>guppy</code> and among its functions it is capable of performing quality filtering for us as it operates. As such, you may not need to perform quality trimming on your Nanopore data.</p> <p>When setting up the sequencing run parameters using the sequencing software (<code>MinKNOW</code>), we can specify if we want the base calling to be performed automatically by the computer. When the basecalling option is ON, the output of the sequencing run will be saved in fastq files. As part of the set up, we are also given the choice of quality filtering and it is standard procedure to use these settings:</p> <p>Enabling quality filtering in MinKNOW</p> <p></p> <p>Configuring quality filtering in MinKNOW</p> <p></p> <p>However, there are still several cases where we may need to revisit our data and perform filtering. For example;</p> <ol> <li>If receiving sequence data from an online source, we may not be able to verify what filtering was applied so need to perform our own to be sure of the data quality.</li> <li>Quality filtering may have been perofrmed, but it is of a level not appropriate for our work (e.g. the filtering criteria were too lenient and poor quality sequences have still slipped through).</li> </ol>"},{"location":"level1/31_nanopore_inspection/#assessing-sequence-quality","title":"Assessing sequence quality","text":"<p>Assuming that we do not know the origin of our data (as you do not in this exercise), the initial step for any sequencing project is quality control to assess the quality of your data. This will give you some statistics of your sequencing data, such as length and quality score distributions, and may be used to identify potential problems with your input DNA/RNA, the sequencing run or the output itself.</p> <p>We are going ot start with a the tool <code>pycoQC</code>. This is a data visualisation and quality control tool for Nanopore data. In contrast to many other assessment tools, it does not take your sequencing files as input but instead requires a Nanopore-specific output file generated during the sequencing run.</p> <p>To run <code>pycoQc</code>, navigate to your <code>quality_nanopore</code>/ folder and run the following command:</p> <p>code</p> <pre><code>pycoQC -f reads/sequencing_summary.txt -o results/sequencing_report.html\n</code></pre> <p>As the tool runs you might see some output that looks like:</p> <pre><code>module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g.     `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n</code></pre> <p>This is fine, just a warning that the way <code>pycoQC</code> was written was not best practice in a few areas. It does not affect the performance at all.</p> <p>Once the command is finished, you can load it into yout <code>JupyterHub</code> session by navigating to it in the file browser, right-clicking, and selecting <code>Open in New Browser Tab</code></p> Opening by double-clicking (incorrect) <p></p> <p>Opening by right-click</p> <p></p> <p><code>pycoQC</code>  generates output reports as html files, which we can open the same way as we did for <code>FastQC</code>.</p> <p>Exercise</p> <p>Inspect the different plots and statistics to answer the following questions:</p> <ol> <li>How many reads do you have in total?</li> <li>What is the median, minimum, and maximum read length?</li> <li>What do the mean quality and the quality distribution of the run look like? (Remember, Q10 means an error rate of 10%)</li> </ol> Solution <ol> <li>~270k reads in total (see the Basecall summary of <code>pycoQC</code>'s output page)</li> <li>The median read length can also be found in the same place. The median length is 3,890 bp for all reads, or 4,070 for those that passed <code>MinKNOW</code>'s quality filtering.</li> <li>To find the minimum and maximum read lengths look at the 'Basecalled read lengths' plot.</li> <li>If you hover over the start and the end of the plotted length distribution you will see the length followed by the number of reads.</li> <li>The minimum read length for the passed reads is about 200 bp, the maximum length ~130,000 bp.</li> <li>The median quality of the reads can be found in the basecall summary, and the distribution in the 'Basecalled reads PHRED quality plot'.</li> <li>The majority of the reads has a Q-score below 10, i.e., an error rate of &gt;10%.</li> <li>These results can be considered normal although it is possible to obtain better quality.</li> </ol> <p>In addition to read statistics, <code>pycoQC</code> also gives a lot of information about the sequencing run and the flowcell itself such as sequencing run, yield over time, number of active pores, and so on.</p> <p>One of the strengths of <code>pycoQC</code> is that it is interactive and highly customisable. Plots can be cropped, you can zoom in and out, sub-select areas and export figures. For detailed usage and examples see the <code>pycoQC</code> documentation: https://a-slide.github.io/pycoQC/.</p>"},{"location":"level1/32_nanopore_filtering/","title":"3.2 - Filtering Nanopore data","text":""},{"location":"level1/32_nanopore_filtering/#32-filtering-nanopore-data","title":"3.2 - Filtering Nanopore data","text":"<p>time</p> <ul> <li>Teaching: 10 minutes</li> <li>Exercises: 10 minutes</li> </ul> <p>Learning objectives</p> <p>Objectives</p> <ul> <li>Be able to perform quality filtering of Nanopore data to remove short reads and adapter sequences</li> </ul> <p>Key points</p> <ul> <li>Sequences can be filtered to remove low quality reads and short reads using tools such as <code>Nanofilt</code></li> <li>It is important to use tools designed specifically for long read data when performing assessment and trimming of Nanopore reads</li> <li>Identifying and removing residual sequencing adapters is not always necessary, but tools exist for it if required</li> </ul>"},{"location":"level1/32_nanopore_filtering/#identifying-and-removing-adapters","title":"Identifying and removing adapters","text":"<p>Before we start worring about the raw sequencing quality of our reads, there is one factor to consider. When creating HTS libraries for sequencing, barcode and adpater sequences are appended to the ends of the template DNA to facilitate sequencing. These are an artificial construct, not reflective of the read biology of our sample, but are sequenced the same as the template DNA by the sequence device.</p> <p>Adapters may be found at the start of end of a sequence, as with Illumina reads, but an issue unique to Nanopore sequencing is the occurence of mid-sequence adapters. This occurs when a sequence moves through the sequencing pore and is immediately followed by another read. If this happens quickly enough that the base calling tool can realise the first sequence has ended, the the new sequence is appended to tail of the first.</p> <p>When this phenomenon happens you end up with a chimeric fusion of the first and second sequence, separated by the adapter of the second sequence. While mid-sequence adapaters artifacts are rare (typically less than 1% of a sequencing library) they are very easy to screen for, and it is important to identify and correct them in order to get the best classification or assembly result possible.</p> <p>Like quality filtering, adapater removal (including mid-sequence adapters) can be performed during base calling, but if it was not, we can use the tool <code>porechop</code> to identify and remove adapaters:</p> <p>code</p> <pre><code>porechop --threads 2 -i reads/nanopore.fq.gz -o results/nanopore.porechop.fq\n</code></pre> Output <pre><code>Trimming adapters from read ends\n     SQK-NSK007_Y_Top: AATGTACTTCGTTCAGTTACGTATTGCT\n  SQK-NSK007_Y_Bottom: GCAATACGTAACTGAACGAAGT\n     1D2_part_2_start: CTTCGTTCAGTTACGTATTGCTGGCGTCTGCTT\n       1D2_part_2_end: CACCCAAGCAGACGCCAGCAATACGTAACT\n\n3,722 / 3,722 (100.0%)\n\n3,120 / 3,722 reads had adapters trimmed from their start (68,630 bp removed)\n  834 / 3,722 reads had adapters trimmed from their end (9,680 bp removed)\n\n\nSplitting reads containing middle adapters\n3,722 / 3,722 (100.0%)\n\n20 / 3,722 reads were split based on middle adapters\n</code></pre> <p>Watch the output as <code>porechop</code> runs, and see how many residual adapters were found in your mock data set. One of the really nice features of <code>porechop</code> is that we do not need to know which adapter sequences were used in library construction - the tool has a built-in reference set of the common adapter sequences and reads are screened for all by default.</p> <p>Exercise</p> <p>From the output of <code>porechop</code>, answere the following questions:</p> <ol> <li>Which adapter sets were used to produce this library?</li> <li>How many sequences were flagged as carrying a forward or reverse adapter sequence?</li> <li>How many sequences had mid-sequence adapaters which needed removal?</li> </ol> Solution <ol> <li>The adapters from the SQK-NSK007 and 1D2 sequencing kits were found in this library. </li> <li>3,120 sequences were found to have a forward adapter attached, and 834 sequences had an end adapter attached.</li> <li>20 of the reads were found ot have a mid-sequence adapter.</li> </ol>"},{"location":"level1/32_nanopore_filtering/#removing-short-and-low-quality-reads","title":"Removing short and low quality reads","text":"<p>Finally, we need to remove low quality data from the sequencing library. It can also be helpful to remove short reads from the data set at this point, as these can negatively impact the results of your downstream analysis in some situations.For example, if you are trying to assemble a genome from the data, the short reads may interfere with the assembly process and result in a worse genome that you would get if they were screened rom the library beforehand.</p> <p><code>NanoFilt</code> is a tool that can filter reads by quality score and length. It is also capable of cropping a specified number of bases from the start or the end of a read, which can be useful sometimes as read quality is not uniform along the sequences - in Nanopore data it is common to see that the average quality score in the first few dozen nucleotides is significantly worse than the rest of the sequence.</p> <p>We will not apply <code>NanoFilt</code> to:</p> <ul> <li>Remove all reads with quality scores under 15</li> <li>Remove all reads shorter than 500 bp</li> <li>Trim the first 50 nucleotides off all reads</li> </ul> <p>code</p> <pre><code>NanoFilt -q 15 -l 500 --headcrop 50 &lt; results/nanopore.porechop.fq &gt; results/nanopore.qc.fq\n</code></pre> <p>Note</p> <p><code>NanoFilt</code> behaves a bit strangely in the way it takes input data and writes output. Rather than specifying files with flags such as <code>-i</code> and <code>-o</code>, it reads a data from a stream called <code>stdin</code>, and writes to a separate channel called <code>stdout</code>.</p> <p>Working with these channels is part of the Level 2 training, for now you just need to know that the <code>&lt;</code> character sets the <code>stdin</code> input, and <code>&gt;</code> specifies where the <code>stdout</code> data will go.</p> <p>We can get a quick estimate for how much data passed our quality requirements by checking the number of sequences in the input and output files using Seqkit. Seqkit has a range of features but the <code>stats</code> command gives really good summary statistics for sequencing datasets. Here we are using the flag <code>-a</code> to return all the available stats. </p> <p>code</p> <pre><code>seqkit stats -a results/*.fq\n</code></pre> Output file format type num_seqs sum_len min_len avg_len max_len Q1 Q2 Q3 sum_gap N50 Q20(%) Q30(%) GC(%) results/nanopore.porechop.fq FASTQ DNA 3,735 45,214,457 6 12,105.6 82,753 2,723.5 8,489 17,927 0 21,118 69.79 50.97 30.83 results/nanopore.qc.fq FASTQ DNA 1,313 16,675,742 500 12,700.5 77,866 3,658 9,245 18,869 0 21,365 78.07 58.66 31.03 <p>How many sequences did we lose in the process?</p>"},{"location":"level1/41_blastn_annotation/","title":"4.1 - Annotating with BLAST","text":""},{"location":"level1/41_blastn_annotation/#41-annotating-with-blast","title":"4.1 - Annotating with BLAST","text":"<p>time</p> <ul> <li>Teaching: 15 minutes</li> <li>Exercises: 15 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level1/41_blastn_annotation/#objectives","title":"Objectives","text":"<ul> <li>Understand the basic idea behind the <code>BLAST</code> algorithm</li> <li>Learn how to submit <code>BLAST</code> jobs to the NeSI cluster using <code>slurm</code></li> <li>Understand the differences between nucleotide and protein sequence matching</li> <li>Be aware of which publicly available databases are appropriate for which data</li> </ul>"},{"location":"level1/41_blastn_annotation/#keypoints","title":"Keypoints","text":"<ul> <li>Annotation is required in order to identify the function and origin of sequences obtained from HTS analysis</li> <li>There are different databases available for annotation and classification</li> </ul>"},{"location":"level1/41_blastn_annotation/#the-blast-process","title":"The <code>BLAST</code> process","text":"<p>BLAST (**B**asic **L**ocal **A**lignment **S**earch **T**ool) was developed in 1990 Altschul et al., 1990 and is now one of the most well known, and widely used Bioinformatics tools. BLAST compares novel (query) sequences against a database of reference sequences (target sequences) and reports back the similarity between each query and target sequence. Using the metadata associated with each reference sequence we can make inferences about the query sequence such as taxanomic origin, or function.</p> <p>Overview of <code>BLAST</code> steps</p> <p></p> <p>Where matches are found, BLAST then extends the ends of the seed one position at a time and assesses how well the seed continues to match the targets.  </p> <p></p> <p>Matches and mismatches are recorded and the seed extenstion continues.    </p> <p> </p> <p>BLAST is also able to introduce insertions to preserve a match between query and target sequences.  </p> <p> </p> <p>The quality of the match between the query and the target are evaluated in terms of how well conserved the sequence composition is between the pair, including how many insertion or deletion events need to be introduced to maintain the matching.</p> <p>In a BLAST analysis, every query sequence will be compared with every target in the database. Results are ranked by the quality of each match. Typically, we restrict our results to only return a certain number of the best matches, rather than everything with any degree of similarity to the query. Regardless of how many results are returned, for high quality matches we would expect to see a strong consensus in the gene function and taxonomy of the top matches. From these matches we can make inferences about the origin and function of the query sequence. </p>"},{"location":"level1/41_blastn_annotation/#submitting-a-nucleotide-blast-job-on-nesi","title":"Submitting a nucleotide <code>BLAST</code> job on NeSI","text":"<p>The power of <code>BLAST</code> lies in its ability to exhaustively search for query sequences in very large databases of target sequence data, however this can make BLAST a computationally expensive process. This means that on NeSI, when we run a BLAST analysis we need to use <code>slurm</code> to request resources and schedule our job. </p> <p><code>BLAST</code> is a commonly used tool and NeSI have template <code>slurm</code> scripts available for us to use. We will use a template today to prepare a <code>slurm</code> script to use <code>BLAST</code> to annotate a collection of sample sequences.</p> Different <code>BLAST</code> options <p>There are different types of <code>BLAST</code> we can use depending on the type of sequence data we have as input, and the types of databases we want to search to generate output. </p> <p></p> <p>Here we want to classify some nucleotide sequences, such as might be generated during diagnostic work. Since we are searching with nucleotide query sequences against a nucleotide reference database, we will use <code>BLASTn</code>.</p> <p>Navigate to the <code>blast_annotation/</code> folder and see what files have been placed there.</p> <p>code</p> <pre><code>cd /home/shared/&lt;username&gt;/level1/blast_annotation/\n\nls\n</code></pre> Output <pre><code>input.fna level1_blast.sl\n</code></pre> <p>You should see two files, one with a <code>.fna</code> extension which contains the sequences we wish to classify using <code>BLASTn</code>. The other is a text file that looks like:</p> <p>code</p> <pre><code>#!/bin/bash -e\n#SBATCH --account nesi03181\n#SBATCH --job-name level1_blast\n#SBATCH --output=level1_blast.%j.out\n#SBATCH --error=level1_blast.%j.err\n#SBATCH --mail-user &lt;insert email here&gt;\n#SBATCH --time 00:30:00\n#SBATCH --cpus-per-task 16\n#SBATCH --mem 30GB\n\nmodule purge\nmodule load BLAST/2.13.0-GCC-11.3.0\nmodule load BLASTDB/2023-07\n\ncd /home/shared/&lt;username&gt;/level1/blast_annotation/\n\nblastn -num_threads ${SLURM_CPUS_PER_TASK} -db ${BLASTDB}/nt \\\n       -outfmt \"6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore salltitles staxids\" \\\n       -evalue 1e-3 -query input.fna -out output.txt\n</code></pre> <p>This is our first example of a <code>slurm</code> script. We will discuss the contents of this script in the next section, while our <code>BLAST</code> job is running. For now, we are just going to make some minor modifications to the script to customise it for you.</p> <p>Exercise</p> <p>Open the <code>level1_blast.sl</code> file by double-clicking it in the file browser, and make the following modifications:</p> <ol> <li>Replace the value <code>&lt;insert email here&gt;</code> with your email address.</li> <li>Replace the value <code>&lt;username&gt;</code> with your login name, completing the path in the <code>cd</code> command.</li> </ol> <p>Once you are happy with the content of your script, you would normally 'submit' the job request to the NeSI cluster using the command below. However, because we are working in a training environment we do not have access to the usual <code>slurm</code> tools.</p> <p>code</p> <pre><code>sbatch level1_blast.sl\n</code></pre> Output <pre><code>Submitted batch job ########\n</code></pre>"},{"location":"level1/42_slurm_introduction/","title":"4.2 - Introduction to slurm","text":""},{"location":"level1/42_slurm_introduction/#42-introduction-to-slurm","title":"4.2 - Introduction to slurm","text":"<p>time</p> <ul> <li>Teaching: 20 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level1/42_slurm_introduction/#objectives","title":"Objectives","text":"<ul> <li>Understand the reason for using a job scheduler when working with HPCs</li> <li>Know the basic commands for starting and monitoring a <code>slurm</code> job</li> </ul>"},{"location":"level1/42_slurm_introduction/#introduction-to-slurm-scheduler-and-directives","title":"Introduction to slurm scheduler and directives","text":"<p>An HPC system might have thousands of nodes and thousands of users. How do we decide who gets what and when? How do we ensure that a task is run with the resources it needs? This job is handled by a special piece of software called the scheduler. On an HPC system, the scheduler manages which jobs run where and when.</p> ![image](../img/level1_31_scheduler_image.png){width=\"500\"} <p>In brief, scheduler is a mechanism to;</p> <ul> <li>Control access by many users to shared computing resources by queuing and scheduling of jobs</li> <li>Manage the reservation of resources and job execution on these resources </li> <li>Allows users to \"fire and forget\" large, long calculations or many jobs (\"production runs\")</li> </ul> <p>A bit more on why do we need a scheduler?</p> <ul> <li>To ensure the machine is utilised as fully as possible</li> <li>To ensure all users get a fair chance to use compute resources (demand usually exceeds supply)</li> <li>To track usage - for accounting and budget control</li> <li>To mediate access to other resources e.g. software licences</li> </ul> <p>There are several commonly used schedulers in HPC clusters around the world</p> <ul> <li>Slurm</li> <li>PBS, Torque</li> <li>Grid Engine</li> </ul> <p>All NeSI clusters use <code>slurm</code> (**S**imple **L**inux **U**tility for **R**esource **M**anagement) scheduler (or job submission system) to manage resources and how they are made available to users.</p> <p></p> <p>Researchers can not communicate directly to  Compute nodes from the login node. Only way to establish a connection OR send scripts to compute nodes is to use scheduler as the carrier/manager</p>"},{"location":"level1/42_slurm_introduction/#life-cycle-of-a-slurm-job","title":"Life cycle of a slurm job","text":"![image](../img/level1_42_batch_system_flow.png){width=\"1000\"}  <p>Commonly used <code>slurm</code> commands</p> Command Function <code>sbatch</code> Submit non-interactive (batch) jobs to the scheduler <code>squeue</code> List jobs in the queue <code>scancel</code> Cancel a job <code>sacct</code> Display accounting data for all jobs and job steps in the slurm job accounting log or Slurm database <code>srun</code> slurm directive for parallel computing <code>sinfo</code> Query the current state of nodes <code>salloc</code> Submit interactive jobs to the scheduler"},{"location":"level1/42_slurm_introduction/#anatomy-of-a-slurm-script-and-submitting-first-slurm-job","title":"Anatomy of a slurm script and submitting first slurm job \ud83e\uddd0","text":"<p>As with most other scheduler systems, job submission scripts in <code>slurm</code> consist of a header section with the shell specification and options to the submission command (<code>sbatch</code> in this case) followed by the body of the script that actually runs the commands you want. In the header section, options to <code>sbatch</code> should be prepended with <code>#SBATCH</code>.</p>  ![image](../img/level1_31_anatomy_of_a_slurm_script.png){width=\"700\"}  <p>Commented lines <code>#</code></p> <p>Commented lines are ignored by the bash interpreter, but they are not ignored by <code>slurm</code>. The <code>#SBATCH</code> parameters are read by <code>slurm</code> when we submit the job. When the job starts, the bash interpreter will ignore all lines starting with <code>#</code>.</p> <p>Similarly, the 'shebang' line is read by the system when you run your script. The program at the path iis used to interpret the script. In our case <code>/bin/bash</code> (the program <code>bash</code> found in the /bin directory).</p> <code>slurm</code> variables Header Example Description --job-name <code>#SBATCH --job-name MyJob</code> The name that will appear when using squeue or sacct --account <code>#SBATCH --account nesi12345</code> The account your core hours will be 'charged' to --time <code>#SBATCH --time DD-HH:MM:SS</code> Job max walltime --mem <code>#SBATCH --mem 512MB</code> Memory required per node --cpus-per-task <code>#SBATCH --cpus-per-task 10</code> Will request 10 logical CPUs per task --output <code>#SBATCH --output %j_output.out</code> Path and name of standard output file. <code>%j</code> will be replaced by the job ID --error <code>#SBATCH --error %j_error.out</code> Path and name of standard eror file. <code>%j</code> will be replaced by the job ID --mail-user <code>#SBATCH --mail-user=me23@gmail.com</code> address to send mail notifications --mail-type <code>#SBATCH --mail-type ALL</code> Will send a mail notification at BEGIN END FAIL STDOUT/STDERR from jobs <ul> <li>STDOUT - your process writes conventional output to this file handle</li> <li>STDERR - your process writes diagnostic output to this file handle.</li> </ul> <p>STDOUT and STDERR from jobs are, by default, written to a file called <code>slurm-JOBID.out</code> and <code>slurm-JOBID.err</code> in the working directory for the job (unless the job script changes this, this will be the directory where you submitted the job). So for a job with ID 12345 STDOUT and STDERR will be <code>slurm-12345.out</code> and <code>slurm-12345.err</code></p> <ul> <li>When things go wrong, first step of debugging starts with a referral to these files. </li> </ul>"},{"location":"level1/42_slurm_introduction/#monitoring-a-slurm-job-while-it-runs","title":"Monitoring a slurm job while it runs","text":"<p>Once your job has been submitted, you might be interested in seeing how it is progressing through the job queue. We can monitor the life of our jobs with two main commands, as indicated above.</p> <p>You can get a high-level view of all of your currently running jobs using the <code>squeue</code> command. By default this command shows all currently running jobs so is not very helpful. You can modify the command to report only your active jobs.</p> <p>code</p> <pre><code>squeue --me\n</code></pre> Output <pre><code>JOBID         USER       ACCOUNT   NAME        CPUS MIN_MEM PARTITI START_TIME     TIME_LEFT STATE    NODELIST(REASON)        \nNNNNNNNN      USERNAME   nesi03181 spawner-jupy   2      4G interac 2023-08-30T1     1:11:58 RUNNING  wbn004              \nNNNNNNNN      USERNAME   nesi03181 level1_blast  16     30G large   2023-08-30T1       18:05 RUNNING  wbn039 \n</code></pre> <p>This reveals some information about your currently running jobs. It tells you the resources allocated to the job (CPUs, memory) as well as when the job started running (if it has), and when it will time out.</p> <p>If you want more detail about a particular job, you can use the <code>sacct</code> command, along with the job ID which was given to you when you submitted your script to see more information.</p> <p>code</p> <pre><code>sacct -j NNNNNNNN\n</code></pre> Output <pre><code>JobID           JobName          Alloc     Elapsed     TotalCPU  ReqMem   MaxRSS State      \n--------------- ---------------- ----- ----------- ------------ ------- -------- ---------- \nNNNNNNNN        level1_blast        16    00:14:22     00:00:00     30G          RUNNING    \nNNNNNNNN.batch  batch               16    00:14:22     00:00:00                  RUNNING    \nNNNNNNNN.extern extern              16    00:14:22     00:00:00                  RUNNING \n</code></pre> <p>This reports any sub-jobs which were launched as part of your <code>slurm</code> request.</p>"},{"location":"level1/42_slurm_introduction/#creating-your-own-slurm-script-optional","title":"Creating your own slurm script (optional)","text":"<p>Since you were provided with a pre-written <code>slurm</code> script for the previous exercise, we will have a go at writting a new script from scratch while the <code>BLAST</code> job runs.</p> <p>Below is a abstract version of the <code>slurm</code> life cycle to assist you with the process</p> ![image](../img/level1_31_slurm_cycle_mini.png) Exercise <p>Create your own <code>slurm</code> script, which runs the following commands.</p> <pre><code>sleep 100\necho \"I am a slurm job and I slept for 100 seconds\"\n</code></pre> <p>You can either use a command-line text editor, such as <code>nano</code> to write your file or use the file explorer to create an empty file when write into it as in the previous exercise. Use the following settings:</p> <ul> <li>Use the account <code>nesi03181</code></li> <li>Set the job to run for 2 minutes</li> <li>Request 1 CPU, and 512MB of memory</li> </ul> Solution <pre><code>#!/bin/bash \n\n#SBATCH --job-name      myfirstslurmjob\n#SBATCH --account       nesi03181\n#SBATCH --time          00:02:00\n#SBATCH --cpus-per-task 1\n#SBATCH --mem           512MB\n#SBATCH --output        myfirstslurmjob.%j.out\n#SBATCH --error         myfirstslurmjob.%j.err\n\nsleep 100\necho \"I am a slurm job and I slept for 100 seconds\"\n</code></pre> Quick Check before compiling the script - Assigning values to <code>slurm</code> variables <p></p><p></p> <p></p><p></p>"},{"location":"level1/42_slurm_introduction/#the-importance-of-resource-utilisation-cpu-memory-time","title":"The importance of resource utilisation (cpu, memory, time)","text":"<p>Understanding the resources you have available and how to use them most efficiently is a vital skill in high performance computing. The three resources that every single job submitted on the platform needs to request are:</p> <ul> <li>CPUs (i.e. logical CPU cores), and</li> <li>Memory (RAM), and</li> <li>Time.</li> </ul> <p>Selecting the correct amount of resources is important to getting optimal job runs. Since <code>slurm</code> 'charges' your account for the resources you request when the job leaves the queue and starts to run, asking for more than needed results in;</p> <ol> <li>Jobs waiting in the queue for longer, as appropriate resource on the cluster must become available</li> <li>Drop in fairshare score, which determines job priority through usage</li> </ol> <p>On the other hand, asking for insufficienct resources can have the following consequences:</p> Resource Consequence Number of CPUs Job will run more slowly than expected, and so may run out time Memory Job will fail, probably with <code>OUT OF MEMORY</code> error, segmentation fault or bus error Wall time Job will run out of time and get aborted <p>How we optimise our requests for <code>slurm</code> jobs is beyond the scope of this training, but be aware of the trade offs when writing your own scripts.</p>"},{"location":"level1/43_blast_interpretation/","title":"4.3 - Interpreting BLAST results","text":""},{"location":"level1/43_blast_interpretation/#43-interpreting-blast-results","title":"4.3 - Interpreting BLAST results","text":"<p>time</p> <ul> <li>Teaching: 15 minutes</li> <li>Exercises: 15 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level1/43_blast_interpretation/#objectives","title":"Objectives","text":"<ul> <li>Learn how to interpret <code>BLAST</code> results </li> <li>Understand the meaning of identity, coverage, e-value, and bitscore metrics</li> </ul>"},{"location":"level1/43_blast_interpretation/#keypoints","title":"Keypoints","text":"<ul> <li>Interpreting the results of BLAST alignments requires thought and attention </li> </ul>"},{"location":"level1/43_blast_interpretation/#interpretting-the-results-of-blast-queries","title":"Interpretting the results of BLAST queries","text":"<p>It is important to remember, like most bioinformatics tools, <code>BLAST</code> has a specific job. In this case sequence alignment to a set of references. <code>BLAST</code> is really good at this job, but it does not offer interpretation of its alignments.</p> <p>Interpretation is completely up to the user on a case by case basis. It is therefore important to know your data and to understand the output metrics given by <code>BLAST</code> to help you make a biologically usefull interpretation of the results. </p> <p>There are typically four main metrics that we need ot check when reviewing a <code>BLAST</code> result:</p> <p>Key <code>BLAST</code> metrics</p> <p>Coverage</p> <ul> <li>This value tells us as a percentage how much of our query aligns with the database match. </li> <li>A small coverage value means only a small part of our sequence has matched. A perfect match would have a coverage of 100. </li> </ul> <p>Identity</p> <ul> <li>This represents the percent of bases which are identical between the query and the database hit, over the aligned region.</li> </ul> <p>E-value</p> <ul> <li>This is the number of hits equivalent to this hit that we would expect to see by chance alone.</li> <li>Smaller E-values represent better hits, but an exact E-value cut off needs to be decided on a case by case basis.</li> <li>E-values take into account the coverage and identity scores for each hit, and also the size of the database queried.</li> </ul> <p>Bit score </p> <ul> <li>Similarly to E-values, bit scores summarise the sequence similarity between the query and database hit.</li> <li>Bit scores are calculated independently from the query sequence length and the database size, as databases are constantly evolving this makes bit scores a constant statistical indicator.</li> <li>A higher bit score indicates a better hit. </li> </ul>"},{"location":"level1/43_blast_interpretation/#examining-our-file","title":"Examining our file","text":"<p>With this in mind lets look at the results from our <code>BLAST</code> job.</p> <p>Return to your <code>blast_annotation/</code> folder, if you left it, and examine the new output file. Take a look at your results using the <code>less</code> or <code>head</code> command.</p> <p>code</p> <pre><code>head output.txt\n</code></pre> Output <pre><code>seq1    gi|1607238104|dbj|AP019558.1|   91.750  1794    148     0       1       1794    366818  3650250.0      2494    Mycoplasma bovis KG4397 DNA, complete genome    28903\nseq1    gi|1441442372|gb|CP022588.1|    91.695  1794    149     0       1       1794    690905  6926980.0      2488    Mycoplasmopsis bovis strain MJ4 chromosome, complete genome     28903\nseq1    gi|1315670167|emb|LT578453.1|   91.695  1794    149     0       1       1794    655989  6577820.0      2488    Mycoplasma bovis isolate JF4278 genome assembly, chromosome: I  28903\nseq1    gi|2507795645|gb|CP058524.2|    91.695  1794    149     0       1       1794    401349  4031420.0      2488    Mycoplasmopsis bovis strain Mb49 chromosome     28903\nseq1    gi|2507793515|gb|CP058496.2|    91.695  1794    149     0       1       1794    320995  3192020.0      2488    Mycoplasmopsis bovis strain Mb222 chromosome    28903\nseq1    gi|2507792460|gb|CP058473.2|    91.695  1794    149     0       1       1794    1055619 10574120.0     2488    Mycoplasmopsis bovis strain VK22 chromosome     28903\nseq1    gi|2507791648|gb|CP058453.2|    91.695  1794    149     0       1       1794    9966    11759 0.0      2488    Mycoplasmopsis bovis strain Mb287 chromosome    28903\nseq1    gi|2507791648|gb|CP058453.2|    91.695  1794    149     0       1       1794    1120377 11221700.0     2488    Mycoplasmopsis bovis strain Mb287 chromosome    28903\nseq1    gi|2506302979|gb|CP058448.2|    91.695  1794    149     0       1       1794    763992  7657850.0      2488    Mycoplasmopsis bovis strain Mb1 chromosome      28903\nseq1    gi|2506302187|gb|CP058432.2|    91.695  1794    149     0       1       1794    354705  3564980.0      2488    Mycoplasmopsis bovis strain Mb216 chromosome    28903\n</code></pre> <p>It looks like a table, but awkwardly there are no column names. However, the names of the columns correspond to the values we provided in our <code>slurm</code> script at the start of this session.</p> Column header Meaning qseqid Sequence ID of the query sequence (input file) sseqid Sequence ID of the target sequence (reference database) pident Percentage of identical positions between query and target length Alignment length (sequence overlap) of the common region between query and target mismatch Number of mismatches between query and target gapopen Number of gap openings in the alignment qstart Position in the query sequence where alignment begins qend Position in the query sequence where alignment ends sstart Position in the target sequence where alignment begins send Position in the target sequence where alignment ends evalue The E-value for the query/target match, as described above bitscore The bit score for the query/target match, as described above salltitles Display All Subject Title(s) for the target sequence staxids Display the NCBI taxid value(s) for the target sequence <p>Applying these to the layout, we get something more sensible:</p> Table layout qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore salltitles staxids seq1 gi|1607238104|dbj|AP019558.1| 91.750 1794 148 0 1 1794 366818 365025 0.0 2494 Mycoplasma bovis KG4397 DNA, complete genome 28903 seq1 gi|1441442372|gb|CP022588.1| 91.695 1794 149 0 1 1794 690905 692698 0.0 2488 Mycoplasmopsis bovis strain MJ4 chromosome, complete genome 28903 seq1 gi|1315670167|emb|LT578453.1| 91.695 1794 149 0 1 1794 655989 657782 0.0 2488 Mycoplasma bovis isolate JF4278 genome assembly, chromosome: I 28903 seq1 gi|2507795645|gb|CP058524.2| 91.695 1794 149 0 1 1794 401349 403142 0.0 2488 Mycoplasmopsis bovis strain Mb49 chromosome 28903 seq1 gi|2507793515|gb|CP058496.2| 91.695 1794 149 0 1 1794 320995 319202 0.0 2488 Mycoplasmopsis bovis strain Mb222 chromosome 28903 seq1 gi|2507792460|gb|CP058473.2| 91.695 1794 149 0 1 1794 1055619 1057412 0.0 2488 Mycoplasmopsis bovis strain VK22 chromosome 28903 seq1 gi|2507791648|gb|CP058453.2| 91.695 1794 149 0 1 1794 9966 11759 0.0 2488 Mycoplasmopsis bovis strain Mb287 chromosome 28903 seq1 gi|2507791648|gb|CP058453.2| 91.695 1794 149 0 1 1794 1120377 1122170 0.0 2488 Mycoplasmopsis bovis strain Mb287 chromosome 28903 seq1 gi|2506302979|gb|CP058448.2| 91.695 1794 149 0 1 1794 763992 765785 0.0 2488 Mycoplasmopsis bovis strain Mb1 chromosome 28903 seq1 gi|2506302187|gb|CP058432.2| 91.695 1794 149 0 1 1794 354705 356498 0.0 2488 Mycoplasmopsis bovis strain Mb216 chromosome 28903 <p>Exercise</p> <p>Download the <code>output.txt</code> file do your computer and open it in <code>Excel</code>. Look through the sequence annotations, and determine the most likely organism that each sequence was obtained from.</p> <p>Are there are values in the results table that you are skeptical about? If so, raise them as a discussion with the group.</p> Solution <ul> <li>seq1 = Mycoplasma bovis</li> <li>seq2 = Bactrocera ritsemai</li> <li>seq3 = Pepino mosaic virus</li> <li>seq4 = No hit</li> <li>seq5 = Fusarium oxysporum (reversed)</li> </ul>"},{"location":"level2/11_shell_manipulation/","title":"1.1 - Manipulating files in the shell","text":""},{"location":"level2/11_shell_manipulation/#11-manipulating-files-in-the-shell","title":"1.1 - Manipulating files in the shell","text":""},{"location":"level2/11_shell_manipulation/#overview","title":"Overview","text":"<p>time</p> <ul> <li>Teaching: 15 minutes</li> <li>Exercises: 15 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/11_shell_manipulation/#objectives","title":"Objectives","text":"<ul> <li>Search the contents of basic text files for specific strings.</li> <li>Copy and remove directories, create tarballs and compress files.</li> </ul>"},{"location":"level2/11_shell_manipulation/#keypoints","title":"Keypoints","text":"<ul> <li>You can view search file contents using the <code>grep</code> command.</li> <li>The commands <code>cp</code> and <code>rm</code> can be applied to directories, with the correct parameters.</li> <li>You can perform find and replace operations in files using the <code>sed</code> command.</li> </ul>"},{"location":"level2/11_shell_manipulation/#searching-files-using-grep","title":"Searching files using <code>grep</code>","text":"<p>As powerful as <code>less</code> can be, at some point it becomes impractical to use it to screen documents as even with the search tools we are retrieving too many search hits, or covering too much content to reasonably interpret it by eye. For such situations, we can use another command line tool to search through documents without opening them and report the matches directory to the terminal. This tool is called <code>grep</code>.</p> <p><code>grep</code> is a command-line utility for searching plain-text files for lines matching a specific set of characters (sometimes called a string) or a particular pattern. We are going to work with one of the fastq files to practice using <code>grep</code> and to demonstrate some of the tool features.</p> <p>For these exercises we will searching some fastq files based on theri sequence content. As we are probably all aware, the four nucleotides that appear in DNA are abbreviated <code>A</code>, <code>C</code>, <code>T</code> and <code>G</code>. Using the IUPAC code, unknown nucleotides are represented with the letter <code>N</code>. An <code>N</code> appearing in a sequencing file represents a position where the sequencing machine was not able to confidently determine the nucleotide in that position.</p> <p>We'll search for strings inside of our fastq files. Let's first make sure we are in the correct directory:</p> <p>code</p> <pre><code>cd /nesi/project/nesi03181/phel/USERNAME/level2/shell_data/\n</code></pre> <p>Suppose we want to see how many reads in our file have really bad segments containing 10 consecutive unknown nucleotides (<code>N</code>s).</p> <p>Let's search for the string <code>NNNNNNNNNN</code> in the <code>SRR098026.fastq</code> file:</p> <p>code</p> <pre><code>grep NNNNNNNNNN SRR098026.fastq\n</code></pre> Output (last 10 lines) <pre><code>TNNNNNNNNNTAAAATAAANNNNNNNNNNNAANNN\nCNNNNNNNNNTTGGTGCTGNNNNNNNNNNNAANNN\nANNNNNNNNNAAAAAAAAANNNNNNNNNNNAANNN\nGNNNNNNNNNTGGCACAATNNNNNNNNNNNCGNNN\nTNNNNNNNNNCGTGGAATTNNNNNNNNNNNATNNN\nANNNNNNNNNGCATTAAACGNNNNNNNNNNCANTN\nGNNNNNNNNNATCAAAAAGCNNNNNNNNNNGTNAN\nANNNNNNNNNGTGGCAATATNNNNNNNNNNCCNGN\nANNNNNNNNNTTCAGCGACTNNNNNNNNNNGTNGN\nCNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n</code></pre> <p>This command returns a lot of output to the terminal. Every single line in the <code>SRR098026.fastq</code> file that contains at least 10 consecutive <code>N</code>s is printed to the terminal, regardless of how long or short the file is. We may be interested not only in the actual sequence which contains this string, but in the name (or identifier) of that sequence.</p> <p>We discussed in levle 1 training that the identifier line immediately precedes the nucleotide sequence for each read in a fastq file and that the quality scores associated with each of these reads is spread over the third and fourth line (see this description of the fastq format if you are unsure why we are using these numbers).</p> <p>We can use the <code>-B</code> argument for grep to return a specific number of lines before each match. The <code>-A</code> argument returns a specific number of lines after each matching line. Here we want the line before and the two lines after each matching line, so we add <code>-B1 -A2</code> to our grep command:</p> <p>code</p> <pre><code>grep -B1 -A2 NNNNNNNNNN SRR098026.fastq\n</code></pre> Output <pre><code>@SRR098026.177 HWUSI-EAS1599_1:2:1:1:2025 length=35\nCNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n+SRR098026.177 HWUSI-EAS1599_1:2:1:1:2025 length=35\n#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n</code></pre> <p>Exercise</p> <ol> <li>Search for the sequence <code>GNATNACCACTTCC</code> in the <code>SRR098026.fastq</code> file. Have your search return all matching lines and the name (or identifier) for each sequence that contains a match.</li> </ol> Solution <p>code</p> <pre><code>grep -B1 GNATNACCACTTCC SRR098026.fastq\n</code></pre> Output <pre><code>@SRR098026.245 HWUSI-EAS1599_1:2:1:2:801 length=35\nGNATNACCACTTCCAGTGCTGANNNNNNNGGGATG\n</code></pre> <ol> <li>Search for the sequence <code>AAGTT</code> in both fastq files (use <code>*.fastq</code> as your file name). Have your search return all matching lines and the name (or identifier) for each sequence that contains a match.</li> </ol> Solution <p>code</p> <pre><code>grep -B1 AAGTT *.fastq\n</code></pre> Output <pre><code>SRR097977.fastq-@SRR097977.11 209DTAAXX_Lenski2_1_7:8:3:247:351 length=36\nSRR097977.fastq:GATTGCTTTAATGAAAAAGTCATATAAGTTGCCATG\n--\nSRR097977.fastq-@SRR097977.67 209DTAAXX_Lenski2_1_7:8:3:544:566 length=36\nSRR097977.fastq:TTGTCCACGCTTTTCTATGTAAAGTTTATTTGCTTT\n--\nSRR097977.fastq-@SRR097977.68 209DTAAXX_Lenski2_1_7:8:3:724:110 length=36\nSRR097977.fastq:TGAAGCCTGCTTTTTTATACTAAGTTTGCATTATAA\n--\nSRR097977.fastq-@SRR097977.80 209DTAAXX_Lenski2_1_7:8:3:258:281 length=36\nSRR097977.fastq:GTGGCGCTGCTGCATAAGTTGGGTTATCAGGTCGTT\n--\nSRR097977.fastq-@SRR097977.92 209DTAAXX_Lenski2_1_7:8:3:353:318 length=36\nSRR097977.fastq:GGCAAAATGGTCCTCCAGCCAGGCCAGAAGCAAGTT\n--\nSRR097977.fastq-@SRR097977.139 209DTAAXX_Lenski2_1_7:8:3:703:655 length=36\nSRR097977.fastq:TTTATTTGTAAAGTTTTGTTGAAATAAGGGTTGTAA\n--\nSRR097977.fastq-@SRR097977.238 209DTAAXX_Lenski2_1_7:8:3:592:919 length=36\nSRR097977.fastq:TTCTTACCATCCTGAAGTTTTTTCATCTTCCCTGAT\n--\nSRR098026.fastq-@SRR098026.158 HWUSI-EAS1599_1:2:1:1:1505 length=35\nSRR098026.fastq:GNNNNNNNNCAAAGTTGATCNNNNNNNNNTGTGCG\n</code></pre>"},{"location":"level2/11_shell_manipulation/#copying-and-removing-folders-of-files","title":"Copying and removing folders of files","text":"<p>In the level 1 training we covered how to copy files and remove empty folders (directories), but it was noted that the copy operation would not work on folders and the <code>rmdir</code> command did not work on folders containing files.</p> <p>If you are in a position where you need to copy a directory of files to a new location, such as when creating backups this can be achieved by adding the <code>-r</code> (recursive) flag to the copy (<code>cp</code>) command:</p> <p>code</p> <pre><code>cp -r shell_data/ shell_data_backup/\n</code></pre> <p>In the level 1 training the way to remove the <code>shell_data_backup/</code> folder would be to navigate into the directory, remove all files, then navigate out of the directory and remove it with the <code>rmdir</code> command.</p> <p>If you are careful this can be compressed into a single command, by adding the <code>-r</code> flag to the remove (<code>rm</code>) command.</p> <p>code</p> <pre><code>rm -r shell_data_backup/\n</code></pre> <p>This is subject to the usual <code>bash</code> warning that contents removed in this way is not recoverable. If you try to perform this operation on a folder with contents which have had their write permissions removed you will receive a confirmation prompt for each file in the directory. This may or may no be a problem depending on the number of files that this applies to.</p> Setting file permissions <p>If it's that easy to permanently delete a file, how can be put some checks in place to prevent it from happening accidentally? The answer to this is through file permissions. In computer systems such as NeSI, the ability to read and write files is stored as metadata in each file. This can be viewed and modified using the appropriate commands.</p> <p>For example, you can run a modified version of the <code>ls</code> command to see the permissions of files:</p> <p>code</p> <pre><code>ls -l\n</code></pre> Output <pre><code>drwxrws---+ 2 dwaite comm00008  4096 Feb 24 15:50 backup\ndrwxrws---+ 2 dwaite comm00008  4096 Feb 24 15:50 other_backup\n-rw-rw-r--+ 1 dwaite comm00008 47552 Jun 25  2021 SRR097977.fastq\n-rw-rw-r--+ 1 dwaite comm00008 43332 Jun 25  2021 SRR098026.fastq\n</code></pre> <p>What we interested in is the first part of the output, the strings of characters which look like <code>-rw-rw-r--+</code>. This represents the current permission state of the file. If we ignore the first and last characters, what we are left with are 9 characters, which represent 3 file permissions for 3 different user groups, like so:</p> <p></p> <p>We're going to concentrate on the three positions that deal with your permissions (as the file owner), which will have the values <code>rw-</code>. This shows that you have permission to read (<code>r</code>) the file as well as edit the contents (write, <code>w</code>). The third position is set to <code>-</code>, indicating that you don't have permission to carry out the ability encoded by that space. This is the space where the ability to execute the file (<code>x</code>) is set.</p> <p>It is possible to modify these permissions, to expand the read/write access to other groups, or restrict the ability to read or write a file using the <code>chmod</code> command. However, we generally discourage the use of this within the PHEL environment as our data are a shared resource and restricting the ability of others to work with out data can lead to problems when handing over data related to diagnostic work.</p>"},{"location":"level2/11_shell_manipulation/#modifying-file-contents-using-sed","title":"Modifying file contents using <code>sed</code>","text":"<p>Sometimes making manual changes to text files is tedious, doubly so when working through command line text editors. Fortunately, there is a command line tool available which can do find-and-replace operations on files or data. The tool <code>sed</code> is used to make changes to the contents of files, either in place or by printing the modified contents to a new file.</p> <p>Navigate to the <code>shell_data/</code> folder, and inspect the contents of the file <code>SRR097977.small.fq</code>.</p> <p>code</p> <pre><code>cd /nesi/project/nesi03181/phel/USERNAME/level2/shell_data/\ncat SRR097977.small.fq\n</code></pre> Output <pre><code>@SRR097977.1 209DTAAXX_Lenski2_1_7:8:3:710:178 length=36\nTATTCTGCCATAATGAAATTCGCCACTTGTTAGTGT\n+SRR097977.1 209DTAAXX_Lenski2_1_7:8:3:710:178 length=36\nCCCCCCCCCCCCCCC&gt;CCCCC7CCCCCCACA?5A5&lt;\n@SRR097977.2 209DTAAXX_Lenski2_1_7:8:3:365:371 length=36\nGGTTACTCTTTTAACCTTGATGTTTCGACGCTGTAT\n+SRR097977.2 209DTAAXX_Lenski2_1_7:8:3:365:371 length=36\nCC:?:CC:?CCCCC??C?:?C-&amp;:C:,?&lt;&amp;*?+7?&lt;\n@SRR097977.3 209DTAAXX_Lenski2_1_7:8:3:663:569 length=36\nTTGTTCGCTTTTGGTAATTAATCCCGGAAATAATAA\n+SRR097977.3 209DTAAXX_Lenski2_1_7:8:3:663:569 length=36\nCCCCCCCCCCCC&amp;9AACCC,C&gt;CCAA&amp;0?4A9&amp;A&lt;6\n@SRR097977.4 209DTAAXX_Lenski2_1_7:8:3:715:205 length=36\nTATCACTAAAGATCAAATCATTGAAGCAGTTGCAGC\n+SRR097977.4 209DTAAXX_Lenski2_1_7:8:3:715:205 length=36\nCCCCCCC:CCC:CCC:CCC9CC??CCCC?0?*?1--\n@SRR097977.5 209DTAAXX_Lenski2_1_7:8:3:639:209 length=36\nTATCTATCAAAGCCAGGCAATGGAAGACCTACTCCC\n+SRR097977.5 209DTAAXX_Lenski2_1_7:8:3:639:209 length=36\nCCCCCCCCC?C?CC3C?CC5C?C1C&lt;?CC8AA+AA%\n</code></pre> <p>This is just the first 20 lines (5 sequences) from the <code>SRR097977.fastq</code> file - we are going to work with this today as we have not yet learned the appropriate commands for saving the output of command line operations in to files, so working with the full verison will be an information overload on the terminal.</p> <p>When running <code>sed</code>, there are two pieces of information we need - the file stream to be modified, and an expression telling the tool how to make the replacement. This expression is the most complicated part of working with <code>sed</code>, but it is a very powerful tool when mastered. For this exercise we are going to just change a few pieces of information in the sequence headers just to get a feel for how the tool operates.</p> <p>As a worked example, take a look at the following command:</p> <p>code</p> <pre><code>sed \"s/A/B/\" SRR097977.small.fq\n</code></pre> Output <pre><code>@SRR097977.1 209DTBAXX_Lenski2_1_7:8:3:710:178 length=36\nTBTTCTGCCATAATGAAATTCGCCACTTGTTAGTGT\n+SRR097977.1 209DTBAXX_Lenski2_1_7:8:3:710:178 length=36\nCCCCCCCCCCCCCCC&gt;CCCCC7CCCCCCBCA?5A5&lt;\n@SRR097977.2 209DTBAXX_Lenski2_1_7:8:3:365:371 length=36\nGGTTBCTCTTTTAACCTTGATGTTTCGACGCTGTAT\n+SRR097977.2 209DTBAXX_Lenski2_1_7:8:3:365:371 length=36\nCC:?:CC:?CCCCC??C?:?C-&amp;:C:,?&lt;&amp;*?+7?&lt;\n@SRR097977.3 209DTBAXX_Lenski2_1_7:8:3:663:569 length=36\nTTGTTCGCTTTTGGTBATTAATCCCGGAAATAATAA\n+SRR097977.3 209DTBAXX_Lenski2_1_7:8:3:663:569 length=36\nCCCCCCCCCCCC&amp;9BACCC,C&gt;CCAA&amp;0?4A9&amp;A&lt;6\n@SRR097977.4 209DTBAXX_Lenski2_1_7:8:3:715:205 length=36\nTBTCACTAAAGATCAAATCATTGAAGCAGTTGCAGC\n+SRR097977.4 209DTBAXX_Lenski2_1_7:8:3:715:205 length=36\nCCCCCCC:CCC:CCC:CCC9CC??CCCC?0?*?1--\n@SRR097977.5 209DTBAXX_Lenski2_1_7:8:3:639:209 length=36\nTBTCTATCAAAGCCAGGCAATGGAAGACCTACTCCC\n+SRR097977.5 209DTBAXX_Lenski2_1_7:8:3:639:209 length=36\nCCCCCCCCC?C?CC3C?CC5C?C1C&lt;?CC8BA+AA%\n</code></pre> <p>In that command, there should be two obvious pieces of information - the name of the <code>sed</code> tool, and the name of the file we are working with. The remaining part, enclosed by quotation marks, is the expression for what to change. This takes the following form;</p> <p>code</p> <pre><code>s/TEXT_TO_BE_REPLACED/TEXT_TO_REPLACE_WITH/\n</code></pre> <p>The <code>/</code> characters are used to separate the <code>s</code> flag, and the find/replace values. you can use any character here as long as they are used consistently.</p> <p>For example, the following three commands all create the same output:</p> <p>code</p> <pre><code>sed \"s/A/B/\" SRR097977.small.fq\nsed \"s|A|B|\" SRR097977.small.fq\nsed \"s,A,B,\" SRR097977.small.fq\n</code></pre> <p>The <code>/</code> character is typically used in online examples, but it can be useful to know it can be replaced if you find the notation confusing or are ever in a situation when you need to find and replace a <code>/</code> character.</p> <p>If you run any of these, you will probably spot that even though we are specifying that the <code>A</code> character be replaced with <code>B</code>, there are plenty of <code>A</code>s in each line of the output. By default, <code>sed</code> only replaces the first instance of the text it is searching for. We can modify the expression to include all instances by making it the following:</p> <p>code</p> <pre><code>sed \"s/A/B/g\" SRR097977.small.fq\n</code></pre> Output <pre><code>@SRR097977.1 209DTBBXX_Lenski2_1_7:8:3:710:178 length=36\nTBTTCTGCCBTBBTGBBBTTCGCCBCTTGTTBGTGT\n+SRR097977.1 209DTBBXX_Lenski2_1_7:8:3:710:178 length=36\nCCCCCCCCCCCCCCC&gt;CCCCC7CCCCCCBCB?5B5&lt;\n@SRR097977.2 209DTBBXX_Lenski2_1_7:8:3:365:371 length=36\nGGTTBCTCTTTTBBCCTTGBTGTTTCGBCGCTGTBT\n+SRR097977.2 209DTBBXX_Lenski2_1_7:8:3:365:371 length=36\nCC:?:CC:?CCCCC??C?:?C-&amp;:C:,?&lt;&amp;*?+7?&lt;\n@SRR097977.3 209DTBBXX_Lenski2_1_7:8:3:663:569 length=36\nTTGTTCGCTTTTGGTBBTTBBTCCCGGBBBTBBTBB\n+SRR097977.3 209DTBBXX_Lenski2_1_7:8:3:663:569 length=36\nCCCCCCCCCCCC&amp;9BBCCC,C&gt;CCBB&amp;0?4B9&amp;B&lt;6\n@SRR097977.4 209DTBBXX_Lenski2_1_7:8:3:715:205 length=36\nTBTCBCTBBBGBTCBBBTCBTTGBBGCBGTTGCBGC\n+SRR097977.4 209DTBBXX_Lenski2_1_7:8:3:715:205 length=36\nCCCCCCC:CCC:CCC:CCC9CC??CCCC?0?*?1--\n@SRR097977.5 209DTBBXX_Lenski2_1_7:8:3:639:209 length=36\nTBTCTBTCBBBGCCBGGCBBTGGBBGBCCTBCTCCC\n+SRR097977.5 209DTBBXX_Lenski2_1_7:8:3:639:209 length=36\nCCCCCCCCC?C?CC3C?CC5C?C1C&lt;?CC8BB+BB%\n</code></pre> <p>Exercise</p> <ol> <li>Create a <code>sed</code> expression to replace the <code>SRR097977</code> text in each line with your name in the <code>SRR097977.small.fq</code> file.</li> </ol> Solution <p>code</p> <pre><code>sed \"s/SRR097977/Bob/\" SRR097977.small.fq\n</code></pre> <ol> <li>Create a <code>sed</code> expression to replace the <code>length=36</code> text with nothing.</li> </ol> Solution <p>code</p> <pre><code>sed \"s/ length=36//\" SRR097977.small.fq\n</code></pre>"},{"location":"level2/12_shell_variables/","title":"1.2 - Working with variables and loops","text":""},{"location":"level2/12_shell_variables/#12-working-with-variables-and-loops","title":"1.2 - Working with variables and loops","text":""},{"location":"level2/12_shell_variables/#overview","title":"Overview","text":"<p>time</p> <ul> <li>Teaching: 15 minutes</li> <li>Exercises: 30 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/12_shell_variables/#objectives","title":"Objectives","text":"<ul> <li>Understand how to declare and print variables in the <code>bash</code> environment.</li> <li>Create a simple loop that performs an operation on a set of files or variables.</li> </ul>"},{"location":"level2/12_shell_variables/#keypoints","title":"Keypoints","text":"<ul> <li>Variables are used when we require the computer to temporarily store a piece of information, particularly in the value of the information can change over time.</li> <li><code>for</code> loops can be used to automatically repeat a command, or set of commands, over a set of input values or files.</li> </ul>"},{"location":"level2/12_shell_variables/#what-are-variables-and-why-do-we-need-them","title":"What are variables and why do we need them","text":"<p>Variables are one of the most fundamental aspects of writing code and scripts. Before explaining what one is, think back to our previous exercises in <code>bash</code> when we were searching for particular nucleotide sequences in a fastq file using the <code>grep</code> tool (here). The commands we ran were perfectly fine, but would only ever work for that one nucleotide sequence in that one fastq file.</p> <p>If we wanted to search the same file for a different nucleotide pattern, or search multiple files for the same pattern we would have to rewrite the command and change part of the statement to the new requirement. For simple commands, or commands which we only need to perform once, this is fine but as you rely more and more on the command line it's really helpful to outsource as much work as possible to the command line rather then doing it all yourself.</p> <p>This is the fundamental idea of variables - we are getting the computer to store a piece of information (file name, sqeuence motif, number) which may change over time to be used or reused over successive commands. What's important about this is that unlike the <code>grep</code> examples in the previous exercise, over time the content that the variable represents may change. If we revisit the command from the previous exercise:</p> <p>code</p> <pre><code>cd /nesi/project/nesi03181/phel/USERNAME/level2/shell_data/\ngrep NNNNNNNNNN SRR098026.fastq\n</code></pre> Output (last 10 lines) <pre><code>TNNNNNNNNNTAAAATAAANNNNNNNNNNNAANNN\nCNNNNNNNNNTTGGTGCTGNNNNNNNNNNNAANNN\nANNNNNNNNNAAAAAAAAANNNNNNNNNNNAANNN\nGNNNNNNNNNTGGCACAATNNNNNNNNNNNCGNNN\nTNNNNNNNNNCGTGGAATTNNNNNNNNNNNATNNN\nANNNNNNNNNGCATTAAACGNNNNNNNNNNCANTN\nGNNNNNNNNNATCAAAAAGCNNNNNNNNNNGTNAN\nANNNNNNNNNGTGGCAATATNNNNNNNNNNCCNGN\nANNNNNNNNNTTCAGCGACTNNNNNNNNNNGTNGN\nCNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n</code></pre> <p>We could run this command a thousand times and it would always return the same result. If we were instead to create a variable which represented this sequence, the value that it represents could be changed over time, giving a different result each time the command was run. Without worry about the syntax for declaring or accessing the variable (which we will cover in the sections below), consider the following code:</p> <p>code</p> <pre><code>MOTIF=\"NNNNNNNNNN\"\ngrep ${MOTIF} SRR098026.fastq\n</code></pre> Output (last 10 lines) <pre><code>TNNNNNNNNNTAAAATAAANNNNNNNNNNNAANNN\nCNNNNNNNNNTTGGTGCTGNNNNNNNNNNNAANNN\nANNNNNNNNNAAAAAAAAANNNNNNNNNNNAANNN\nGNNNNNNNNNTGGCACAATNNNNNNNNNNNCGNNN\nTNNNNNNNNNCGTGGAATTNNNNNNNNNNNATNNN\nANNNNNNNNNGCATTAAACGNNNNNNNNNNCANTN\nGNNNNNNNNNATCAAAAAGCNNNNNNNNNNGTNAN\nANNNNNNNNNGTGGCAATATNNNNNNNNNNCCNGN\nANNNNNNNNNTTCAGCGACTNNNNNNNNNNGTNGN\nCNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN\n</code></pre> <p>While that statement may remain untouched and be reused over time, the line contains a variable called <code>MOTIF</code> which represents a dynamic value. This value might change over time, such that the same <code>grep</code> command will return different results. For example:</p> <p>code</p> <pre><code>MOTIF=\"NNNNNNNNNN\"\ngrep ${MOTIF} SRR098026.fastq\n\nMOTIF=\"ATCGATCGAT\"\ngrep ${MOTIF} SRR098026.fastq\n\nMOTIF=\"TTTTTTTTTT\"\ngrep ${MOTIF} SRR098026.fastq\n</code></pre> <p>Why we need this technique can seem a little bit abstract at this stage but think of it like this - when you are performing routine diagnostic work, you will refer to the appropriate SOP detailing how a particular test is performed. If it is a PCR, for example, the SOP will detail the exact primer sequences to use, which chemicals to include in the reaction, the cycling conditions (temperatures, durations, and number of cycles).</p> <p>However, the SOP does not tell you the name of the sample you are handling or what host it comes from. This is obviously because the SOP does not know specifically which sample the diagnostician is going to be working with but the instructions are applicable across many different samples. Using variables allows us to achieve this effect in a coding environment. We can have specific, unchanging (hardcoded) rules written out as above, but then use variables as placeholder values which can be changed on a per-sample basis.</p>"},{"location":"level2/12_shell_variables/#declaring-variables-and-accessing-their-values","title":"Declaring variables and accessing their values","text":"<p>The code examples above declared a variable called <code>MOTIF</code> and assigned several pieces of information to it over time. We're now going to dive into that process in more detail. To begin with, we'll just copy the example above and create a variable named <code>MOTIF</code> and assign the text 'NNNNNNNNNN' to it. Type the following into the command line:</p> <p>code</p> <pre><code>MOTIF=\"NNNNNNNNNN\"\n</code></pre> <p>And it's done. You have now created a variable named 'MOTIF'. This variable represents a location in the computers memory, at which the text 'NNNNNNNNNN' is stored. If we want to view the content of the 'MOTIF' variable we can use the <code>echo</code> command.</p> <p>code</p> <pre><code>echo MOTIF\n</code></pre> Output <pre><code>MOTIF\n</code></pre> <p>What do you notice when you run this command. You see the word 'MOTIF' (the name of the variable) and not the value 'NNNNNNNNNN' (the value it is meant to contain) printed to the console. This happens because when working from the command line by default everything we enter into the terminal is considered a literal instruction to the computer. There is a specific notation required when we need to computer to understand that we are using a variable:</p> <p>code</p> <pre><code>echo ${MOTIF}\n</code></pre> Output <pre><code>NNNNNNNNNN\n</code></pre> <p>In this case, the value of the variable <code>MOTIF</code> is correctly accessed by the computer, returning the nucleotide motif value instead of the word 'MOTIF'.</p> <p>Notes on naming variables</p> <p>In these examples we are using uppercase words to represent our variable names but this is not neccessary. <code>bash</code> will accept any alphabetical letter (upper or lower case) as well as underscore characters in a variable name. This means that you can name your variables as a single letter, word, or a series of words joined together or separated by underscores with any casing. All of the following examples are valid (from the computer's point of view).</p> <p>code</p> <pre><code>m=\"NNNNNNNNNN\"\nM=\"NNNNNNNNNN\"\nmotif=\"NNNNNNNNNN\"\nMOTIF=\"NNNNNNNNNN\"\nmOtIf=\"NNNNNNNNNN\"\nTHE_NUCLEOTIDE_MOTIF_I_WANT_TO_USE_IN_MY_BASH_SCRIPT=\"NNNNNNNNNN\"\nThe_Nucleotide_Motif_I_Want_To_Use_In_My_Bash_Script=\"NNNNNNNNNN\"\nTheNucleotideMotifIWantToUseInMyBashScript=\"NNNNNNNNNN\"\n</code></pre> <p>The most important thing is to make sure that your variables are informative when the code is read, and easy to distinguish from each other. Variable names are case sensitive so if you are choosing to use mixed case you must make sure you use the same case when recalling the variable. Also, there is a trade off between making variable names long enough to be informative and so long that they become hard to read and type.</p> <p>Generally speaking, it is best to keep your varaibles very narrow in scope - that is, you might have a set of variables used in a particular block of code but if you move to a new block you create fresh variables for that task. Keeping variables specific to a purpose reduces the need to complicated naming schemes and avoids issues which might arise when you accidentally access the wrong variable.</p> <p>Notes on accessing variables</p> <p>When looking online for examples of variables being used, you might notice a few different styles used to call to a variable, such as</p> <p>code</p> <pre><code>echo $MOTIF\necho ${MOTIF}\necho \"${MOTIF}\"\n</code></pre> <p>Technically these all work, but there are subtle differences in how they are interpretted by the computer executing the command. In the first value, everything that follows the $ symbol is interpreted as part of a variable name until some character is encountered to break the name. This will typically be either a space or a quotation mark. In the second example, only the name enclosed by the { and } symbols is considered the variable name.    </p> <p>In these examples it makes no difference but there are sometimes cases where we need to use this notation to explicitly tell the computer where the variable name begins and ends rather than let it work this out itself. The third example is useful if we are trying to access a value with space characters inside it. As we have seen in our examples to date, the <code>bash</code> environment uses spaces to separate terms in a command, for example: </p> <p>code</p> <pre><code>grep NNNNNNNNNN SRR098026.fastq\n|    |          |\n|    |          |\n|    |          Third term (file to search)\n|    Second term (search text)\nFirst term (command)\n</code></pre> <p>If for some reason we were searching for a piece of text with a space in it, like a binomial species name, the space between the genus and species name would be incorrectly interpreted as the bounds between terms, like so:  </p> <p>code</p> <pre><code>SPECIES=\"Escherichia coli\"\ngrep ${SPECIES} target_document.txt\n</code></pre> <p>Interpretation</p> <pre><code>grep Escherichia coli target_document.txt\n|    |           |    |\n|    |           |    |\n|    |           |    Fourth term (another file to search)\n|    |           Third term (file to search)\n|    Second term (search text)\nFirst term (command)\n</code></pre> <p>In this instance, wrapping the variable name in quotation marks will expand the command differently:</p> <p>code</p> <pre><code>grep \"${SPECIES}\" target_document.txt\n</code></pre> <p>Interpretation</p> <pre><code>grep \"Escherichia coli\" target_document.txt\n|    |                  |\n|    |                  |\n|    |                  Third term (file to search)\n|    Second term (search text)\nFirst term (command)\n</code></pre> <p>In the <code>bash</code> environment, variables persist until you reset your session. This means that when you log out and log back in, any variables you declare are gone. This is actually a really helpful feature as we do not end up with an ever-expanding list of variables to remember.</p> <p>We can also overwrite a variable at any time, simply by making a new assignment statement:</p> <p>code</p> <pre><code>MOTIF=\"NNNNNNNNNN\"\nMOTIF=\"ATCGATCGAT\"\nMOTIF=\"TTTTTTTTTT\"\n</code></pre> <p>Finally, if you ever try to access a variable which does not exist, <code>bash</code> will return a piece of text with no characters in it. This is helpful in that it means scripts don't outright crash if you use the wrong varaible name, but unfortunately they also won't do what you intended.</p> <p>code</p> <pre><code>echo \"The value of MOTIF is '${MOTIF}'\"\necho \"The value a nonexistant variable is '${NON_EXISTANT_VARIABLE}'\"\n</code></pre> Output <pre><code>The value of MOTIF is 'TTTTTTTTTT'\nThe value a nonexistant variable is ''\n</code></pre>"},{"location":"level2/12_shell_variables/#writing-loops-using-variables","title":"Writing loops using variables","text":"<p>Loops are key to productivity improvements through automation as they allow us to write commands with repeat themselves a given number of times. This allows us to write a basic set of commands once and then let the computer apply the command(s) to some pre-defined set of values instead of typing out each iteration ourselves. This reduces the amount of typing we need to perform which among other things, reduces our likelihood of making errors.</p> <p>A loop essentially acts as a wrapper around a block of code, taking a list of values and executing the code on each element of the list. They are invaluable when performing the same operation on groups of files, such as compressing or performing quality trimming. </p> <p>When working with loops, we must make use of variables to identify pieces on information in the command which change with each iteration of the looped code block. We will use the example of <code>MOTIF</code> above to create a simple loop which searches through a file multiple times, looking for a different nucleotide sequence each time. The base command we will be using is:</p> <p>code</p> <pre><code>grep -c ${MOTIF} SRR098026.fastq\n</code></pre> Output <pre><code>0\n</code></pre> <p>This differs a little bit from the previous <code>grep</code> command. By adding the <code>-c</code> flag, we are making <code>grep</code> print the number of times the search sequence is counted in the file, rather than print the lines which contain the sequence. In practice this can be a useful feature in itself but we are mainly using it to keep the text printed by <code>grep</code> minimal.</p> <p>The first loop we will write looks like this:</p> <p>code</p> <pre><code>for MOTIF in \"NNNNNNNN\" \"TGTTACAG\" \"CTCAAACC\";\ndo\n    grep -c ${MOTIF} SRR098026.fastq\ndone\n</code></pre> Output <pre><code>223\n0\n0\n</code></pre> <p>Let's look at the what each line and statement actually mean. The loop above can be broken down ito the following model:</p> <p>code</p> <pre><code>for {VARIABLE} in {LIST OF VALUES};\ndo\n    {code block}\ndone\n</code></pre> <p>The first line declares that we are running a <code>for</code> loop, which is one of the two major types of loops and the one most commonly used in command line scripting. Each time the loop runs (called an iteration), the next value in the list of values is assigned to the variable name then commands inside the loop are executed using that instance of the variable. Once the commands have executed, the value of <code>VARIABLE</code> is updated to the next value in the list and the process repeats.</p> <p>The <code>do</code> and <code>done</code> words denote the start and end of the code block to be executed. For each value assigned to <code>VARIABLE</code> the commands between these keywords are executed in order, then once the <code>done</code> keyword is encountered the value of <code>VARIABLE</code> moves to the next value in the list.</p> <p>Let's start writing the loop, going line by line. Once you have entered the first line of the loop you should notice that the shell prompt changes from <code>$</code> to <code>&gt;</code>. This change is to show us that we are currently writing an extension of the previous line and that the code will not execute if we were to press Enter. Only once you complete the loop declaration with the <code>done</code> keyword will the console execute the loop and return the prompt to the standard <code>$</code> value.</p> <p>Note</p> <p>It's really easy to get stuck in a loop declaration if you forget to close off your loop block, or close any quotation marks in your code. At any time you can cancel your current command using Ctrl+C. This is helpful if you get stuck in a badly written loop, or notice an obvious error in a previous line which is going to prevent your loop for executing correctly.</p> <p>Exercise</p> <p>Extend the loop above so that the value of the <code>${MOTIF}</code> variable is also printed to the console, so that we can track which nucleotide sequence each number corresponds to.</p> Solution <p>code</p> <pre><code>for MOTIF in \"NNNNNNNNNN\" \"GCTGGCGNNN\" \"TTTTTTTTTT\";\ndo\n    echo ${MOTIF}\n    grep -c ${MOTIF} SRR098026.fastq\ndone\n</code></pre> Output <pre><code>NNNNNNNN\n223\nTGTTACAG\n0\nCTCAAACC\n0\n</code></pre>"},{"location":"level2/12_shell_variables/#extending-our-loop-with-multiple-variables","title":"Extending our loop with multiple variables","text":"<p>Our loop is now functional, but it is quite narrow in scope. We're now going to try to expand it a little bit, so that it can be pointed to a file defined by a second variable. First up, we need to declare a variable that holds the value of one of the file names.</p> <p>code</p> <pre><code>FILENAME=\"SRR098026.fastq\"\n</code></pre> <p>Exercise</p> <p>Modify the loop code to point towards the value of <code>FILENAME</code> instead of the text <code>SRR098026.fastq</code>. Since it's not necesarily clear which file will be searched for these nucleotide sequences, also modify the loop to report the name of the file being searched, as well as the nucleotide sequence and its number of occurences in the input file</p> Solution <p>code</p> <pre><code>for MOTIF in \"NNNNNNNNNN\" \"GCTGGCGNNN\" \"TTTTTTTTTT\";\ndo\n    echo ${FILENAME}\", nucleotide \"${MOTIF}\n    grep -c ${MOTIF} ${FILENAME}\ndone\n</code></pre> Output <pre><code>SRR098026.fastq, nucleotide NNNNNNNNNN\n134\nSRR098026.fastq, nucleotide GCTGGCGNNN\n1\nSRR098026.fastq, nucleotide TTTTTTTTTT\n0\n</code></pre> <p>If your code works as expected, you can now change the value of <code>FILENAME</code> to any valid file and it will provide the equivalent searches in the next file. Confirm this by changing your <code>FILENAME</code> value and repeating the loop.</p> <p>code</p> <pre><code>FILENAME=SRR098026.fastq\nfor MOTIF in \"NNNNNNNNNN\" \"GCTGGCGNNN\" \"TTTTTTTTTT\";\ndo\n    echo ${FILENAME}\", nucleotide \"${MOTIF}\n    grep -c ${MOTIF} ${FILENAME}\ndone\n\nFILENAME=SRR097977.fastq\nfor MOTIF in \"NNNNNNNNNN\" \"GCTGGCGNNN\" \"TTTTTTTTTT\";\ndo\n    echo ${FILENAME}\", nucleotide \"${MOTIF}\n    grep -c ${MOTIF} ${FILENAME}\ndone\n</code></pre> Output <pre><code>SRR098026.fastq, nucleotide NNNNNNNNNN\n134\nSRR098026.fastq, nucleotide GCTGGCGNNN\n1\nSRR098026.fastq, nucleotide TTTTTTTTTT\n0\n\nSRR097977.fastq, nucleotide NNNNNNNNNN\n0\nSRR097977.fastq, nucleotide GCTGGCGNNN\n0\nSRR097977.fastq, nucleotide TTTTTTTTTT\n0\n</code></pre> <p>This is functional, but still quite messy. As a final exercise, we're going to write what is called a 'nested loop', a <code>for</code> loop within a <code>for</code> loop. The first (outer) loop will iterate through a set of fastq files and assign them to <code>FILENAME</code> and the second (inner) loop will perform the <code>grep</code> commands.</p> <p>We can make use of a wildcard here to automatically pick up the names of all files which match a naming convention. In this case, by using the term <code>*.fastq</code>, the <code>bash</code> interpreter will read the directory and create a list of all files which end with the extension <code>.fastq</code>.</p> <p>code</p> <pre><code>for FILENAME in *.fastq;\ndo\n    echo \"File: \"${FILENAME}\n    for MOTIF in \"NNNNNNNNNN\" \"GCTGGCGNNN\" \"TTTTTTTTTT\";\n    do\n        echo \"Nucleotide: \"${MOTIF}\n        grep -c ${MOTIF} ${FILENAME}\n    done\ndone\n</code></pre> Output <pre><code>File: SRR097977.fastq\nNucleotide: NNNNNNNNNN\n0\nNucleotide: GCTGGCGNNN\n0\nNucleotide: TTTTTTTTTT\n0\nFile: SRR098026.fastq\nNucleotide: NNNNNNNNNN\n134\nNucleotide: GCTGGCGNNN\n1\nNucleotide: TTTTTTTTTT\n0\n</code></pre>"},{"location":"level2/13_shell_redirection/","title":"1.3 - Working with redirection","text":""},{"location":"level2/13_shell_redirection/#13-working-with-redirection","title":"1.3 - Working with redirection","text":""},{"location":"level2/13_shell_redirection/#overview","title":"Overview","text":"<p>time</p> <ul> <li>Teaching: 15 minutes</li> <li>Exercises: 15 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/13_shell_redirection/#objectives","title":"Objectives","text":"<ul> <li>Understand the basic idea or shell redirection and what can be done with it.</li> <li>Understand the differences between redirecting to a file and another command.</li> <li>Know that different command line tools can be linked together to produce complex operations.</li> </ul>"},{"location":"level2/13_shell_redirection/#keypoints","title":"Keypoints","text":"<ul> <li>When a command on the shell is printing information to the terminal, we can capture and redirect that information to a new location or into a new command.</li> <li>When redirecting to a file, we can create a new file, overwrite an existing file, or append to an existing file.</li> <li>Each channel can be independently captured and directed to a new location. SUch locations include:</li> </ul>"},{"location":"level2/13_shell_redirection/#redirecting-output-to-a-new-file","title":"Redirecting output to a new file","text":"<p>In the previous session we were using the <code>grep</code> command allowed us to identify sequences in fastq files that matched particular patterns. All of these sequences were printed to our terminal screen which wasn't particularly helpful so what we're now going to do is learn how to capture that output in a more useful manner.</p> <p>We can do this with something called redirection. The idea is that we are taking what would ordinarily be printed to the terminal screen and redirecting it to another location. In our case, we want to move the information printed on our terminal into a file so that we can look at it later.</p> <p>Navigate to the <code>redirection/</code> folder and we'll get started.</p> <p>code</p> <pre><code>cd /nesi/project/nesi03181/phel/USERNAME/level2/redirection/\n</code></pre> <p>The command for redirecting output to a file is &gt;. Let's revisit our example using the <code>${MOTIF}</code> variable to represent the sequence motif we want to capture and we'll redirect the output of the search into a new file.</p> <p>code</p> <pre><code>MOTIF=\"NNNNNNNNNN\"\ngrep ${MOTIF} SRR098026.fastq &gt; my_file.txt\n</code></pre> <p>This might take a second or two to complete, but when it is done a quick <code>ls</code> should show you a new file called <code>my_file.txt</code>. This file did not exist before, but the &gt; symbol when performing redirection is understood by the computer to mean that a new file must be created with the name corresponding to the text which follows the &gt; character.</p> <p>Overwriting files</p> <p>If you redirect into an existing file using this method, the old file contents will be replaced with the new values. You will lose the data permanently so make sure you are redirecting data to the correct place.</p> <p>File extensions</p> <p>You might notice that we have used the file extension <code>.txt</code> in the output file rather than <code>.fastq</code>. This is largely semantics, in the command line environment the extensions to files generally have no meaning to the computer, they are for the user to help identify the expected contents of the file in question.</p> <p>In this instance, we are using a different extension as the <code>grep</code> command will only extract the lines (sequences) with the <code>${MOTIF}</code> value and not the sequence name or quality information so it will no longer be recognised as a <code>fastq</code> file.</p> <p>Comparing the contents of the input and output files from the <code>grep</code> command reveals some differences:</p> First 10 lines of <code>SRR098026.fastq</code> <pre><code>@SRR098026.1 HWUSI-EAS1599_1:2:1:0:968 length=35\nNNNNNNNNNNNNNNNNCNNNNNNNNNNNNNNNNNN\n+SRR098026.1 HWUSI-EAS1599_1:2:1:0:968 length=35\n!!!!!!!!!!!!!!!!#!!!!!!!!!!!!!!!!!!\n@SRR098026.2 HWUSI-EAS1599_1:2:1:0:312 length=35\nNNNNNNNNNNNNNNNNANNNNNNNNNNNNNNNNNN\n+SRR098026.2 HWUSI-EAS1599_1:2:1:0:312 length=35\n!!!!!!!!!!!!!!!!#!!!!!!!!!!!!!!!!!!\n@SRR098026.3 HWUSI-EAS1599_1:2:1:0:570 length=35\nNNNNNNNNNNNNNNNNANNNNNNNNNNNNNNNNNN\n</code></pre> First 10 lines of <code>my_file.txt</code> <pre><code>NNNNNNNNNNNNNNNNCNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNANNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNANNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNANNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNGNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNGNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNGNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNANNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNGNNNNNNNNNNNNNNNNNN\nNNNNNNNNNNNNNNNNGNNNNNNNNNNNNNNNNNN\n</code></pre>"},{"location":"level2/13_shell_redirection/#applying-redirection-to-loops","title":"Applying redirection to loops","text":"<p>We previously created a dynamic loop that searched a single, hardcoded file and printed the results of a <code>grep</code> call. We could use what we have seen above to redirect the outputs of this search to a file, but is this a good idea?</p> <p>Exercise</p> <p>Consider this revised version of the <code>for</code> loop from the previous session (notes here), which loops through a number of values for <code>${MOTIF}</code> and writes the results to a file using redirection.</p> <p>code</p> <pre><code>FILENAME=SRR098026.fastq\nfor MOTIF in \"NNNNNNNNNN\" \"GCTGGCGNNN\" \"TTTTTTTTTT\";\ndo\n    grep -B1 -A2 ${MOTIF} ${FILENAME} &gt; my_file.fastq\ndone\n</code></pre> <p>After the loop completes and the <code>grep</code> search has run three times, what are the final contents of the output file and why?</p> <ol> <li>The results from all <code>grep</code> searches.</li> <li>The results from the first <code>grep</code> search.</li> <li>The results from the second <code>grep</code> search.</li> <li>The results from the last <code>grep</code> search.</li> </ol> Solution <ol> <li>Ony the results from the last search will be shown in <code>my_file.fastq</code>, as each iteration of the loop overwrites the contents of the file using the <code>&gt;</code> operator.</li> </ol> <p>This is a problem because we may need to record the results of multiple searches, or even search multiple <code>fastq</code> files and report each set of results independently. There are two ways to deal with this and the best option depends on your requirements.</p>"},{"location":"level2/13_shell_redirection/#using-appending-redirection","title":"Using appending redirection","text":"<p>The first modification we can make is to replace the redirection operator <code>&gt;</code> with the appending operator <code>&gt;&gt;</code>. This is a small change but now the results from each iteration of the loop will be added to the end of the output <code>fastq</code> file instead of replacing the current contents.</p> <p>Write the following loop and see how the outputs differ.</p> <p>code</p> <pre><code>FILENAME=SRR098026.fastq\nfor MOTIF in \"NNNNNNNNNN\" \"GCTGGCGNNN\" \"TTTTTTTTTT\";\ndo\n    grep -B1 -A2 ${MOTIF} ${FILENAME} &gt;&gt; my_appended_file.fastq\ndone\n</code></pre>"},{"location":"level2/13_shell_redirection/#using-variables-in-the-output-file-name","title":"Using variables in the output file name","text":"<p>If you wanted an output where all of your search motifs were placed into a single file, that approach is fine. However, it is equally likely that we will not want this outcome and will instead want to create different output files for each motif searched. Try to incorporate this into your existing loop.</p> <p>Exercise</p> <p>Modify your loop above to create individual files according to the value of <code>${MOTIF}</code>. You can use either the overwriting (<code>&gt;</code>) of appending (<code>&gt;&gt;</code>) operator.</p> Solution <p>code</p> <pre><code>FILENAME=SRR098026.fastq\nfor MOTIF in \"NNNNNNNNNN\" \"GCTGGCGNNN\" \"TTTTTTTTTT\";\ndo\n    grep -B1 -A2 ${MOTIF} ${FILENAME} &gt; my_file.${MOTIF}.fastq\ndone\n</code></pre>"},{"location":"level2/13_shell_redirection/#redirecting-output-to-a-different-command","title":"Redirecting output to a different command","text":"<p>Capturing outputs in a file is a very useful technique when working on the command line. However, when we are using a command line tool to process files we often run into the problem that there is no single tool powerful enough to do all the things we need in a single operation. This is largely part of the design philosophy behind command line tools - they should be very good at one thing, but only really do that one thing.</p> <p>There is then a form of redirection to pass the output of one tool directly into another, which allows us to chain together multiple different commands into a more powerful pipeline.</p> <p>We will not cover all of the command line tools that exist on NeSI, but here are a few of the commonly used ones and when you might need to use them.</p> Tool Purpose <code>grep</code> Search a file for a keyword, or set of keywords <code>sed</code> Search a file for a keyword, or set of keywords, and replace them with a new value <code>cat</code> Print the content of a file to the terminal <code>head</code> Print the first N lines of a file to the terminal <code>tail</code> Print the last N lines of a file to the terminal <code>sort</code> Sort the contents of a file alphabetically <code>uniq</code> Print the unique occurences in a stream of text entries <code>cut</code> Extract a specific set of columns from a tab (default) or overwise delimited text file The <code>uniq</code> command <p>The <code>uniq</code> command is basically as described above - from a set of entries it will print out the unique occurences but it works by comparing the current entry in a sequence and testing whether or not it is the same as the last entry.</p> <p>This is not necessarily how you might expect it to work. A common assumption is that it will compare the current entry against all previously observed entries but this is not the case. To see the difference in action, consider a file with the following values:</p> <p>Input file</p> <pre><code>a\na\na\nb\nb\nc\n</code></pre> <p>If we were to run this through <code>uniq</code> we would see the following:</p> <p>Output</p> <pre><code>a\nb\nc\n</code></pre> <p>Which is pretty much as expected. However, if the content was instead:</p> <p>Input file</p> <pre><code>a\na\na\nb\nb\nc\na\n</code></pre> <p>Output</p> <pre><code>a\nb\nc\na\n</code></pre> <p>This is because <code>uniq</code> keeps no record that it has previously seen an <code>a</code> value. When it hits the last line of the file and encounters the <code>a</code> it only tests if that <code>a</code> matches the value of the previous line, which was a <code>c</code>. Since they do not match, the <code>a</code> is reported. This means that we must sort the information in our text stream before passing it into <code>uniq</code> if we want to see only the unique pieces of information.</p> <p>We are going to write a command that chains together three of the commands above to produce a basic summary of a table of information. In the <code>redirection/</code> folder there is a table called <code>ncbi_viruses.txt</code> which is a non-exhaustive list of the virus names recorded in the NCBI taxonomy database. It's a big file (22,387 lines) so we're going to build a set of commands to perform a quick summary of some of the information it contains.</p> <p>If you run a quick <code>less</code> (remembering Q to quit) or <code>head</code> command you will see that the contents look something like</p> <p>Output</p> <pre><code>taxid   Kingdom Phylum  Class   Order   Family  Genus   Species\n2716741 Viruses Pisuviricota    Pisoniviricetes Picornavirales  Iflaviridae     Iflavirus       ACT flea iflavirus\n1244521 Viruses Negarnaviricota Ellioviricetes  Bunyavirales    Hantaviridae    Orthohantavirus ANAJ Hantavirus\n1482734 Viruses Pisuviricota    Pisoniviricetes Picornavirales  Picornaviridae  Aalivirus       Aalivirus A\n2320189 Viruses Uroviricota     Caudoviricetes  unclassified    Autographiviridae       Aarhusvirus     Aarhusvirus dagda\n</code></pre> <p>The rendering of the contents is a bit uneven, but each text value is separated by a Tab character so if you copied this data into Excel you would see the values as a table:</p> taxid Kingdom Phylum Class Order Family Genus Species 2716741 Viruses Pisuviricota Pisoniviricetes Picornavirales Iflaviridae Iflavirus ACT flea iflavirus 1244521 Viruses Negarnaviricota Ellioviricetes Bunyavirales Hantaviridae Orthohantavirus ANAJ Hantavirus 1482734 Viruses Pisuviricota Pisoniviricetes Picornavirales Picornaviridae Aalivirus Aalivirus A 2320189 Viruses Uroviricota Caudoviricetes unclassified Autographiviridae Aarhusvirus Aarhusvirus dagda <p>Our aim here is to use the commands mentioned above to perform a quick tally of the genera within the family Potyviridae. This will require us to perform the following steps:</p> <ol> <li>Select only the rows which contain the value 'Potyviridae'.</li> <li>Extract the values from the column corresponding to 'Genus'.</li> <li>Identify the unique genera within the column.</li> </ol> <p>Exercise</p> <p>Before we proceed have a quick look through the tools listed above and try to identify which tools can be used for each step.</p> Solution <ol> <li><code>grep</code></li> <li><code>cut</code></li> <li><code>uniq</code>, combined with <code>sort</code> (see notes on <code>uniq</code> above)</li> </ol> <p>When we redirect from one tool to another, we use the pipe character (<code>|</code>). This is probably not a key on your keyboard you use very much, so let's take a minute to find that key.</p> <p>For the standard QWERTY keyboard layout, the <code>|</code> character can be found using the key combination Shift+\\. Rather than direct to a file location, this now captures the printed output and directs it to an input channel of the command that follows the pipe operator.</p> <p>For example, if we are passing the output of the <code>grep</code> command to find lines in the table with the term 'Potyviridae' and want to pass it into the <code>cut</code> command, it will look like:</p> <p>code</p> <pre><code>grep \"Potyviridae\" ncbi_viruses.txt | &lt;next command&gt;\n</code></pre> <p>From there we can provide <code>cut</code> with the parameters we need and then either direct its output to a file (<code>&gt;</code>) or to another command (<code>|</code>). In this instance we only have a single parameter to provide <code>cut</code> and that is which column(s) we want it to extract from the input data. This is done with the <code>-f</code> parameter which takes the column position (not label) that we want to retain. You can provide multiple values to <code>-f</code>, separated with a comma, but in this case we will only need the one.</p> <p>code</p> <pre><code>grep \"Potyviridae\" ncbi_viruses.txt | cut -f7 | sort\n</code></pre> Output (last 10 lines) <pre><code>Roymovirus\nRymovirus\nRymovirus\nRymovirus\nTritimovirus\nTritimovirus\nTritimovirus\nTritimovirus\nTritimovirus\nTritimovirus\n</code></pre> <p>And that's most of the command we need. The initial file is filtered for lines that contain the text 'Potyviridae', those lines are passed into the <code>cut</code> command which extracts just the information in the genus column. The results are then alphabetically sorted and <code>uniq</code> reports the unique names found.</p> <p>The only thing we need to do to complete the exercise is to report how many of each genus were found.</p> <p>Exercise</p> <p>Search the manual for <code>uniq</code> and find the parameter required to print out the number of observations for each unique entry.</p> Solution <p>code</p> <pre><code>grep Potyviridae ncbi_viruses.txt | cut -f7 | sort | uniq -c\n</code></pre> Output <pre><code>2 Arepavirus\n1 Bevemovirus\n1 Brambyvirus\n7 Bymovirus\n1 Celavirus\n9 Ipomovirus\n14 Macluravirus\n3 Poacevirus\n316 Potyvirus\n2 Roymovirus\n3 Rymovirus\n6 Tritimovirus\n</code></pre>"},{"location":"level2/14_nesi_environment/","title":"1.4 - The NeSI computing environment","text":""},{"location":"level2/14_nesi_environment/#14-the-nesi-computing-environment","title":"1.4 - The NeSI computing environment","text":""},{"location":"level2/14_nesi_environment/#overview","title":"Overview","text":"<p>time</p> <ul> <li>Teaching: 30 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/14_nesi_environment/#objectives","title":"Objectives","text":"<ul> <li>Understand the differences between the <code>project/</code> and <code>nobackup/</code> directories.</li> <li>Understand the key commands when working with the module system.</li> <li>Understand the key commands when working with <code>slurm</code>.</li> </ul>"},{"location":"level2/14_nesi_environment/#keypoints","title":"Keypoints","text":"<ul> <li>NeSI provide two data directories for performing bioinformatic work. Data should be kept in the location most appropriate for the task.</li> <li>The module system is used to load and unload software for specific computing tasks.</li> <li>Software versioning can be controlled via the module system.</li> <li>Jobs are submitted to the cluster via the <code>slurm</code>, where they run asynchronously from the users session.</li> </ul>"},{"location":"level2/14_nesi_environment/#persistant-and-temporary-storage-locations","title":"Persistant and temporary storage locations","text":"<p>When working on NeSI there are two locations we can use to store our data; the <code>project/</code> directory (where we work for these training workshops) and the <code>nobackup/</code> location. These file locations are similar in terms of how they are accessed:</p> <p>code</p> <pre><code>cd /nesi/project/nesi03181/\n\ncd /nesi/nobackup/nesi03181/\n</code></pre> <p>There are important differences between them. Data on the <code>project/</code> directory is persistent, backed up, and a billed resource. Anything you place in the <code>project/</code> directory will stay there until it is deleted.</p> <p>In contrast, files written to the <code>nobackup/</code> directory do not last forever. Files that have not been modified (a proxy for 'used') in the last three months are automatically removed from the system. The benefit of this directory though is that we are not charged for our usage of the <code>nobackup/</code> directory.</p> <p>There are many tools we work with which create a lot of intermediate files as they run, and realistically we do not need to keep everything that produce. In particular, assembly tools such as <code>SPAdes</code> and <code>Canu</code> create a lot of temporary files as they move through rounds of data cleaning, assembly, and refinement but in the end only a handful of the outputs are actually useful for the end user.</p> <p>When working with large projects, it is best practice to keep your raw data in the <code>project/</code> directory but perform your day to day work in the <code>nobackup/</code> side. Only copy the critical outputs back to the <code>project/</code> side.</p>"},{"location":"level2/14_nesi_environment/#search-and-load-software-with-the-module-system","title":"Search and load software with the <code>module</code> system","text":"<p>Up until this point we have worked exclusively with software that is available by default in <code>bash</code> environments. For biology-related tasks though, these tools alone are not sufficient and we must use specialised software for performing analyses.</p> <p>While we are free to install new software into NeSI there are many common bioinformatics tools already available on the platform, we just need to know how to access them. The most user-friendly option for finding pre-installed software is the NeSI Supported Applications web page. This provides an up to date list of everything available on NeSI. Each piece of software lists the versions installed and links to the software documentation, and every entry is tagged with some handy keywords to enable quick searching.</p> <p>Alternatively, if we are already logged into NeSI then we can search from the command line to find software relevant to us. To view our currently loaded software modules, we can use the <code>module list</code> command.</p> <p>code</p> <pre><code>module list\n</code></pre> Output <pre><code>Currently Loaded Modules:\n1) XALT/minimal                      15) imkl-FFTW/2022.0.2-gimpi-2022a      29) libxml2/2.9.10-GCCcore-11.3.0\n2) NeSI                         (S)  16) gimkl/2022a                         30) libxslt/1.1.34-GCCcore-11.3.0\n3) slurm                             17) nodejs/16.15.1-GCCcore-11.3.0       31) cURL/7.83.1-GCCcore-11.3.0\n4) GCCcore/11.3.0                    18) git/2.23.3                          32) netCDF/4.8.1-gimpi-2022a\n5) zlib/1.2.11-GCCcore-11.3.0        19) ZeroMQ/4.3.4-GCCcore-11.3.0         33) SQLite/3.36.0-GCCcore-11.3.0\n6) binutils/2.38-GCCcore-11.3.0      20) bzip2/1.0.8-GCCcore-11.3.0          34) Tcl/8.6.10-GCCcore-11.3.0\n7) GCC/11.3.0                        21) XZ/5.2.5-GCCcore-11.3.0             35) Tk/8.6.10-GCCcore-11.3.0\n8) libpmi/2-slurm                    22) libpng/1.6.37-GCCcore-11.3.0        36) OpenSSL/1.1.1k-GCCcore-11.3.0\n9) numactl/2.0.14-GCC-11.3.0         23) freetype/2.11.1-GCCcore-11.3.0      37) Python/3.10.5-gimkl-2022a\n10) UCX/1.12.1-GCC-11.3.0             24) Szip/2.1.1-GCCcore-11.3.0           38) JupyterLab/.2023.1.0-gimkl-2022a-3.5.3 (H)\n11) impi/2021.5.1-GCC-11.3.0          25) HDF5/1.12.2-gimpi-2022a             39) craype-broadwell\n12) AlwaysIntelMKL/1.0                26) libjpeg-turbo/2.1.3-GCCcore-11.3.0  40) craype-network-infiniband\n13) imkl/2022.0.2                     27) ncurses/6.2-GCCcore-11.3.0\n14) gimpi/2022a                       28) libreadline/8.1-GCCcore-11.3.0\n\nWhere:\nH:  Hidden Module\n</code></pre> <p>If we want to see what additional software is available to load, there are two options. The first is to simply report a list of every software module available on NeSI:</p> <p>code</p> <pre><code># See all tools...\nmodule avail\n\n# See all tools that match a given keyword...\nmodule avail blast\n</code></pre> Output (blast version) <pre><code>----------------------------------------------- /opt/nesi/CS400_centos7_bdw/modules/all -----------------------------------------------\nBLAST/2.3.0                BLAST/2.12.0-GCC-9.2.0         BLASTDB/2023-04                  samblaster/0.1.24-gimkl-2017a\nBLAST/2.6.0-gimkl-2017a    BLAST/2.13.0-GCC-11.3.0 (D)    BLASTDB/2023-07           (D)    samblaster/0.1.26-GCC-9.2.0   (D)\nBLAST/2.6.0-gimkl-2018b    BLASTDB/2022-07                RMBlast/2.6.0-gimkl-2017a\nBLAST/2.9.0-gimkl-2018b    BLASTDB/2022-10                RMBlast/2.9.0-GCC-7.4.0\nBLAST/2.10.0-GCC-9.2.0     BLASTDB/2023-01                RMBlast/2.10.0-GCC-9.2.0  (D)\n\nWhere:\nD:  Default Module\n\nUse \"module spider\" to find all possible modules.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n</code></pre> <p>In the first instance, we see everything available on NeSI. In the second, we see everything available on NeSI with the keyword 'blast' in the name. There is also a more thorough search option:</p> <p>code</p> <pre><code>module spider blast\n</code></pre> Output <pre><code>-----------------------------------------------------------------------------------------------------------------------------------\nBLAST:\n-----------------------------------------------------------------------------------------------------------------------------------\n    Description:\n    Basic Local Alignment Search Tool, or BLAST, is an algorithm for comparing primary biological sequence information, such as\n    the amino-acid sequences of different proteins or the nucleotides of DNA sequences. \n\n    Versions:\n        BLAST/2.3.0\n        BLAST/2.6.0-gimkl-2017a\n        BLAST/2.6.0-gimkl-2018b\n        BLAST/2.9.0-gimkl-2018b\n        BLAST/2.10.0-GCC-9.2.0\n        BLAST/2.12.0-GCC-9.2.0\n        BLAST/2.13.0-GCC-11.3.0\n\n-----------------------------------------------------------------------------------------------------------------------------------\nFor detailed information about a specific \"BLAST\" module (including how to load the modules) use the module's full name.\nFor example:\n\n    $ module spider BLAST/2.9.0-gimkl-2018b\n-----------------------------------------------------------------------------------------------------------------------------------\n\n-----------------------------------------------------------------------------------------------------------------------------------\nBLASTDB:\n-----------------------------------------------------------------------------------------------------------------------------------\n    Description:\n    BLAST databases downloaded from NCBI.\n\n    Versions:\n        BLASTDB/2022-07\n        BLASTDB/2022-10\n        BLASTDB/2023-01\n        BLASTDB/2023-04\n        BLASTDB/2023-07\n\n-----------------------------------------------------------------------------------------------------------------------------------\nFor detailed information about a specific \"BLASTDB\" module (including how to load the modules) use the module's full name.\nFor example:\n\n    $ module spider BLASTDB/2023-07\n-----------------------------------------------------------------------------------------------------------------------------------\n\n-----------------------------------------------------------------------------------------------------------------------------------\nRMBlast:\n-----------------------------------------------------------------------------------------------------------------------------------\n    Description:\n    RMBlast supports RepeatMasker searches by adding a few necessary features to the stock NCBI blastn program. These include:\n    Support for custom matrices ( without KA-Statistics ). Support for cross_match-like complexity adjusted scoring. Cross_match\n    is Phil Green's seeded smith-waterman search algorithm. Support for cross_match-like masklevel filtering.. \n\n    Versions:\n        RMBlast/2.6.0-gimkl-2017a\n        RMBlast/2.9.0-GCC-7.4.0\n        RMBlast/2.10.0-GCC-9.2.0\n\n-----------------------------------------------------------------------------------------------------------------------------------\nFor detailed information about a specific \"RMBlast\" module (including how to load the modules) use the module's full name.\nFor example:\n\n    $ module spider RMBlast/2.9.0-GCC-7.4.0\n-----------------------------------------------------------------------------------------------------------------------------------\n\n-----------------------------------------------------------------------------------------------------------------------------------\nsamblaster:\n-----------------------------------------------------------------------------------------------------------------------------------\n    Description:\n    samblaster is a fast and flexible program for marking duplicates in read-id grouped paired-end SAM files. It can also\n    optionally output discordant read pairs and/or split read mappings to separate SAM files, and/or unmapped/clipped reads to a\n    separate FASTQ file. When marking duplicates, samblaster will require approximately 20MB of memory per 1M read pairs. \n\n    Versions:\n        samblaster/0.1.24-gimkl-2017a\n        samblaster/0.1.26-GCC-9.2.0\n\n-----------------------------------------------------------------------------------------------------------------------------------\nFor detailed information about a specific \"samblaster\" module (including how to load the modules) use the module's full name.\nFor example:\n\n    $ module spider samblaster/0.1.26-GCC-9.2.0\n-----------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>The difference here is that <code>avail</code> searches the module names, and <code>spider</code> searches their description and other information for the keyword. Both are useful, and in both cases the search is case insensitive. For example, above we used the lowercase spelling of <code>BLAST</code> but in the results we have a mixture of cases (<code>BLAST/2.3.0</code>, <code>RMBlast/2.6.0-gimkl-2017a</code>, <code>samblaster/0.1.24-gimkl-2017a</code>).</p> <p>When we want to go and load a module, the <code>module load</code> command is case sensitive so we must use the exact result from <code>module avail</code> or <code>module spider</code>.</p> <p>code</p> <pre><code>module load blast/2.3.0\n</code></pre> Output <pre><code>Lmod has detected the following error:  The following module(s) are unknown: \"blast/2.3.0\"\n\nPlease check the spelling or version number. Also try \"module spider ...\"\nIt is also possible your cache file is out-of-date try:\n$ module --ignore-cache load \"blast/2.3.0\"\n</code></pre> <p>code</p> <pre><code>module load BLAST/2.3.0\n</code></pre> <p>When we load a module, if there is no feedback from the command prompt then the load was successful. We can now access our tool from the command line like we can for the native tools like <code>grep</code>, <code>ls</code>, and <code>cp</code>.</p>"},{"location":"level2/14_nesi_environment/#considerations-when-working-with-modules","title":"Considerations when working with modules","text":"<p>Software modules are used for a number of reasons. The first is that whenever a session starts (i.e. you log into NeSI) all required tools must be found and loaded by the operating system. With the number of tools available on NeSI this would be prohibitive.</p> <p>The second reason is that software versions change through time, as new features are added or bugs are fixed. For the <code>samtools</code> software, which we will use later in this training program, there are currently 10 versions of the software installed into NeSI (0.1.18, 0.1.19, 1.3.1, 1.8, 1.9, 1.10, 1.12, 1.13, 1.15, 1.16). All of these are executed through the <code>samtools</code> command, so if all were simultaneously available NeSI would not know which one to use.</p> <p>In most situations we would want to be working with the most recent version of the software, and if we use the <code>module load</code> command without providing a version number then this is what will load, but sometimes there are reasons to use an older version.</p> <p>code</p> <pre><code>module avail samtools\n</code></pre> Output <pre><code>----------------------------------------------- /opt/nesi/CS400_centos7_bdw/modules/all -----------------------------------------------\nSAMtools/0.1.18-gimkl-2017a      SAMtools/0.1.19-GCCcore-11.3.0    SAMtools/1.8-gimkl-2018b    SAMtools/1.12-GCC-9.2.0\nSAMtools/0.1.18-gimkl-2018b      SAMtools/0.1.19-gimkl-2017a       SAMtools/1.9-GCC-7.4.0      SAMtools/1.13-GCC-9.2.0\nSAMtools/0.1.19-GCCcore-7.4.0    SAMtools/1.3.1-gimkl-2017a        SAMtools/1.10-GCC-9.2.0     SAMtools/1.15.1-GCC-11.3.0\nSAMtools/0.1.19-GCCcore-9.2.0    SAMtools/1.8-gimkl-2017a          SAMtools/1.11-GCC-7.4.0     SAMtools/1.16.1-GCC-11.3.0 (D)\n\nWhere:\nD:  Default Module\n\nUse \"module spider\" to find all possible modules.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n</code></pre> <p>Be specific in your versioning</p> <p>It is always better to load a specific version as you then have a record of which version was used should you ever need to revisit your work. This helps to avoid dependency clashes, which is the final consideration when loading modules.</p>"},{"location":"level2/14_nesi_environment/#conflicting-dependencies-between-software","title":"Conflicting dependencies between software","text":"<p>One major benefit of working with the module system is that it allows us to avoid conflicts between tools which were written in different programming environments. When we write software, it is very rare to write the entire program from scratch. There are a wealth of publicly available resources which can be used when developing a tool which saves the developer from writing every single line of code they need to achieve their intent. This saves time and leads to more stable and robust code but can be a problem if we are trying to use tools with incompatible dependencies.</p> <p>For a common example of clashing dependencies, we can try to load the <code>BLAST/2.6.0-gimkl-2017a</code> and <code>SAMtools/1.8-gimkl-2018b</code> modules in the same NeSI session.</p> <p>code</p> <pre><code>module load BLAST/2.6.0-gimkl-2017a\nmodule load \n</code></pre> Output <pre><code>Lmod has detected the following error:  These module(s) exist but cannot be loaded as requested: \"gimkl/2018b\"\nTry: \"module spider gimkl/2018b\" to see how to load the module(s).\n</code></pre> <p>This cannot be done. The first module can be loaded without issue, but the second one will always produce an error on load. For example;</p> <p>This issue occurs because these two different tools were created using a different set of development tools, and both sets of code cannot be active in parallel.</p> <p>This is one of the key considerations we must keep in mind when working with NeSI.</p> <p>Ideally, our sessions will be minimalistic and only load a single module for each job, as in order to keep our resource usage minimal and efficient we will use resource requests tailored for the specific job. When we need to perform multiple commands in a single script we must make sure that all modules can be loaded together, or make use of the <code>module purge</code> command to isolate the software at each step of the script.</p> <p>code</p> <pre><code>module load BLAST/2.6.0-gimkl-2017a\n# Do some BLAST work...\n\nmodule purge\nmodule load SAMtools/1.8-gimkl-2018b\n# Do some samtools work...\n</code></pre> <p>The NeSI module nomenclature works in the manner of <code>SOFTWARE/VERSION-BUILD</code>, for the <code>BLASTn</code> example above we can see this as:</p> <pre><code>BLAST/2.6.0-gimkl-2017a\n|-----|-----|----------\nTool  Ver.  BUILD\n</code></pre> <p>If tools have the same build description then they should be able to load together, but always test that your modules are compatible by loading them from your JupyterHub terminal before loading them in a script.</p>"},{"location":"level2/21_assembly_de_novo/","title":"2.1 - Genome Assembly","text":""},{"location":"level2/21_assembly_de_novo/#21-genome-assembly","title":"2.1 - Genome Assembly","text":""},{"location":"level2/21_assembly_de_novo/#overview","title":"Overview","text":"<p>time</p> <ul> <li>Teaching: 15 minutes</li> </ul> <p>Key points</p>"},{"location":"level2/21_assembly_de_novo/#keypoints","title":"Keypoints","text":"<ul> <li>Assembly is the process of reconstructing long nucletic sequences (contigs) from fragmented read data.</li> <li>There are two major approaches for how assembly is bets performed - Debruijn graph and Overlap Layout Consensus assemblies.</li> <li>There are multiple tools for performing assembly, and specifically different tools for assembling short or long read data.</li> </ul>"},{"location":"level2/21_assembly_de_novo/#the-assembly-process","title":"The assembly process","text":"<p>Genome assembly is the act of organising sequencing data to produce a representation of the complete genome sequence of an organism (ie a plant, fungus, bacterium or virus).  An assembled genome can then be used to perform down stream analysis such as annotation of coding regions, BLAST analysis or comparison with known or reference material. </p> <p>The success of a genome assembly project depends on a range of factors including;</p> <ol> <li>The type of genome being assembled (ie a viral genome is smaller and easier to assemble than a fungal genome) </li> <li>The quality of the original sample and the extracted DNA </li> <li>The sequencing technology used to generate the data</li> <li>Read length, read quality and read quantity can all impact genome assembly</li> <li>The software used to assemble the genome </li> </ol>"},{"location":"level2/21_assembly_de_novo/#assembly-methods","title":"Assembly methods","text":"<p>Although there is a wide range of genome assembly tools, two main approaches which genome assemblers use depending on the type of data available. These classes are: Overlap Layout Consensus (OLC) and De Bruijn Graph (DBG).   </p> <p>Understanding how each assembly type works is generally beyond the scope of this training, but it is important to use an approriate assembly method for your data type. </p> <p>We're not going to dive deeply into the differences between these methods, but as a brief differentiation, OLC methods work by aligning the sequence reads against each other, and trying to build consensus sequences where sequence align and extend upon one another.</p> <p>By contrast DBG methods cut the input sequence data into smaller pieces, and maps the transition from one of these sub-reads to the next. Where common sub-reads are identified in different sequences, a map of how these sub-reads relate to each other is made and eventually a consensus sequence can be extracted.</p> <p>The key important distinction between these methods is that OLC is robust to a degree of sequence variation between the reads it is trying to align, whereas DBG methods require very high quality data to keep the map of how sub-reads relate to each other manageable. Therefore, the key takeaway is that Overlap Layout Consensus methods are best suited to long read sequence data while De Bruijn Graph methods are better suited to short read sequence data. </p> <p>In this training we will use short and long read sequence data to familiarise ourselves with two popular assemblers SPAdes (short read) and Flye (long read).</p>"},{"location":"level2/22_assembly_spades/","title":"2.2 - Assembling short-read Illumina data","text":""},{"location":"level2/22_assembly_spades/#22-assembling-short-read-illumina-data","title":"2.2 - Assembling short-read Illumina data","text":""},{"location":"level2/22_assembly_spades/#overview","title":"Overview","text":"<p>time</p> <ul> <li>Teaching: 10 minutes</li> <li>Exercises: 10 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/22_assembly_spades/#objectives","title":"Objectives","text":"<ul> <li>Perform an assemble of a bacterial genome using the <code>SPAdes</code> assembly tool.</li> </ul>"},{"location":"level2/22_assembly_spades/#keypoints","title":"Keypoints","text":"<ul> <li>The <code>SPAdes</code> genome assembler is a powerful tool for assemnling genoms and contigs from a wide range of sampel types.</li> </ul>"},{"location":"level2/22_assembly_spades/#introduction-to-the-spades-assembly-tool","title":"Introduction to the SPAdes assembly tool","text":"<p>The <code>SPAdes</code> assembler is a very powerful and popular assembler which utilises de Bruijn graphs to assemble short read sequence data into larger contigs.</p> <p>SPAdes is particularly good at...</p> <ul> <li>De novo assembly of short read sequences (i.e. Illumina or IonTorrent)</li> <li>Assembling small genomes (ideally &lt;100 Mb, such as bacterial, viral, fungal, mitochondrial genomes)</li> </ul> <p>Despite this, it is possible to apply <code>SPAdes</code> to larger genomes and obtain very good results.</p> <p>In order to begin, we must first find the versions of <code>SPAdes</code> installed on NeSI and load the module of interest.</p> <pre><code>$ module load SPAdes/3.15.2-gimkl-2020a\n\n$ spades.py -h\n</code></pre> <p>Remember to filter your data prior to assembly!</p> <p>Before beginning to work with <code>SPAdes</code> we need to ensure that our data is free from adapter sequences. The main reason for doing this is when we assemble sequences to form a genome, the assembler is looking for spans of nucleic acid sequence which are common to multiple reads, so that those reads can be joined together to create longer contigs. As the adapter sequence is an identical tag added to every read, these create regions of artificial homology between sequences which have no real connection to each other.</p> <p>Before we attempt assembly it is critical to remove these from our data but as this was covered in the level 1 training we will not be revisiting the process here.</p>"},{"location":"level2/22_assembly_spades/#performing-an-assembly-using-spades","title":"Performing an assembly using SPAdes","text":"<p>The test data is a set of Illumina MiSeq sequencing reads from an M. bovis genome. To save time, the reads have already been quality filtered with <code>fastp</code> and the number of reads reduced to speed up analysis.</p> <p>Navigate to the <code>assembly_illumina/</code> folder to begin.</p> <p>code</p> <pre><code>cd /nesi/project/nesi03181/phel/USERNAME/level2/assembly_illumina/\n</code></pre> <p>Exercise</p> <p>Use your knowledge of the module system on NeSI to find the most recent release of <code>SPAdes</code>.</p> Solution <p>code</p> <pre><code>module avail spades\n\n# OR...\nmodule spider spades\n</code></pre> Output <pre><code>----------------------------------------------- /opt/nesi/CS400_centos7_bdw/modules/all -----------------------------------------------\n  SPAdes/3.13.1-gimkl-2018b    SPAdes/3.15.0-gimkl-2020a    SPAdes/3.15.3-gimkl-2020a\n  SPAdes/3.14.0-gimkl-2020a    SPAdes/3.15.2-gimkl-2020a    SPAdes/3.15.4-gimkl-2022a-Python-3.10.5 (D)\n</code></pre> <p>Create a <code>slurm</code> script with the following contents. Be sure to replace the <code>YOUR_EMAIL</code> and <code>USERNAME</code> values with your details.</p> <p>spades_asm.sl</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi03181\n#SBATCH --job-name      spades_asm\n#SBATCH --time          00:40:00\n#SBATCH --cpus-per-task 16\n#SBATCH --mem           20G\n#SBATCH --error         spades_asm.%j.err\n#SBATCH --output        spades_asm.%j.out\n#SBATCH --mail-type     END\n#SBATCH --mail-user     YOUR_EMAIL\n\nmodule purge\nmodule load SPAdes/3.15.4-gimkl-2022a-Python-3.10.5\n\n# Set the path from which the script will execute SPAdes\ncd /nesi/project/nesi03181/phel/USERNAME/level2/assembly_illumina/\n\n# Execute SPAdes\nspades.py --isolate --threads ${SLURM_CPUS_PER_TASK} \\\n    -1 reads/Mbovis_87900.miseq_R1.fq.gz \\\n    -2 reads/Mbovis_87900.miseq_R2.fq.gz \\\n    -o assembly/\n</code></pre> <p>Submit this job to <code>slurm</code>:</p> <p>code</p> <pre><code>sbatch spades_asm.sl\n</code></pre> Output <pre><code>Submitted batch job ########\n</code></pre> <p>When this job is complete, we will have a folder named <code>assembly/</code> which contains a lot of different files and pieces of information. A lot of the contents are files generated internally by <code>SPAdes</code> and we do not need to pay attention to them. The most important files for us are:</p> <ol> <li><code>contigs.fasta</code> - the assembled contigs, as a fasta file.</li> <li><code>scaffolds.fasta</code> - the scaffolded contigs, as a fasta file.</li> <li>This is similar to the contigs file, except it will contain sequences where contigs have been joined by an indeterminate piece of sequence.</li> <li>This occurs when <code>SPAdes</code> can tell from the pairing information in our library that two contigs belong adjacent to each other, but it has no information for filling the gap between them.</li> <li><code>spades.log</code> - the log file of the steps <code>SPAdes</code> performed and any warnings which occurred during assembly.</li> <li><code>assembly_graph.fastg</code> - a map of how well the assembly is resolved.</li> <li>This can be useful if we are trying to obtain a complete genome, as it shows us areas which have assembled cleaning and areas which were difficult to resolve.</li> </ol> <p>Different modes for running <code>SPAdes</code></p> <p>If you checked the help command for <code>SPAdes</code>, or were examining the contents of the <code>slurm</code> script carefully, you might have noted the flag <code>--isolate</code> which we passed to the command.</p> <p><code>SPAdes</code> was originally created as a tool for a very specific use case - the assembly of single-cell amplified genomes, which were problematic to assembly due to the highly uneven coverage of the nucleic acid content in the input libraries (Bankevich et al., 2012). It was also noted at the time that despite this fairly niche scope for use, the tool outperformed many traditional short read assemblers of the time and so over the years the team behind <code>SPAdes</code> have expanded the tools and added a number of alternate assembly modes into successive iterations of the tool.</p> <p>There are now specific subroutines in the <code>SPAdes</code> assembler for working with the following data types:</p> <ol> <li>metaSPAdes - assembly of metagenomic samples (sequence libraries from multiple organisms)</li> <li>plasmidSPAdes - selective assembly of plasmids from genomic libraries</li> <li>metaplasmidSPAdes - selective assembly of plasmids from metagenomic </li> <li>rnaSPAdes - de novo assembly of transcriptome libraries</li> <li>biosyntheticSPAdes - selective assembly of biosynthetic gene clusters (BCG) from a library</li> <li>rnaviralSPAdes - selective de novo assembly of RNA viruses from transcriptome, metatranscriptome, and metavirome libraries</li> </ol> <p>It will take a while for <code>SPAdes</code> to complete, so we will move to the next exercise and return to this folder later.</p>"},{"location":"level2/23_assembly_flye/","title":"2.3 - Assembling Oxford Nanopore data","text":""},{"location":"level2/23_assembly_flye/#23-assembling-oxford-nanopore-data","title":"2.3 - Assembling Oxford Nanopore data","text":""},{"location":"level2/23_assembly_flye/#overview","title":"Overview","text":"<p>time</p> <ul> <li>Teaching: 15 minutes</li> <li>Exercises: 10 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/23_assembly_flye/#objectives","title":"Objectives","text":"<ul> <li>Perform an assemble of a bacterial genome using the <code>Flye</code> assembly tool.</li> </ul>"},{"location":"level2/23_assembly_flye/#keypoints","title":"Keypoints","text":"<ul> <li>The <code>Flye</code> assembler is one of several very powerful tools for assembling genomes from long read data.</li> <li>Different tools may be better suited for different data, and long read assembly is still a developing field so if working with these data, make sure to experiment with multiple tools.</li> </ul>"},{"location":"level2/23_assembly_flye/#introduction-to-flye","title":"Introduction to Flye","text":"<p>Although the gap is closing rapidly, Oxford Nanopore sequences are fundamentally more error prone than the sequences we obtain through Illumina sequencing and a considerable amount of assembly is spent identifying and correcting errors to produce high-quality contigs from a comparably low-quality set of reads.</p> <p>Comparison of average sequence qualities on different platforms</p> <p></p><p></p> <p>The median sequence quality for the Nanopore data produced using the (now retired 9.4 chemistry) data sits around Q20 for most of the sequence. This corresponds to 99% accuracy which might sound good, but by definition half of the sequences have lower quality than this. At the low end of this plot the sequences are slightly above Q10, which denotes 90% accuracy.</p> <p>Finding consensus regions between pairs of reads, when one of them might differ by up to 10% of it's composition just due to sequencing error alone makes assembly a complicated process and assembly tools which are aware of the error profiles of our long read data are essential.</p> Why use <code>Flye</code>? <p>The complete workflow of <code>Flye</code> (Kolmogorov et al., 2019) is quite complicated process. The novel aspect of assembly with <code>Flye</code> when compared with other asssembly tools was the developers observation that when working with noisy reads (as mentioned above) mapping sequences against each other is confounded by highly similar repeat regions, which create hotspots of local alignment between reads with very different flanking sites.</p> <p>For our purposes, the main points of the assembly process to understand are:</p> <ol> <li>Repetitive regions of the genome are identified</li> <li>Repeat regions are clustered together to form deliberately misassembled disjointigs</li> <li>Disjointigs are concatenated and a repeat graph (similar to a de Bruijn graph) is created</li> <li>The input reads are mapped to the repeat graph</li> <li>Using the reads that map to the repeat region and it's 5' and 3' flanking regions, the loops in the assembly graph are unwound to create long contigs</li> </ol> <p>To run <code>Flye</code>, navigate to your <code>assembly_nanopore/</code> directory, and prepare the following <code>slurm</code> script:</p> <p>flye_asm.sl</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi03181\n#SBATCH --job-name      flye_asm\n#SBATCH --time          00:10:00\n#SBATCH --cpus-per-task 16\n#SBATCH --mem           20G\n#SBATCH --error         flye_asm.%j.err\n#SBATCH --output        flye_asm.%j.out\n#SBATCH --mail-type     END\n#SBATCH --mail-user     YOUR_EMAIL\n\nmodule purge\nmodule load Flye/2.9.1-gimkl-2022a-Python-3.10.5\n\n# Set the path from which the script will execute Flye\ncd /nesi/project/nesi03181/phel/USERNAME/level2/assembly_nanopore/\n\n# Execute Flye\nflye --threads ${SLURM_CPUS_PER_TASK} \\\n    --nano-raw reads/Mbovis_87900.nanopore.fq.gz \\\n    --out-dir assembly/\n</code></pre> <p>When you are ready, submit the job to <code>slurm</code>:</p> <p>code</p> <pre><code>sbatch flye_asm.sl\n</code></pre> Output <pre><code>Submitted batch job ########\n</code></pre> <p>As with the <code>SPAdes</code> session, we will end up with a folder named <code>assembly/</code> which contains a number of files, only some of which we care about. The key files for us are:</p> <ol> <li><code>assembly.fasta</code> - the assembled contigs, as a fasta file.</li> <li><code>flye.log</code> - the log file of the steps <code>SPAdes</code> performed and any warnings which occured during assembly.</li> <li><code>assembly_graph.gfa</code> - a map of how well the assembly is resolved.</li> </ol>"},{"location":"level2/23_assembly_flye/#a-note-on-assembly-tools","title":"A note on assembly tools","text":"<p>For the exercise today are using the <code>Flye</code> assembler with one of the M. bovis genomes. Like with other areas of genomics, there are many good options for assembly tools and our usage of <code>Flye</code> today is in no way an endorsement that we consider this tool to be the 'best' long read assembler. <code>Flye</code> is a very good tool and will give us good results with the data we process today, but when working with real data there are many other good options to try, including:</p> <ol> <li><code>UniCycler</code> (and <code>TriCycler</code>) (Wick et al., 2017) - https://github.com/rrwick/Unicycler</li> <li><code>Canu</code> (Koren et al., 2017)</li> <li><code>raven</code> (Vaser et al., 2021)</li> </ol> <p>A recent comparison of assembly tools was published by Wick &amp; Holt (2021) tests some of the options listed above along with several other tools. Their manuscript is a 'living paper', which has been updated several times as new versions of each tool are released. Different assemblers have risen to the top at different time points, so this is very much still an evolving field, and it is difficult to say which assembler is the 'best'.</p> <p>In practice, there are sometimes particular cases where a tool will not be compatible with your data, so it is helpful to be aware of several tools so that you have options if assembly proves problematic for a particular sample.</p>"},{"location":"level2/24_assembly_evaluation/","title":"2.3 - Evaluating the results of an assembly","text":""},{"location":"level2/24_assembly_evaluation/#23-evaluating-the-results-of-an-assembly","title":"2.3 - Evaluating the results of an assembly","text":""},{"location":"level2/24_assembly_evaluation/#overview","title":"Overview","text":"<p>time</p> <ul> <li>Teaching: 20 minutes</li> <li>Exercises: 15 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/24_assembly_evaluation/#objectives","title":"Objectives","text":"<ul> <li>Use <code>QUAST</code> to assess the assembly status.</li> <li>(Optional) Use <code>Bandage</code> to view how well the assembly resolved.</li> </ul>"},{"location":"level2/24_assembly_evaluation/#keypoints","title":"Keypoints","text":"<ul> <li>Tools like <code>QUAST</code> can be used to perform quick and easy comparisons between an assembly and a trusted reference genome.</li> <li>It is important to make sure that your genome is sufficiently resolved to address your need, but we often do not need to go further than what an assembler provides.</li> </ul>"},{"location":"level2/24_assembly_evaluation/#assessing-the-results-of-an-assembly","title":"Assessing the results of an assembly","text":"<p>Once assembly is complete, we have a complex process of determining the quality of the assembly. How 'good' a genome is can be difficult to measure, but as we are mostly working with well characterised pathogens a good starting place is to compare our assembled genome with previously characterised members of the same species to see how well the conserved genomic features have been reconstructed by our assembly tool.</p> <p>Navigate to the <code>assembly_evaluation/</code> folder and we will begin.</p> <p>You have been provided with a copy of a reference Mycoplasmopsis bovis genome in the <code>reference/</code> folder, but we will need some draft assemblies to test as part of this module.</p> <p>Exercise</p> <p>Create a new directory and copy in the <code>assembly_evaluation/</code> folder and copy in your <code>SPAdes</code> and <code>Flye</code> fasta and fastg assembly files.</p> Solution <p>code</p> <pre><code>mkdir assemblies/\n\ncp ../assembly_illumina/assembly/contigs.fasta assemblies/\n\ncp ../assembly_nanopore/assembly/assembly.fasta assemblies/\n</code></pre> Help, my assembly failed! <p>If your assembly did not complete, don't worry about it. There is a training set of assemblies we can provide for you if required.</p> <p>Once you have a local copy of your assemblies, we will be comparing these to the reference genome using a tool called QUAST.</p> <p>Running <code>QUAST</code> is quite simple:</p> <p>code</p> <pre><code>module load QUAST/5.2.0-gimkl-2022a\n\nquast.py -r reference/Mbovis_87900.genome.fna --gene-finding -o quast/ assemblies/*.fasta\n</code></pre> Output <pre><code>Version: 5.2.0\n\nSystem information:\n  OS: Linux-3.10.0-693.2.2.el7.x86_64-x86_64-with-glibc2.17 (linux_64)\n  Python version: 3.10.5\n  CPUs number: 2\n\nStarted: 2023-09-28 15:02:30\n\n# Text omitted...\n\nFinished: 2023-09-28 15:02:40\nElapsed time: 0:00:10.422580\nNOTICEs: 4; WARNINGs: 1; non-fatal ERRORs: 0\n\nThank you for using QUAST!\n</code></pre> <p>Open the resulting <code>quast/report.pdf</code> file in Jupyter using the file browser. Take a look through the report and see if you can get a feel for how well your assemblies compare to the reference.</p> <p>How do the Illumina and Nanopore assemblies differ, if at all?</p>"},{"location":"level2/24_assembly_evaluation/#optional-visualising-assemblies-with-bandage","title":"(Optional) Visualising assemblies with <code>Bandage</code>","text":"<p>We can also visualise the assemblies by looking at how well the loops and fragments of the assembly graph were resolved. For this, we require a different set of files from the assembly output folders.</p> <p>Exercise</p> <p>Copy the <code>.fastg</code> (<code>SPAdes</code>) and <code>*.gfa</code> (<code>Flye</code>) files from your previous output folders into your current assembly directory, ready for analysis.</p> Solution <p>code</p> <pre><code>cp ../assembly_illumina/assembly/assembly_graph.fastg assemblies/\ncp ../assembly_nanopore/assembly/assembly_graph.gfa assemblies/\n</code></pre> <p>Running the tool is then a matter of:</p> <p>code</p> <pre><code>module load Bandage/0.8.1_Centos\n\nBandage image assemblies/assembly_graph.fastg spades_bandage.svg\n</code></pre> <p>You can then open the <code>assembly_bandage.svg</code> file in the Jupyter browser. Unfortunately, we cannot filter out the short contigs from this result. However, it should be clear that there is one long contig which has been assembled, and then a large number of short fragments.</p> <p>Exercise</p> <p>Repeat the <code>Bandage</code> command for your Nanopore assembly, then contrast the result from what you obtained with <code>SPAdes</code>. How do they differ?</p> Solution <p>code</p> <pre><code>Bandage image assemblies/assembly_graph.gfa flye_bandage.sv\n</code></pre> <p>The data have assembled cleanly into a single contig, without bubbles, and there are no short fragments plotted.</p>"},{"location":"level2/24_assembly_evaluation/#concluding-comments","title":"Concluding comments","text":"<p>As you can see from this exercise, getting a pretty good genome assembly is not particularly difficult with the right tools. However, the distance between a draft assembly, which we have produced, and a final completed genome is a very long process and involved multiple rounds of assembly refinement, scaffolding, and often requires the creation of custom primers to perform PCRs to close sequence gaps which were not covered in your HTS library.</p> <p>It can be hard knowing when the assembly is good enough to move out of the assembly stage and into annotation. In research groups working with genomic data, the yardstick for working with these kinds of data is typically to ask whether the current assembly is sufficient to answer the research question which led to its sequencing in the first place.</p> <p>We can copy this logic and ask, what was the purpose of sequencing this genome and can we achieve that with the current data. Typically, we are most likely looking to perform a species identification. If we find that the genome assembly contains the right marker genes or operons to perform the identification then, regardless of whether the genome is officially completed or not, it has served its purpose.</p>"},{"location":"level2/25_assembly_polishing/","title":"2.5 - Polishing of Oxford Nanopore assemblies","text":""},{"location":"level2/25_assembly_polishing/#25-polishing-of-oxford-nanopore-assemblies","title":"2.5 - Polishing of Oxford Nanopore assemblies","text":"<p>time</p> <ul> <li>Teaching: 15 minutes</li> <li>Exercises: 15 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/25_assembly_polishing/#objectives","title":"Objectives","text":"<ul> <li>Use <code>racon</code> to polish a Nanopore assembly</li> </ul>"},{"location":"level2/25_assembly_polishing/#keypoints","title":"Keypoints","text":"<ul> <li>Rounds of refinement are sometimes required to reduce error and can improve the initial assembly when working with long rad data.</li> <li>Sometimes the above sentence does not apply and polishing does not really improve the assembly. The only way to know is trying!</li> </ul>"},{"location":"level2/25_assembly_polishing/#preparing-to-polish-our-assembly","title":"Preparing to polish our assembly","text":"<p>Once we have our draft assembly, we want to revisit it and attempt to improve regions by aligning the original reads against the assembled contigs and trying to improve regions which might have been modelled incorrectly by ther assembler. This is an important process as long read assemblers have to make assumptions about the data they are working with, and do not always make the 'correct' call.</p> <p>The process of using our raw data to re-call areas of the assembly is called polishing, and there are many tools available for performing this task. Similar to the case with assembly, there are specific tools for using short read or long read data for polishing an assembly.</p> <p>Today we are going to use a long read polishing tool called <code>racon</code> (Vaser et al., 2017).</p> What are our other options? <ol> <li>medaka (GitHub)</li> <li>FMLRC (Wang et al., 2018)</li> <li>LoRDEC (Salmela et al., 2014)</li> </ol> <p>A comprehensive comparison of polishing tools was published in 2019 (Fu et al., 2019), which is still a useful reference for getting started if you are using short reads to polish a long read assembly. When working with exclusively long read data, <code>racon</code> is probably the best (most robust and universally applied) tool for a first attempt at error correction.</p> <p>Do we always need to polish an assembly?</p> <p>Whether or not our assembly will benefit from polishing is hard to predict. When working through this process it is a good idea to make copies of your data as you perform different correction procedures (or combinations of procedures) and to evaluate each outcome. You often need to compare each iteration of the assembly to a reference to see how different regions are affects.</p>"},{"location":"level2/25_assembly_polishing/#performing-the-first-round-of-polishing","title":"Performing the first round of polishing","text":"<p>Strictly speaking, <code>racon</code> is designed for polishing assemblies which have been obtained from tools that do not perform extensive error correction themselves. In practice, it rarely has a negative impact on assembly quality so while it doesn't hurt to apply it to as assembly from a tool like <code>Flye</code>, it is not always worthwhile.</p> <p>We will start working with one of several draft genome assemblies, of varying quality (in terms of mismatches to the reference). We will start with the <code>draft_moderate.fna</code> genome, which has a 2% rate of mismatch with the reference genome.</p> <p>Before running <code>racon</code> we must produce a mapping file of the quality filtered sequences against the assembly. We can do this with <code>minimap2</code>. We will work with <code>minimap2</code> more in the mapping exercises in the next session, so will not explain its parameters and workflow today.</p> <p>code</p> <pre><code>module load minimap2/2.24-GCC-11.3.0\n\nminimap2 -t 4 -ax map-ont draft_genomes/draft_moderate.fna reads/Mbovis_87900.nanopore.fq.gz &gt; draft_moderate.sam\n</code></pre> Output <pre><code>[M::mm_idx_gen::0.023*0.62] collected minimizers\n[M::mm_idx_gen::0.031*0.79] sorted minimizers\n[M::main::0.031*0.79] loaded/built the index for 1 target sequence(s)\n[M::mm_mapopt_update::0.032*0.80] mid_occ = 10\n[M::mm_idx_stat] kmer size: 15; skip: 10; is_hpc: 0; #seq: 1\n[M::mm_idx_stat::0.033*0.80] distinct minimizers: 47527 (99.61% are singletons); average occurrences: 1.004; average spacing: 5.316; total length: 253631\n[M::worker_pipeline::6.513*1.92] mapped 1766 sequences\n[M::main] Version: 2.24-r1122\n[M::main] CMD: minimap2 -t 4 -ax map-ont draft_genomes/draft_moderate.fna reads/Mbovis_87900.nanopore.fq.gz\n[M::main] Real time: 6.517 sec; CPU: 12.478 sec; Peak RSS: 0.187 GB\n</code></pre> <p>We can then use this mapping file as the input for <code>racon</code>:</p> <p>code</p> <pre><code>module load Racon/1.5.0-GCC-11.3.0\n\nracon -t 4 reads/Mbovis_87900.nanopore.fq.gz draft_moderate.sam draft_genomes/draft_moderate.fna &gt; draft_moderate.racon_1.fna\n</code></pre> Output <pre><code>[racon::Polisher::initialize] loaded target sequences 0.019926 s\n[racon::Polisher::initialize] loaded sequences 0.415303 s\n[racon::Polisher::initialize] loaded overlaps 0.360286 s\n[racon::Polisher::initialize] aligning overlaps [====================] 0.159391 s\n[racon::Polisher::initialize] transformed data into windows 0.027524 s\n[racon::Polisher::polish] generating consensus [====================] 21.940342 s\n[racon::Polisher::] total = 22.926985 s\n</code></pre> <p>Before we assess the results of this, we will run <code>racon</code> over a few different genomes, so that when we assess the final qualities, we can generate a single report for all genomes.</p> <p>Exercise</p> <p>Run <code>racon</code> polishing on either the <code>draft_mild.fna</code> or <code>draft_severe.fna</code> genome (or both!).</p> Solution <p>code</p> <pre><code>for assembly in draft_mild draft_severe;\ndo\n    minimap2 -t 4 -ax map-ont draft_genomes/${assembly}.fna reads/Mbovis_87900.nanopore.fq.gz &gt; ${assembly}.sam\n    racon -t 4 reads/Mbovis_87900.nanopore.fq.gz ${assembly}.sam draft_genomes/${assembly}.fna &gt; ${assembly}.racon_1.fna\ndone\n</code></pre>"},{"location":"level2/25_assembly_polishing/#performing-additional-rounds-of-polishing","title":"Performing additional rounds of polishing","text":"<p>It is possible to perform the <code>racon</code> process iteratively, remapping reads to the output and then running the polishing cycle again. There is some data (link here) which suggests that up to four rounds of <code>racon</code> polishing, in conjunction with <code>medaka</code>, produces better quality output than running a single polishing step.</p> <p>However, there are costs associated with this approach both in terms of time invested and over-zealous correction to repeat regions. Whether or not improvement with multiple rounds will be seen in your data is unclear, and ultimately it is your decision whether or not to perform this approach so although this can work, it is a judgement call as to whether or not it is necessary.</p> <p>Running <code>racon</code> the second time is pretty much the same as the first time, except that instead of mapping to the original draft genome, we now map our reads to the output of the first <code>racon</code> run and use that alignment for correction.</p> <p>code</p> <pre><code># Map the reads, overwriting our previous sam file\nminimap2 -t 4 -ax map-ont draft_moderate.racon_1.fna reads/Mbovis_87900.nanopore.fq.gz &gt; draft_moderate.sam\n\n# Perform correction, creating a new output file for the polished genome\nracon -t 4 reads/Mbovis_87900.nanopore.fq.gz draft_moderate.sam draft_moderate.racon_1.fna &gt; draft_moderate.racon_2.fna\n</code></pre> <p>Exercise</p> <p>Complete the second rounds of <code>racon</code> polishing for the <code>draft_mild</code> and/or <code>draft_severe</code> genomes.</p> Solution <p>code</p> <pre><code>for assembly in draft_mild draft_severe;\ndo\n    minimap2 -t 4 -ax map-ont ${assembly}.racon_1.fna reads/Mbovis_87900.nanopore.fq.gz &gt; ${assembly}.sam\n    racon -t 4 reads/Mbovis_87900.nanopore.fq.gz ${assembly}.sam ${assembly}.racon_1.fna &gt; ${assembly}.racon_2.fna\ndone\n</code></pre> Post-polishing follow up <p>One important piece on information to note with the polishing process is that <code>racon</code> does not change the names of contigs during polishing. This is helpful, as it allows us to easily compare contigs between different polishing steps but it also means that you have to be careful when importing the data into <code>Geneious</code> as it might become hard to track which step of the analysis your contig comes from.</p> <p>As an easy solution to this is to rename your contigs using a tool like <code>seqtk</code> to add some versioning information to each sequence name:</p> <p>code</p> <pre><code>seqtk/1.4-GCC-11.3.0\n\nseqtk rename draft_genomes/draft_moderate.fna \"BASE_\" &gt; draft_moderate.rename.fna\nseqtk rename draft_moderate.racon_1.fna \"RACON1_\" &gt; draft_moderate.racon_1.rename.fna\nseqtk rename draft_moderate.racon_2.fna \"RACON2_\" &gt; draft_moderate.racon_2.rename.fna\n</code></pre>"},{"location":"level2/25_assembly_polishing/#assessing-the-results-with-quast","title":"Assessing the results with <code>QUAST</code>","text":"<p>As a quick confirmation of how successful the cleaning step was, we will use <code>QUAST</code> to compare our raw and polished genomes to the reference genome.</p> <p>When running this command, modify it to include the genomes your polished as part of the exercises above.</p> <p>code</p> <pre><code>module load QUAST/5.2.0-gimkl-2022a\n\nquast.py -r reference/Mbovis_87900.genome.fna --gene-finding -o quast/ \\\n    draft_genomes/draft_mild.fna \\\n    draft_mild.racon_1.fna \\\n    draft_mild.racon_2.fna # ...plus your genomes\n</code></pre> Output <pre><code>Version: 5.2.0\n\nSystem information:\n  OS: Linux-3.10.0-693.2.2.el7.x86_64-x86_64-with-glibc2.17 (linux_64)\n  Python version: 3.10.5\n  CPUs number: 2\n\nStarted: 2023-09-28 15:02:30\n\n# Text omitted...\n\nFinished: 2023-09-28 15:02:40\nElapsed time: 0:00:10.422580\nNOTICEs: 4; WARNINGs: 1; non-fatal ERRORs: 0\n\nThank you for using QUAST!\n</code></pre> <p>Compare the results of the before and after of polishing. Did this process improve the quality of your genome(s)?</p>"},{"location":"level2/31_coverage_mapping/","title":"3.1 - Overview of mapping sequences to a reference genome","text":""},{"location":"level2/31_coverage_mapping/#31-overview-of-mapping-sequences-to-a-reference-genome","title":"3.1 - Overview of mapping sequences to a reference genome","text":"<p>time</p> <ul> <li>Teaching: 25 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/31_coverage_mapping/#objectives","title":"Objectives","text":"<ul> <li>Understand the fundamental process of mapping reads to a reference sequence.</li> <li>Understand the output files from mapping tools.</li> <li>Learn how to import mapping results into GUI programs such as <code>Geneious</code> for further analysis.</li> </ul>"},{"location":"level2/31_coverage_mapping/#keypoints","title":"Keypoints","text":"<ul> <li>Reads can be mapped to any reference sequence, for example whole genomes or individual genes, to generate a consensus sequence or view the coverage in particular regions.</li> <li>Mapping alignments are stored in the <code>sam</code> format.</li> <li>Different mapping tools work in different ways and make different assumptions when working with your data.</li> <li>Sometimes, a given tool will work better with a given dataset.</li> <li>It is recommended to try a few tools when working with new datasets to see which one produces better results.</li> </ul>"},{"location":"level2/31_coverage_mapping/#introduction-to-mapping","title":"Introduction to mapping","text":"<p>Sequencing produces a collection of sequences without genomic context - as the sequencing process reads genomic fragments in a random order we do not know the region of the genome to which each sequence corresponds. Mapping the reads of an experiment to a reference genome or a reference gene is a key step for resolving this issue. After mapping, the reads are assigned to a specific location in the genome/gene and a consensus file can be generated that reconstructs the sequence of that region on the organism that provided the reads.</p> <p>During sequencing, errors are introduced, such as incorrect nucleotides being called. Sequencing errors might bias the analysis and can lead to a misinterpretation of the data and erroneous mapping. It is good practice to filter out low quality data from our dataset before mapping, as we learned in the previous session.</p> <p>Read mapping is the process of aligning reads to a reference sequence. A mapping tool takes as input a reference genome and a set of reads. Its aim is to align each read in the set of reads on the reference genome, allowing mismatches, indels and clipping of some short fragments on the two ends of the reads.</p>"},{"location":"level2/31_coverage_mapping/#overview-of-the-mapping-process","title":"Overview of the mapping process","text":"<p>When mapping sequences, the input consists of a set of reads and a reference genome. when mapping reads to a reference the matches can be a perfect alignment between the read and reference, but often the match will be inexact. This is generally due to sequencing error, or the reference genome and the set of sequences are not genetically identical. This may be due to either individual-specific mutations, or the fact that there is not a closely related reference genome for use in the mapping process.</p> <p>Example of how reads can mismatch</p> <p></p><p></p> <p>The first read is aligned at position 100 and the alignment has two mismatches.</p> <p>The second read is aligned at position 114. It is a local alignment with clippings on the left and right.</p> <p>The third read is aligned at position 123. It consists of a 2-base insertion and a 1-base deletion.</p> <p>When working with mapped reads it is our job to examine these areas of mismatch and evaluate whether or not these represent real sequence variation.</p>"},{"location":"level2/31_coverage_mapping/#storing-read-mapping-information","title":"Storing read mapping information","text":"<p>Mapping tools usually produce their output in the Sequence Alignment/Map (<code>sam</code>) format. This file stores the read sequences, a flag on whether they have been aligned to a reference sequence, and if so, the position on the reference sequence at which they have been aligned. You can view this file using any text viewer, although owing to the file size <code>less</code> is generally the best choice.</p> Content of a <code>sam</code> file <p>Should you choose to manually inspect a <code>sam</code> file, it is basically a table of the alignment between each read and the reference and consists of the following columns:</p> Column Field Description 1 QNAME Query template (read) name 2 FLAG Bitwise flag 3 RNAME References sequence name 4 POS 1- based leftmost mapping position 5 MAPQ Mapping quality 6 CIGAR CIGAR string 7 RNEXT Ref. name of the mate/next read 8 PNEXT Position of the mate/next read 9 TLEN Observed template length 10 SEQ Segment sequence 11 QUAL ASCII representation of Phred-scaled base quality <p>The full specification for the file format can be found here.</p> <p>However, it is not usually necessary to inspect the <code>sam</code> file directly - there is a lot of information in here and unless you are looking to extract specific information from the alignment it is just an intermediate file in the workflow. In order to save disk space, and to prepare the file for downstream analysis, <code>sam</code> files are usually sorted and compressed into a format with is not human readable.</p>"},{"location":"level2/31_coverage_mapping/#sorting-mapped-results-for-analysis","title":"Sorting mapped results for analysis","text":"<p>Sorting the mapping information is an important prerequisite for performing certain downstream processes. Not every tool we use requires reads to be sorted, but it can be frustrating having to debug the instances where read sorting matters, so we typically just get it done as soon as possible and then we don't have to worry about it again. Reads will initially be mapped in an unsorted order, as they are added to the <code>sam</code> file in more or less the same order as they are encountered in the input fastq files.</p> <p>When sorted the reads are ordered by the position of their first mapped nucleotide, as exemplified below:</p> <p>Unsorted mapping content</p> <pre><code>Ref: REFERENCECONTIG\nMap: --------ECONT--\nMap: REFE-----------\nMap: --FERENCECO----\nMap: -----------NTIG\n</code></pre> <p>Sorted mapping content</p> <pre><code>Ref: REFERENCECONTIG\nMap: REFE-----------\nMap: --FERENCECO----\nMap: --------ECONT--\nMap: -----------NTIG\n</code></pre> <p>The sorted file can be used as input for any downstream processeses in our command line work, or can be imported into GUI programs for a visual analysis. Although we rely on NeSI for performing mapping in these exercises, we will use <code>Geneious</code> for visualising the results.</p> <p>There are several ways to import <code>sam</code> and <code>bam</code> files into <code>Geneious</code> either by using the <code>Import Files...</code> option from the <code>File</code> menu, or simply dragging and dropping the files into the <code>Geneious</code> document table.</p> <p>It is important that both the <code>sam</code> file and the reference sequence against which the reads were mapped are imported.</p> <p>If you import the <code>sam</code> without the reference, <code>Geneious</code> will not know the content of the original reference sequence.</p>"},{"location":"level2/31_coverage_mapping/#different-tools-for-the-job","title":"Different tools for the job","text":"<p>When it comes to mapping reads to a reference sequence, there is a vast list of tools available with different strengths and weaknesses. As this is a constantly expanding area of the literature we are going to focus on two mapping tools for these exercises but for your specific application they may not be the appropriate choice. If you are in a position where you need to perform reference mapping there are two main questions to consider when deciding which mapping tool to use.</p> <p>Are you working with short (Illumina) or long (Nanopore, PacBio) sequencing data?</p> <p>The heuristics used for accelerating the mapping process differ between short and long sequences, due to both the differences in length and expected average read quality between the platforms.</p> <p>Are you mapping DNA or RNA sequence data to your reference?</p> <p>Most of the time your reference will be a genome sequence, but if you are working with RNA sequence data from eukaryotic (or some viral) species then some portion of reads will most likely have undergone splicing after transcription.</p> <p>The reference genome contains the full gene sequences not just the exonic regions so a mapping tool which is aware of gene splicing and can map around a spliced intron sequences will achieve a higher mapping rate than a mapping tool which only considers direct matches to the reference sequence.</p> <p>As a starting point, some of the most popular mapping tools and their uses include:</p> Tool Reference Application minimap2 Li, 2018 Short and long read mappingSpecific mapping models for Nanopore and PacBio sequence profilesOption for splice-aware mapping bowtie2 Landmead &amp; Salzberg, 2012 Short read mappingDNA mapping applications HISAT2 Kim et al., 2019 Short read mappingRNA (splice-aware) mapping, haplotype resolution bwa-mem Li, 2013 Short read mappingDNA mapping applicationsLong read mapping settings exist, but have been superceded by <code>minimap2</code>"},{"location":"level2/31_coverage_mapping/#obtaining-a-reference-sequence","title":"Obtaining a reference sequence","text":"<p>In order to perform read mapping we need to have a downloaded copy of the sequence we wish to map against. Generally this will be either a completed or near-complete reference genome of the species/strain of interest, or a closely related organism. In this exercise, however, we are going to use a single gene so that when we later visualise the results in <code>Geneious</code> the display will be easy to navigate.</p> <p>Performing the reference download will not be required as you have been provided with a reference sequence for this exercise. Navigate to the <code>/nesi/project/nesi03181/phel/USERNAME/level2/mapping/</code> folder and check the contents of the <code>references/</code> folder to begin.</p> <p>code</p> <pre><code>cd /nesi/project/nesi03181/phel/USERNAME/level2/mapping/\nls references/\n</code></pre> <p>There are two sequence files in this directory. The larger of the two is a copy of the representative genome for Mycoplasmopsis bovis 8790 (accession NZ_LAUS00000000) and the smaller file is the 16S rRNA gene sequence extracted from this genome.</p> <p>Finding the correct reference sequence is less bioinformatics and more a matter of subject expertise so we are not covering the details of how to perform this work. The commands used to produce these reference sequences are provided below in case this is something you wish to repeat in your own work.</p> Downloading the reference sequence <p>If you are interested in how this sequence was obtained, it was downloaded from the NCBI GenBank database using the <code>Entrez Direct</code> toolkit provided by NCBI (https://www.ncbi.nlm.nih.gov/books/NBK179288/). Once the genome was obtained the coordinates of the 16S ribosomal RNA sequence were found by browsing the annotation information for the genome (available here) by simply searching the web page for the text '16S ribosomal RNA'.</p> <p>The coordinate information for this sequence if given by the line entry <code>rRNA 41621..43134</code> which we can use in combination with <code>seqmagick</code> to extract just the region of interest for our mapping example.</p> <p>code</p> <pre><code>module purge\nmodule load entrez-direct/13.3\nmodule load seqmagick/0.8.4-gimkl-2020a-Python-3.8.2\n\nefetch -format fasta -db sequences -id NZ_LAUS01000004 &gt; Mbovis_87900.genome.fna\nseqmagick convert --cut 41621-43134 Mbovis_87900.genome.fna Mbovis_87900.16S_rRNA.fna\n</code></pre>"},{"location":"level2/32_illumina_mapping/","title":"3.2 - Mapping Illumina sequences to a reference","text":""},{"location":"level2/32_illumina_mapping/#32-mapping-illumina-sequences-to-a-reference","title":"3.2 - Mapping Illumina sequences to a reference","text":"<p>time</p> <ul> <li>Teaching: 10 minutes</li> <li>Exercises: 20 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/32_illumina_mapping/#objectives","title":"Objectives","text":"<ul> <li>Use <code>bowtie2</code> to index a reference genome and map DNA paired-end reads against a reference genome.</li> </ul>"},{"location":"level2/32_illumina_mapping/#keypoints","title":"Keypoints","text":"<ul> <li>Understand how to index a reference sequence for mapping.</li> <li>Understand how to apply <code>bowtie2</code> to map a set of DNA paired-end reads to the reference.</li> </ul>"},{"location":"level2/32_illumina_mapping/#indexing-the-reference-sequence","title":"Indexing the reference sequence","text":"<p>Before we can map our sequence data to the reference genome (or gene sequence) obtained in the previous exercise we need to perform a step known as indexing. How this process works is well beyond the scope of this tutorial, but it is a process of performing a scan of the reference sequence and transforming it into an organised format amenable to the <code>bowtie2</code> rapid mapping algorithm.</p> <p>Navigate to the <code>/nesi/project/nesi03181/phel/USERNAME/level2/mapping/</code> directory and perform the following commands:</p> <p>code</p> <pre><code>cd /nesi/project/nesi03181/phel/USERNAME/level2/mapping/\n</code></pre> <p>code</p> <pre><code>module purge\nmodule load Bowtie2/2.4.5-GCC-11.3.0\n\nbowtie2-build references/Mbovis_87900.16S_rRNA.fna references/Mbovis_87900.16S_rRNA\n</code></pre> <p>This will create a series of files with the prefix <code>Mbovis_87900.16S_rRNA</code> and extensions <code>.bt2</code> in the <code>references/</code> folder. The number of files depends on the size of the reference sequence which was indexed, but collectively these files comprise the index for the reference. When we perform mapping we specify the target as the file path <code>reference/Mbovis_87900.16S_rRNA</code> and <code>bowtie2</code> will automatically find and make sense of the index files.</p> <p>Exercise</p> <p>Once this has completed, also index the full genome file (<code>Mbovis_87900.genome.fna</code>) to practice mapping against a more realistic reference sequence.</p> Solution <p>code</p> <pre><code>bowtie2-build references/Mbovis_87900.genome.fna references/Mbovis_87900.genome\n</code></pre>"},{"location":"level2/32_illumina_mapping/#mapping-reads-with-bowtie2","title":"Mapping reads with <code>bowtie2</code>","text":"<p>Once you have an index produced, it is now time to map the short sequences against the reference. The nature of the <code>bowtie2</code> mapping tool is that it can be run with one of several preset configurations depending on your requirements, or you can devote quite a bit of time ot fine-tuning the parameters to optimise your output. As with most tools, you can view these options by running the command with the <code>-h</code> (help) parameter, or read the online manual for the tool.</p> <p>For today, we will only discuss the parameters which we are going to use:</p> Parameter Value Purpose <code>--sensitive</code> Use the 'sensitive' mapping parameters for end-to-end read mapping.Mapping can be performed on a sliding scale changing sensitivity (thoroughness) for speed. <code>-x</code> <code>references/Mbovis_87900.16S_rRNA</code> The path to the index file(s) corresponding to the reference sequence <code>-1</code> <code>reads/Mbovis_87900.miseq_R1.fq.gz</code> The forward reads file to be mapped <code>-2</code> <code>reads/Mbovis_87900.miseq_R2.fq.gz</code> The reverse reads file to be mapped <code>-S</code> <code>Mbovis_87900.16S_rRNA.bowtie2.sam</code> The output <code>sam</code> file to which the results are to be written <p>Assemble these values into a command and run:</p> <p>code</p> <pre><code>bowtie2 --sensitive \\\n    -x references/Mbovis_87900.16S_rRNA \\\n    -1 reads/Mbovis_87900.miseq_R1.fq.gz \\\n    -2 reads/Mbovis_87900.miseq_R2.fq.gz \\\n    -S Mbovis_87900.16S_rRNA.bowtie2.sam\n</code></pre> <p>Exercise</p> <p>Repeat the mapping command, this time against the full reference genome.</p> <p>You will need to increase the number of computing threads to the maximum available in your <code>JupyterHub</code> session to get a reasonable run time. Use the help manual to find the parameter for adjusting the thread number.</p> Solution <p>Where <code>${n}</code> is the number of threads available in your JupyterHub session:</p> <p>code</p> <pre><code>bowtie2 --sensitive \\\n    --threads ${n} \\\n    -x references/Mbovis_87900.genome \\\n    -1 reads/Mbovis_87900.miseq_R1.fq.gz \\\n    -2 reads/Mbovis_87900.miseq_R2.fq.gz \\\n    -S Mbovis_87900.genome.bowtie2.sam\n</code></pre>"},{"location":"level2/33_nanopore_mapping/","title":"3.3 - Mapping Oxford Nanopore sequences to a reference","text":""},{"location":"level2/33_nanopore_mapping/#33-mapping-oxford-nanopore-sequences-to-a-reference","title":"3.3 - Mapping Oxford Nanopore sequences to a reference","text":"<p>time</p> <ul> <li>Teaching: 10 minutes</li> <li>Exercises: 5 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/33_nanopore_mapping/#objectives","title":"Objectives","text":"<ul> <li>Use <code>minimap2</code> to index a reference genome and map long-read data produced with Oxford Nanopore sequencing technology to a reference genome.</li> </ul>"},{"location":"level2/33_nanopore_mapping/#keypoints","title":"Keypoints","text":"<ul> <li>Understand how to index a reference sequence for mapping with <code>minimap2</code>, and know when you may need to perform this step.</li> <li>Understand how to apply <code>minimap2</code> to map a set of DNA paired-end reads to the reference.</li> </ul>"},{"location":"level2/33_nanopore_mapping/#indexing-the-reference-sequence","title":"Indexing the reference sequence","text":"<p>Similar to <code>bowtie2</code>, it is possible to create a pre-computed index file for performing mapping using <code>minimap2</code>. However this is not strictly necessary - <code>minimap2</code> will automatically read the reference file and determine if it is indexed or not. If the file is in fasta format, <code>minimap2</code> will produce an index file on the fly for performing the alignment operations. If the index file is already indexed, this step is skipped.</p> When should we index the refernece file? <p>Whether or not you need to manually index your reference depends on the situation. The indexing process in <code>minimap2</code> is very fast and for quick, one-off applications you can probably skip it. If you are producing an index which is going to be reused many times it might be worth creating a common index file which can be recycled between mapping applications. This might particularly be desirable if you application is for some routine diagnostic purpose, so that you have a single mapping index for all analyses for quality tradking purposes.</p> <p>To perform mapping, navigate to the <code>/nesi/project/nesi03181/phel/USERNAME/level2/mapping/</code> directory again, and map the smaller fasta file using <code>minimap2</code>:</p> <p>code</p> <pre><code>module purge\nmodule load minimap2/2.24-GCC-11.3.0\n\nminimap2 -d references/Mbovis_87900.genome.mmi references/Mbovis_87900.genome.fna\n</code></pre> <p>Note that this time we provide the target path for the index file as a named parameter before providing the path to the fasta file to be indexed. We also have specified an extension for the output file (<code>.mmi</code>). Unlike <code>bowtie2</code>, which splits the indexing information over a number of files as size dictates, <code>minimap2</code> contains all indexing information in a single file. This can be handy for portability, and can also be convenient when writing commands as it is easier to use tab-completion to get to the index file directly.</p>"},{"location":"level2/33_nanopore_mapping/#mapping-reads-with-minimap2","title":"Mapping reads with <code>minimap2</code>","text":"<p>Similar to <code>bowtie2</code> there are a number of pre-configured settings for mapping with <code>minimap2</code>.</p> <p>For most cases, the most important parameter to be aware of is the <code>-x</code> parameter. This is the toggle for applying appropriate parameters for applying short- or long-read mapping, with mapping profiles for PacBio and Nanopore sequence reads. The data we are working with in this exercise is based on the Oxford Nanopore MinION technology so this is the mapping preset we will apply.</p> <p>code</p> <pre><code>minimap2 -ax map-ont references/Mbovis_87900.genome.mmi reads/Mbovis_87900.nanopore.fq.gz &gt; Mbovis_87900.genome.nanopore.sam\n</code></pre>"},{"location":"level2/33_nanopore_mapping/#differences-in-sequence-quality-between-short-and-long-read-platforms","title":"Differences in sequence quality between short- and long-read platforms","text":"<p>The nature of long-read sequencing is that it is inherently more error-prone than short-read sequencing. This is due to the fundamental difference in approach between long-read platforms, which sequence individual nucleic acid sequences, and short-read platforms which perform PCR amplification to create a clonal population of sequences from which sequencing signals are generated.</p> <p>While the error rates of Nanopore sequences are drastically better than they were a few years ago they are still more prone to spontaneous sequencing errors when compared with Illumina sequences. In practice, this error can be as little as 1% difference between platforms but when Nanopore sequencing is producing sequences which are thousands of nucleotides in length, a 1% error rate does result in significant divergence from the original sequence.</p> <p>In addition to stochastic error, which all platforms suffer from, there are also instances of platform-specific errors in which particular nucleotide motifs exhibit a propensity to certain error types. Dedicated long-read mapping tools, including <code>minimap2</code>, are trained on these platform-specific error profiles and can achieve greater mapping accuracy by according for these errors when they are aware of the sequencing platform used to produce the input sequences.</p> <p>It is therefore critical to note which sequencing method was used in producing your HTS data and perform reference mapping with parameters customised to the platform.</p>"},{"location":"level2/34_mapping_filters/","title":"3.4 - Filtering and compressing sam files","text":""},{"location":"level2/34_mapping_filters/#34-filtering-and-compressing-sam-files","title":"3.4 - Filtering and compressing sam files","text":"<p>time</p> <ul> <li>Teaching: 20 minutes</li> <li>Exercises: 20 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/34_mapping_filters/#objectives","title":"Objectives","text":"<ul> <li>Use <code>samtools</code> to sort and compress a raw <code>sam</code> file into the <code>bam</code> format.</li> <li>Use <code>samtools</code> to filter a <code>bam</code> file into either the successfully mapped, or unmapped reads.</li> <li>Use <code>samtools</code> to recover reads in <code>fastq</code> format from a <code>bam</code> file.</li> </ul>"},{"location":"level2/34_mapping_filters/#keypoints","title":"Keypoints","text":"<ul> <li>Understand the reasons for sorting and compressing files in the <code>sam</code> and <code>bam</code> formats.</li> <li>Understand the situations in which you may wish to filtering a <code>sam</code>/<code>bam</code> file and what the downstream applications of the output would be.</li> </ul>"},{"location":"level2/34_mapping_filters/#why-we-need-to-compress-and-filter-sam-files","title":"Why we need to compress and filter <code>sam</code> files","text":"<p>Now that we have created some basic mapping data in the <code>sam</code> files , let's take a look at the size of these files. Navigate to the <code>/nesi/project/nesi03181/phel/USERNAME/level2/mapping/</code> directory and run the following command:</p> <p>code</p> <pre><code>ls -sh *.sam\n</code></pre> Output <pre><code>1.3G Mbovis_87900.16S_rRNA.bowtie2.sam\n1.5G Mbovis_87900.genome.bowtie2.sam\n52M Mbovis_87900.genome.nanopore.sam\n</code></pre> <p>These are quite large files - especially in the case of the 16S rRNA mapping file where the mapping success rate was only around 0.4% of the Illumina sequences. Why is this file so far? There are two reasons.</p> <p>When mapping is performed, all of the reads in the input file are recorded</p> <p>This is a deliberate design feature of the mapping tools as depending on our context we may prefer to have the mapped or unmapped reads. For example, if we are trying to perform a reference-based genome assembly or determine the sequencing coverage of the genome (or a particular region) having the mapped reads is critical.</p> <p>However, if we are trying to subtract the host reads from a set of reads, such as when we are trying to find a pathogen, the reads which do not map to the host genome are actually the ones we are interested in examining in further detail.</p> <p>Although there are only about 8,700 reads actually mapped to the reference sequence in the 16S rRNA file (we'll see how to calculate that number in the next session), the file still contains all 2 million reads from the Illumina library.</p>"},{"location":"level2/34_mapping_filters/#sorting-and-compressing-sam-files","title":"Sorting and compressing <code>sam</code> files","text":"<p>The first step of reducing the file size is to efficienctly compress the contents of the sam file. Fortunately there is a built-in solution for this - the sam file specification also has a binary-encoded equivalent which records the exact same information, in a much more efficieny format. This is the Binary Alignment/Map (<code>bam</code>) format.</p> <p>When performing this compression from sam to bam we also use this opportunity to sort the mapped reads in terms of their starting position in the reference sequence. This sorting is important as it increases the speed of many of the operations we need to perform using a <code>sam</code> file, particular when producing coverage statistics. Because the sorting and compression can be performed in a single command line operation we tend to do these things together once and never worry about it again.</p> <p>We should always sort our sequences!</p> <p>In the situations where you need your mapped reads sorted, operations will fail if the reads are not sorted. In situations where you do not need them sorted, operations will succeed.</p> <p>It is generally just easier to sort as soon as mapping completes then never worry about it again.</p> <p>The main tool used for handling sam and bam files is called <code>samtools</code>. Load the module and execute the file below. While it is running, refresh yourself on what the <code>|</code> operator in the command is doing.</p> <p>code</p> <pre><code>module purge\nmodule load SAMtools/1.16.1-GCC-11.3.0\n\nsamtools view -bS Mbovis_87900.16S_rRNA.bowtie2.sam | samtools sort -o Mbovis_87900.16S_rRNA.bowtie2.bam\n</code></pre> What is the pipe for... <p>This is redirection, taking the output of the first <code>samtools</code> command and passing it as input into the second command.</p> <p>The reason we need to redirect the data from one <code>samtools</code> command to another is due to the behaviour of the <code>samtools sort</code> subcommand. If you examine the manual for this command, you will see that while the output of the command can be written in sam or bam format the input must be bam.</p> <p>We therefore need to use the <code>samtools view</code> subcommand to first convert the sam file into bam format. Rather than write the results to NeSI's hard drive then perform <code>samtools sort</code> as a second command we can redirect between the commands to save on hard drive space and speed up the operation.</p> <p>Compare the file sizes between the <code>sam</code> and <code>bam</code> files:</p> <p>code</p> <pre><code>ls -sh Mbovis_87900.16S_rRNA.bowtie2.sam Mbovis_87900.16S_rRNA.bowtie2.bam\n</code></pre> Output <pre><code>1.3G Mbovis_87900.16S_rRNA.bowtie2.sam\n470M Mbovis_87900.16S_rRNA.bowtie2.bam\n</code></pre> <p>Do we need both files...</p> <p>There is no point in retaining the original sam file at this point, as the information it contains is more efficiently encoded within the bam file.</p> <p>If working with real data, this is the point you should delete your sam file. In this training exercise, it does not matter whether you delete the sam file or not.</p> <p>By compressing the data of the sam file into the bam file we have already reduced its size to about one third of the original. Not only will this save us space on NeSI, it also makes downloading the data much quicker, and also makes subsequent analyses of the file faster as it requires less time to read the smaller bam file.</p> <p>Exercise</p> <p>Sort and compress any other <code>sam</code> files you have produced during the previous mapping tutorials.</p> Solution <p>code</p> <pre><code>for i in bowtie2 nanopore;\ndo\n    samtools view -bS Mbovis_87900.genome.${i}.sam | samtools sort -o Mbovis_87900.genome.${i}.bam\ndone\n</code></pre>"},{"location":"level2/34_mapping_filters/#splitting-bam-files-to-separate-mapped-and-unmapped-reads","title":"Splitting <code>bam</code> files to separate mapped and unmapped reads","text":"<p>Most of the time when we are mapping against a reference sequence, we are interested in the sequences which successfully mapped to the target. In these cases, having a bam file which contains all of the unmapped reads is not particularly useful so we will now practice filtering bam files according to the mapping state of the reads in the file.</p> <p>This is simple to do in terms of the command which needs to be run, but it the meaning of the command can be a bit confusing. Within the sam and bam file format specification is a numeric flag which may be assign to an unmapped read, denoting its status as such. Because it is a positive marker of unmapped state, if we want to filter for mapped reads we need to filter for sequences which do not have the marker.</p> <p>When running the <code>samtools view</code> command there is a pair of optional paramters which allow us to filter reads by their mapping flags. We use the <code>-f</code> to keep reads with the flag(s) we specify, or <code>-F</code> to reject reads with the flag(s) we specify. As the unmapped flag is only applied to reads which fail to map to the reference we use the following logic:</p> <ul> <li><code>-f 4</code> = Include reads with the unmapped flag = Keep unmapped reads</li> <li><code>-F 4</code> = Reject reads with the unmapped flag = Keep mapped reads</li> </ul> <p>The value <code>4</code> is the numeric flag for ummapped reads</p> <p>We will run <code>samtools</code> to filter the <code>Mbovis_87900.16S_rRNA.bowtie2.bam</code> file to only keep mapped reads:</p> <p>code</p> <pre><code>samtools view -h -F 4 -b Mbovis_87900.16S_rRNA.bowtie2.bam &gt; Mbovis_87900.16S_rRNA.bowtie2.mapped.bam\n</code></pre> <p>In this command, the difference between keeping mapped or unmapped reads is the case of the <code>-f</code> flag. It is really easy to get confused about these values so be very careful when applying this command.</p> <p>In this case we are expecting only a handful of reads to have actually mapped to the reference sequence so a good acid test for whether we used the right commnad will be to check the size of the output file. If it is roughly the same size as the input then we have probably kept the unmapped reads. If it is a lot smaller, we have most likely kept only the mapped reads.</p> <p>code</p> <pre><code>ls -sh Mbovis_87900.16S_rRNA.bowtie2.bam Mbovis_87900.16S_rRNA.bowtie2.mapped.bam\n</code></pre> Output <pre><code>470M Mbovis_87900.16S_rRNA.bowtie2.bam\n1.8M Mbovis_87900.16S_rRNA.bowtie2.mapped.bam\n</code></pre> <p>Exercise</p> <p>Write a loop to filter the genome mapping files, creating an output file for both the mapped and unmapped reads for each case.</p> Solution <p>code</p> <pre><code>for i in bowtie2 nanopore;\ndo\n    samtools view -h -F 4 -b Mbovis_87900.genome.${i}.bam &gt; Mbovis_87900.genome.${i}.mapped.bam\n    samtools view -h -f 4 -b Mbovis_87900.genome.${i}.bam &gt; Mbovis_87900.genome.${i}.unmapped.bam\ndone\n</code></pre>"},{"location":"level2/35_mapping_statistics/","title":"3.5 - Summarising coverage information from a mapping file","text":""},{"location":"level2/35_mapping_statistics/#35-summarising-coverage-information-from-a-mapping-file","title":"3.5 - Summarising coverage information from a mapping file","text":"<p>time</p> <ul> <li>Teaching: 15 minutes</li> <li>Exercises: 20 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/35_mapping_statistics/#objectives","title":"Objectives","text":"<ul> <li>Use <code>samtools flagstat</code> to get an overview of a <code>bam</code> file content.</li> <li>Use <code>samtools depth</code> to produce a coverage table from the complete <code>bam</code> record.</li> <li>Use <code>samtools depth</code> to produce a coverage table from a subsection of a complete <code>bam</code> record.</li> </ul>"},{"location":"level2/35_mapping_statistics/#keypoints","title":"Keypoints","text":"<ul> <li>Understand the reasons for summarising a <code>bam</code> file without digging into the contents in detail.</li> <li>Understand the situations in which you might require the coverage information, either in its entirity or for a specific genomic region.</li> </ul>"},{"location":"level2/35_mapping_statistics/#summarising-the-contents-of-a-bam-file","title":"Summarising the contents of a <code>bam</code> file","text":"<p>This section will start with a new mapping file, although you could apply these commands to anything produced as part of the filtering exercises instead. Navigate to the <code>/nesi/project/nesi03181/phel/USERNAME/level2/mapping_statistics/</code> directory and view the contents.</p> <p>We wil take a look at the high-level summary of the contents of these files with the <code>samtools flagstat</code> command:</p> <p>code</p> <pre><code>module purge\nmodule load SAMtools/1.16.1-GCC-11.3.0\n\nsamtools flagstat SRR18260232.raw_reads.bam\n</code></pre> Output <pre><code>8630882 + 0 in total (QC-passed reads + QC-failed reads)\n8629544 + 0 primary\n1338 + 0 secondary\n0 + 0 supplementary\n0 + 0 duplicates\n0 + 0 primary duplicates\n3236663 + 0 mapped (37.50% : N/A)\n3235325 + 0 primary mapped (37.49% : N/A)\n8629544 + 0 paired in sequencing\n4314772 + 0 read1\n4314772 + 0 read2\n3040828 + 0 properly paired (35.24% : N/A)\n3049552 + 0 with itself and mate mapped\n185773 + 0 singletons (2.15% : N/A)\n0 + 0 with mate mapped to a different chr\n0 + 0 with mate mapped to a different chr (mapQ&gt;=5)\n</code></pre> Interpreting the flagstat output <p>This is not exactly easy to interpret. The documentation is here and requires you to look through the <code>sam</code> file specification to understand the fields.</p> <p>In brief, the pairs of numbers on each line represent the number of mappings with passed/failed the alignment quality conditions. This is not the same thing as the fastq quality score - when we map reads there is a scoring system used to assess how well a read sticks to the reference at a given position. Depending on how mapping is performed there may be reads which do align to a region, but too poorly for <code>samtools</code> to consider them a likely match.</p> <p>Going through the lines of the output, the most important parts are:</p> Term Meaning <code>X + Y in total (QC-passed reads + QC-failed reads)</code> Total mappings (sum of primary, secondary, and supplementary) <code>X + Y primary</code> Number of reads mapped without multiple or chimeric aligments (i.e. the most reliable ones) <code>X + Y secondary</code> Number of reads mapped with multiple mappings <code>X + Y supplementary</code> Number of reads mapped with a chimeric alignment <code>X + Y duplicates</code> Number of duplicate mappings <code>X + Y properly paired (Z% : N/A)</code> Number of instances where paired reads are mapped together in the correct orientation <code>X + Y singletons (Z% : N/A)</code> Number of instances of a read mapping when its paired partner failed to do so <p>Understanding the 'duplicates' term</p> <p>The term duplicates is used to represent instances where two or more identical sequences mapped to the region, not different reads mapping to the same region in a manner that overlaps. In these instances, it is phenominally rare to see two identical reads produced as part of library preparation and they are much more commonly due to PCR artefacts which occur during library preparation, or some form of read duplication during sequencing.</p> <p>In some instances it is worth identifying and removing these duplication events using a tool like <code>picard</code> (documentation).</p> <p>If you are working with amplicon sequence, where you have produced a PCR product which is then sent for sequencing your duplication rate will be high but this is an expected consequence of the method used to produce your input DNA. In such cases a high duplication rate is expected.</p>"},{"location":"level2/35_mapping_statistics/#producing-a-coverage-table-from-a-complete-alignment","title":"Producing a coverage table from a complete alignment","text":"<p>Now that we have seen a high-level overview of the mapping in the table, how do we get something actionable out of this? One of the common tasks we need to perform with a mapping file is to assess the depth of coverage over the reference sequence to determine</p> <p>Is there sufficient depth of coverage to be confident in the consensus sequence of our reads?</p> <p>Is the mapping depth even across the reference sequence, or does it pile up in hotspots?</p> <p>Piling into hotspots can be indicative of poor mapping, where a low complexity region of a sequence (like the poly-A tail on a virus) is attracting a lot of non-specific binding.</p> <p>In a case like this you would not want to infer the average coverage by dividing the number of mapped reads by the reference length as there are likely to be spots with no mapping.</p> <p>One of the simplest ways to view the per-position coverage of a mapping result is to use the <code>samtools depth</code> command, which reads through the <code>bam</code> file and reports the number of nucleotides mapped at each position of the reference sequence. It is very quick to run, and the output is a simple text table which can be imported into <code>Excel</code> or <code>R</code> to calculate statistics.</p> <p>By default the tool prints to the command line which is not ideal, so we will capture the results with a redirection.</p> <p>code</p> <pre><code>samtools depth -a SRR18260232.clean_reads.bam &gt; SRR18260232.clean_reads.txt\n</code></pre> <p>Running the <code>head</code> command should print the first 10 lines of the file to the command line. This is enough to see the structure of the file.</p> <p>First 10 lines of <code>SRR18260232.clean_reads.txt</code></p> <pre><code>NZ_LAUS01000004.1       1       6\nNZ_LAUS01000004.1       2       9\nNZ_LAUS01000004.1       3       9\nNZ_LAUS01000004.1       4       9\nNZ_LAUS01000004.1       5       9\nNZ_LAUS01000004.1       6       9\nNZ_LAUS01000004.1       7       9\nNZ_LAUS01000004.1       8       9\nNZ_LAUS01000004.1       9       9\nNZ_LAUS01000004.1       10      9\n</code></pre> <p>Although there are no column headers, there are only three columns in the file so it is easy to just describe the contents. The first column is the name of the sequence in the reference file which is being reported. If you have a single sequence in your reference then this will be a single value throughout the file. If you are mapping to a draft reference genome, or a genome with plasmids or other non-chromosomal elements, the content of this column will change as you move through the file.</p> <p>The second column is the nucleotide position in the reference sequence, and the third column is the number of nucleotides mapped against that position. From the command line it is hard to see the maximum depth, but we can use a quick combination of command line tools to tell us something of the mapping:</p> <p>code</p> <pre><code>cut -f3 SRR18260232.clean_reads.txt | sort | uniq -c | head -n1\n</code></pre> Output <pre><code>27988 0\n</code></pre> <p>What this command does is take the third column of the table (<code>cut</code>), sort the values (<code>sort</code>), and then tally up the unique instances and count them (<code>uniq -c</code>).</p> <p>Which means that there are 27,988 positions in the reference with 0 nucleotides mapped. This tells us that there are some gaps in our mapping. Given that the reference sequence is 254,357 nucleotides long this means that about 11% of our genome does not have reads that correspond to it.</p> <p>If you wanted to push this further, you could import the summary table into a tool like <code>Excel</code> or <code>R</code> and plot the depth of coverage along the genome or calculate the average depth and variance to decide if you wanted to trust this alignment.</p> <p>We will not be pursuring this further in this workshop, as plotting is a fairly easy task which you can perform by yourself if you are interested.</p>"},{"location":"level2/35_mapping_statistics/#producing-a-coverage-table-from-a-specific-region-of-an-alignment","title":"Producing a coverage table from a specific region of an alignment","text":"<p>Viewing the depth across the full genome is helpful if you're working with a small genome, such as a virus, but for larger organisms it's probably not going to be very informative. Depending on your application, you may not even want to view the coverage over an entire genome - there may be a specific marker gene or region of interest you wish to examine and as long as you have good coverage in this area, then that is sufficient for your application.</p> <p>It is only a minor step to modify the previous <code>samtools</code> command to restrict the mapping down to a single region, or set of regions. We can do this by modifying the <code>samtools depth</code> command in one of two ways:</p> <ol> <li>Adding the region of interest to be reported to the command line</li> <li>Specifying a <code>bed</code> file that contains the region of interest.</li> </ol> <p>The second option is good if it is a region we are going to be examining often, such as a known marker for a BAU test. It is also useful when specifying multiple regions, since it is essentially a small text table of start/stop coordinates. For simplicity we are not going to pursue this step in this training but if this is something you wish to do in your own work, you can find the documentation for writing <code>bed</code> files online here.</p> <p>For now, we are going to extract the depth information over two genes of the reference genome. There is no real trick to finding these gene coordinates - you simple need to read through an appropriate annotation file (such as the NCBI <code>genbank</code> file for the reference genome) and find them. This can also be done by downloading and viewing the genome in <code>Geneious</code>.</p> Gene Sequence Start position End position Translation elongation factor 4 NZ_LAUS01000004.1 94,986 96,779 16S ribosomal RNA NZ_LAUS01000004.1 41,621 43,134 <p>The syntax for performing this kind of depth summary is to add a parameter specifying the name of the sequence carrying the region, followed by the start and stop coordinates to be reported in the form <code>SEQUENCE:START-STOP</code>. We must also index the <code>bam</code> file, so that <code>samtools</code> is able to quickly parse the file, skipping the regions we are not interested in. Indexing will produce a file with the extension <code>bai</code>. We do not need to hold onto this file for long, it is quick to generate and not very large on disk space.</p> <p>In our case there is only a single sequence in the reference genome, so all mapping must be to this lone sequence. Nevertheless, we must still specify the sequence as part of the command</p> <p>code</p> <pre><code>samtools index SRR18260232.clean_reads.bam\nsamtools depth -a -r NZ_LAUS01000004.1:94986-96779 SRR18260232.clean_reads.bam  &gt; SRR18260232.clean_reads.tef4.txt\n\nwc -l SRR18260232.clean_reads.txt SRR18260232.clean_reads.tef4.txt\n</code></pre> Output <pre><code>254357 SRR18260232.clean_reads.txt\n1794 SRR18260232.clean_reads.tef4.txt\n256151 total\n</code></pre> <p>As you can see, this has cut the mapping space down considerably. It is now of a size that, if this was your marker of interest, you could easily plot this information and see the mapping depth along the TEF4 gene:</p> Plotting code (interest only) <p>Here's a short snippet of <code>python</code> code to create a plot of the depth along the positions in the tef4 file.</p> <p>code</p> <pre><code>import plotly.express as px\nimport pandas as pd\n\ndf = pd.read_csv(\"SRR18260232.clean_reads.tef4.txt\", sep=\"\\t\", names=[\"Chr\", \"Position\", \"Depth\"])\n\nfig = px.area(df, x=\"Position\", y=\"Depth\")\nfig.write_image(\"SRR18260232.clean_reads.tef4.svg\")\n</code></pre> Output <p></p><p></p> <p>Exercise</p> <p>Create a 16S rRNA sequence coverage report using the <code>SRR18260232.raw_reads.bam</code> mapping file as your starting point.</p> <p>If you're interested, also produce a small plot of the results.</p> Solution <p>code</p> <pre><code>samtools index SRR18260232.raw_reads.bam\nsamtools depth -a -r NZ_LAUS01000004.1:41621-43134 SRR18260232.raw_reads.bam  &gt; SRR18260232.raw_reads.16s_rrna.txt\n</code></pre>"},{"location":"level2/41_annotation_overview/","title":"4.1 - Prediction of protein coding sequences","text":""},{"location":"level2/41_annotation_overview/#41-prediction-of-protein-coding-sequences","title":"4.1 - Prediction of protein coding sequences","text":"<p>time</p> <ul> <li>Teaching: 20 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/41_annotation_overview/#objectives","title":"Objectives","text":"<ul> <li>Understand the limitations of protein coding predictions when working with annotations</li> </ul>"},{"location":"level2/41_annotation_overview/#keypoints","title":"Keypoints","text":"<ul> <li>De novo protein sequence prediction is a good starting point, but proper annotation will require significant manual curation</li> <li>Not all tools are able to predict gene boundaries over splice junctions - be careful when interpreting predictions</li> <li>Protein sequence prediction does not predict all informative genetic elements - additional tools are required for features such as rRNA, tRNA, or ncRNA elements</li> </ul>"},{"location":"level2/41_annotation_overview/#complexities-of-predicting-protein-coding-sequences-from-a-de-novo-assembly","title":"Complexities of predicting protein coding sequences from a de novo assembly","text":"<p>Prediction of genes in a genome assembly is a complicated process - there are many tools which can perform good initial predictions from assembled contigs, but there are often many biological features which confound the prediction process and make it more complicated than simply finding start and stop codons within a sequence.</p> <p>At the most basic level, searching for proteins is simply looking for open reading frames (ORF) within a contig, but in practice there are many factors which confound the process. At the biological level a number of features complicate the process of predicting protein coding regions:</p> <ol> <li>Alternate coding schemes, including the amber, umber, and ochre stop codons</li> <li>Stop codon read-through</li> <li>Splicing of intronic regions</li> </ol> <p>Simply translating the nucleotide sequence between a start/stop pairing is not sufficient to correctly identify the complete protein complement of the genome.</p> <p>Furthermore, if our genome assembly is not complete we run the risk of encountering partial coding sequeences in which either the 5' or 3' region of the sequence were not assembled. In these cases, a simple search for ORFs will fail to detect the partial sequence. The prediction of protein coding sequences must be achieved using more complicated techniques than a simple <code>grep</code> search for the start and stop codon(s).</p> <p>Similar to assembly we can perform gene prediction in either a reference-guided manner or through the use of ab initio prediction tools. We will not be covering the reference-guided approach, as it is quite simple to perform in <code>Geneious</code>, but it is not to be underestimated as a technique - particularly when working with viruses or other organisms with complex read-through or splicing properties.</p> <p>Ab initio prediction is akin to de novo assembly - the tool is created with some internal models for what coding regions look like, which are then applied to query sequences to find putative coding regions.</p> <p>Depending on the intended use of the tool, each prediction tool may be better tuned for partiular assumptions of the data. We are going to use two different tools today, one designed for prediction of prokaryotic coding sequences (which generally lack introns) and one designed primarily for eukaryotic sequences, where splicing is common.</p>"},{"location":"level2/41_annotation_overview/#limitations-of-protein-coding-predictions","title":"Limitations of protein coding predictions","text":"<p>As you will see from both examples above, protein coding prediction is at best a good starting point for identifying genes. Careful validation of each sequence needs to be performed if you are trying to produce a comprehensive annotation.</p> <p>In addition, these tools are only for prediction of protein coding sequences so if you're trying to recover a particular element from your data make sure that the tool you are using is suitable for the job.</p> <p>There are many other genomic features you may need to look for - some additional tools to examine if you are looking for a complete annotation:</p> <ol> <li>Metaxa2 (Bengtsson-Palme et al, 2015) - Prediction of small and large subunit ribosomal RNA sequences</li> <li>Barrnap - Prediction of small and large subunit ribosomal RNA sequences</li> <li>ARAGORN (Laslett &amp; Canback, 2004) - Prediction of tRNA and tm RNA sequences</li> <li>Infernal (Nawrocki &amp; Eddy et al, 2013) - Prediction of non-coding RNA sequences, requires rfam database</li> </ol>"},{"location":"level2/42_annotation_prodigal/","title":"4.2 - Prediction with prodigal","text":""},{"location":"level2/42_annotation_prodigal/#42-prediction-with-prodigal","title":"4.2 - Prediction with <code>prodigal</code>","text":"<p>time</p> <ul> <li>Teaching: 10 minutes</li> <li>Exercises: 15 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/42_annotation_prodigal/#objectives","title":"Objectives","text":"<ul> <li>Understand how to run <code>prodigal</code> for predicting protein coding regions in prokaryotic genomes</li> </ul>"},{"location":"level2/42_annotation_prodigal/#keypoints","title":"Keypoints","text":"<ul> <li>Selecting the correct prediction mode (genome or metagenome), and translation table is key to getting good outputs.</li> <li><code>prodigal</code> reports output in several formats - you do not need all of the output files, but know which ones you want to keep and why.</li> </ul>"},{"location":"level2/42_annotation_prodigal/#getting-started-and-loading-the-tool","title":"Getting started and loading the tool","text":"<p>We are going to be predicting protein coding sequences in a reference M. bovis genome using the tool <code>prodigal</code> (Hyatt et al., 2010). This is a powerful prediction tool which is quick to run, and flexible enough for most projects.</p> <p>Navigate to the <code>/nesi/project/nesi03181/phel/USERNAME/level2/annotation_prodigal/</code> directory and prepare to run <code>prodigal</code>.</p> <p>Exercise</p> <p>Use your knowledge of <code>slurm</code> to find and load the more current version of <code>prodigal</code> available on NeSI. Once loaded, run with the help paramter (<code>-h</code>) to confirm that the module has successfully loaded.</p> Solution <p>code</p> <pre><code># One of the following:\nmodule spider prodigal\n\nmodule avail prodigal\n\n# Load and confirm\nmodule purge\nmodule load prodigal/2.6.3-GCCcore-7.4.0\n\nprodigal -h\n</code></pre> Output <pre><code>Usage:  prodigal [-a trans_file] [-c] [-d nuc_file] [-f output_type]\n                [-g tr_table] [-h] [-i input_file] [-m] [-n] [-o output_file]\n                [-p mode] [-q] [-s start_file] [-t training_file] [-v]\n\n        -a:  Write protein translations to the selected file.\n        -c:  Closed ends.  Do not allow genes to run off edges.\n        -d:  Write nucleotide sequences of genes to the selected file.\n        -f:  Select output format (gbk, gff, or sco).  Default is gbk.\n        -g:  Specify a translation table to use (default 11).\n        -h:  Print help menu and exit.\n        -i:  Specify FASTA/Genbank input file (default reads from stdin).\n        -m:  Treat runs of N as masked sequence; don't build genes across them.\n        -n:  Bypass Shine-Dalgarno trainer and force a full motif scan.\n        -o:  Specify output file (default writes to stdout).\n        -p:  Select procedure (single or meta).  Default is single.\n        -q:  Run quietly (suppress normal stderr output).\n        -s:  Write all potential genes (with scores) to the selected file.\n        -t:  Write a training file (if none exists); otherwise, read and use\n            the specified training file.\n        -v:  Print version number and exit.\n</code></pre> <p>There are quite a few options given here, which we can split into input and output parameters.</p> <p>Input parameters</p> Parameter Function <code>-i</code> Specify FASTA/Genbank input file (default reads from stdin). <code>-p</code> Select procedure (single or meta).  Default is single. <code>-g</code> Specify a translation table to use (default 11). <p>The first parameter here should be obvious, so will not be discussed.</p> <p>The <code>-p</code> parameter is an important one to pay attention to, as it determines whether or not we are predicting sequences in an single genome (i.e. a genome obtained from a pure culture) or a metagenomic mix of sequences. In <code>single</code> mode prediction goes through a round of training against the input sequences before producing a prediction output tailored to your contigs. The <code>meta</code> mode uses pre-calculated profiles of gene features. Strictly speaking it is possible to run <code>meta</code> mode on an isolate genome, or <code>single</code> mode on a metagenome but the results do differ.</p> <p>Selecting the translation table (<code>-g</code>) is something we usually do not need to wory about. <code>prodigal</code> supports numbers 1 through 25 of the NCBI genetic codes. By default, it will start with code 11 (bacteria, archaea, and plastids) but shift to code 4 if the predictions are too short. This is important to us because genetic code 4 corresponds to the Mycoplasmataceae - the bacterial family that contains the genera Mycoplasma and Ureaplasma, and the family Spiroplasmataceae. These lineages have repurposed <code>UGA</code> from a stop codon to a tryptophan codon.</p> <p>Output parameters</p> Parameter Function <code>-a</code> Write protein translations to the selected file. <code>-d</code> Write nucleotide sequences of genes to the selected file. <code>-o</code> Specify output file (default writes to <code>stdout</code>). <code>-f</code> Select output format. Default is gbk. <p>Three of these capture the prediction information in commonly used formats. The output of <code>-d</code> and <code>-a</code> are simply fasta format, and the <code>-o</code> (or <code>stdout</code>) output are in GenBank format. This is really the same information reported in multiple different ways, but the different files have different uses so it's recommended to take a few of them.</p> <p>At the very least, we should be capturing either <code>-d</code> and <code>-a</code>, or <code>-o</code> to get the full prediction. It is helpful to have both the nucleotide and protein sequence of a coding region, as the different annotation techniques can be applied to each one, and certain methods of phylogenetic analysis favour one sequence type over the other.</p>"},{"location":"level2/42_annotation_prodigal/#predicting-protein-coding-regions","title":"Predicting protein coding regions","text":"<p>One of the nice features of <code>prodigal</code> is that it does not take a lot of resources to run, so we can easily run it without resorting to <code>slurm</code> for a single genome.</p> <p>code</p> <pre><code>prodigal -p single -g 4 -i input/M_bovis.NZ_CP005933.fna \\\n    -d outputs/M_bovis.NZ_CP005933.prod.fna \\\n    -a outputs/M_bovis.NZ_CP005933.prod.faa \\\n    -o outputs/M_bovis.NZ_CP005933.prod.gbk\n</code></pre> Output <pre><code>-------------------------------------\nPRODIGAL v2.6.3 [February, 2016]         \nUniv of Tenn / Oak Ridge National Lab\nDoug Hyatt, Loren Hauser, et al.     \n-------------------------------------\nRequest:  Single Genome, Phase:  Training\nReading in the sequence(s) to train...948516 bp seq created, 29.29 pct GC\nLocating all potential starts and stops...30752 nodes\nLooking for GC bias in different frames...frame bias scores: 2.38 0.43 0.19\nBuilding initial set of genes to train from...done!\nCreating coding model and scoring nodes...done!\nExamining upstream regions and training starts...done!\n-------------------------------------\nRequest:  Single Genome, Phase:  Gene Finding\nFinding genes in sequence #1 (948516 bp)...done!\n</code></pre> <p>Which value of <code>-g</code> are we using?</p> <p>As mentioned above, for most cases the standard genetic code is the correct starting place, but we are working with an M. bovis sequence, which uses translation table 4.</p> <p>This will only take a few seconds to complete. Once done, we can quickly see how many protein coding sequences were predicted by counting the number of sequences in either of the output fasta files.</p>"},{"location":"level2/42_annotation_prodigal/#interpretting-the-output-format","title":"Interpretting the output format","text":"<p>code</p> <pre><code>grep -c \"&gt;\" outputs/M_bovis.NZ_CP005933.prod.fna outputs/M_bovis.NZ_CP005933.prod.faa\n</code></pre> Output <pre><code>outputs/M_bovis.NZ_CP005933.prod.fna:792\noutputs/M_bovis.NZ_CP005933.prod.faa:792\n</code></pre> <p>Since this is a chromosome downloaded from the NCBI RefSeq database we can look to the official annotation and see how many proteins we should expect to find. A copy of the protein coding sequences from this genome have been provided in your <code>outputs/</code> folder.</p> <p>Exercise</p> <p>Use <code>grep</code> to count the number of protein coding sequences in the official annotation for this M. bovis genome.</p> Solution <p>code</p> <pre><code>grep -c \"&gt;\" outputs/M_bovis.canonical.faa\n</code></pre> Output <pre><code>765\n</code></pre> <p>This number is slightly fewer than what <code>prodigal</code> predicts for us, but the numbers are very close. We would need to examine the predictions in detail to understand what is causing the difference.</p> <p>If we inspect the output of the fasta files, you will notice that there is a lot of metadata stored in each line. For example:</p> <p>code</p> <pre><code>head -n1 outputs/M_bovis.NZ_CP005933.prod.faa\n</code></pre> Output <pre><code>NZ_CP005933.1_1 # 1 # 1401 # 1 # ID=1_1;partial=10;start_type=Edge;rbs_motif=None;rbs_spacer=None;gc_cont=0.293\n</code></pre> <p>There is quite a lot to unpack here - some is quite important to know and other parts are just descriptive. We can break down the results like so:</p> <p>prodigal metadata</p> <pre><code>&gt;NZ_CP005933.1_1 # 1 # 1401 # 1 # ID=1_1;partial=10;start_type=Edge;rbs_motif=None;rbs_spacer=None;gc_cont=0.293\n|              |   |   |      |   |      |\n|              |   |   |      |   |      Gene completeness\n|              |   |   |      |   Unique gene ID\n|              |   |   |      Orientation\n|              |   |   Stop position\n|              |   Start position\n|              Gene suffix\nContig name\n</code></pre> <p>Interpreting the prodigal header</p> <p>Contig name and Gene suffix</p> <p>As <code>prodigal</code> is only concerned with predicting sequences, not annotating them, there is no functional information which can be used to guide the name of each prediction. For simplicity, protein predictions are simply named as <code>[CONTIG NAME]_[PREDICTION]</code>. This has some nice implications when we want to study gene synteny but for the most part numbering off the predicitons in the order they are made is the simpliest way to generate names.</p> <p>Start, Stop and Orientation</p> <p>The next three numbers provide the nucleotide coordinates of the coding sequence and the orientation (1 for forward, -1 for reverse). Again, these are very useful when tracking down the genomic context of a sequence and can sometimes provide a quick-and-dirty means for spotting rearrangements.</p> <p>Unique gene ID</p> <p>After these parameters are done, there are a series of keyword-linked pieces of information about the sequence. The unique gene identifier is used to link the entries in the fasta file to the output obtained from the <code>-o</code> (or <code>stdout</code>) channel. Similar to the prediction number, it is simply derived from the order of contigs and sequence of predictions.</p> <p>Gene completeness</p> <p>The main piece of information we are going to inspect is whether or not the prediction is complete of not. The 'partial' keyword provides a two digit code that reports the status of the prediction, the first digit corresponds to the start of the sequence and the second to the end of the sequence. The values these can take are either 0 (complete) or 1 (incomplete). A fully complete prediction will therefore have a code of <code>00</code>, and a partial predictions can be:</p> <ul> <li><code>01</code> - Started, but no end found - typically when a prediction runs off the end of a contig</li> <li><code>10</code> - No start identified, but a complete end found - often a sequence which occurs at the start of the contig</li> <li><code>11</code> - No ends found - likely due to predicting for a very short contig</li> </ul> <p>You can also inspect the sequences to see if they appear complete. Typically protein predictions begin with a methionine (M) amino acid residue, as this is the translation of the <code>ATG</code> codon. There is no residue which corresponds to stop (as stop is a gap in translation) so <code>prodigal</code> reports the stop position with an asterisk (<code>*</code>).</p>"},{"location":"level2/43_annotation_augustus/","title":"4.3 - Prediction with AUGUSTUS","text":""},{"location":"level2/43_annotation_augustus/#43-prediction-with-augustus","title":"4.3 - Prediction with <code>AUGUSTUS</code>","text":"<p>time</p> <ul> <li>Teaching: 10 minutes</li> <li>Exercises: 30 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/43_annotation_augustus/#objectives","title":"Objectives","text":"<ul> <li>Understand how to run <code>AUGUSTUS</code> for predicting protein coding regions in genomes</li> </ul>"},{"location":"level2/43_annotation_augustus/#keypoints","title":"Keypoints","text":"<ul> <li>Selecting the best prediction model is key to getting accurate predictions.</li> <li>You can create your own training models if there are no closely related organisms for your species of interest.</li> </ul>"},{"location":"level2/43_annotation_augustus/#getting-started-with-the-tool","title":"Getting started with the tool","text":"<p>Unlike prokaryotic genomes, the genes of eukaryotes carry intronic sequences which need to be spliced out of the gene sequence before undergoing translation. The detection of splicing boundaries is a difficult task, as there are many organism-specific patterns used to mark splice sites.</p> <p>Protein prediction tools for this purpose typically come with  number of pre-trained models for finding protein domains within contigs, but if there is no model for your organism, or a closely related lineage, then results may not be ideal.</p> <p>A recently published article (Scalzitti et al., 2020) which profiled a number of these tools found <code>AUGUSTUS</code> to be one of the best performing tools for gene prediction in eukaryotic organisms, so this is what we will use today.</p> <p>Note: <code>AUGUSUTUS</code> does require training against a closely related model organisms to generate accurate predictions, which we do not have for this workshop. We will instead be performing predictions with a few different models and seeing how the outputs differ.</p> <p>Exercise</p> <p>Find the latest version of <code>AUGUSTUS</code> on NeSI and load it.</p> Solution <p>code</p> <pre><code>module spider augustus\n\nmodule purge\nmodule load AUGUSTUS/3.5.0-gimkl-2022a\n\naugustus\n</code></pre> Output <pre><code>AUGUSTUS (3.5.0) is a gene prediction tool.\nSources and documentation at https://github.com/Gaius-Augustus/Augustus\n\nusage:\naugustus [parameters] --species=SPECIES queryfilename\n\n'queryfilename' is the filename (including relative path) to the file containing the query sequence(s)\nin fasta format.\n\nSPECIES is an identifier for the species. Use --species=help to see a list.\n\nparameters:\n...\n\nFor a complete list of parameters, type \"augustus --paramlist\". A description of the important ones can be found in the file RUNNING-AUGUSTUS.md.\n</code></pre>"},{"location":"level2/43_annotation_augustus/#predicting-protein-coding-regions","title":"Predicting protein coding regions","text":"<p>The sequence(s) we have to work with today are from the brown marmorated stink bug (Halyomorpha halys). Run <code>AUGUSTUS</code> with the prompt below to see what species models are available for prediction.</p> <p>code</p> <pre><code>augustus --species=help\n</code></pre> Output <pre><code>usage:\naugustus [parameters] --species=SPECIES queryfilename\n\nwhere SPECIES is one of the following identifiers\n\nidentifier                               | species\n-----------------------------------------|----------------------\npea_aphid                                | Acyrthosiphon pisum\naedes                                    | Aedes aegypti\namphimedon                               | Amphimedon queenslandica\nancylostoma_ceylanicum                   | Ancylostoma ceylanicum\nadorsata                                 | Apis dorsata\nhoneybee1                                | Apis mellifera\narabidopsis                              | Arabidopsis thaliana\n...\n(maize5)                                 | Zea mays\n</code></pre> <p>As you will see, there is no good model for H. halys, or even a closely related species from the Pentatomidae. </p> <p>We will use two different models for an initial round of prediction on the Halyomorpha halys sequences - one insect and one bacterial species. We will first run <code>AUGUSTUS</code> using the Apis mellifera* (honey bee) model.</p> <p>code</p> <pre><code>augustus --genemodel=partial --protein=on --codingseq=on \\\n    --species=honeybee1 \\\n    input/NW_020110202.fna \\\n    &gt; outputs/NW_020110202.aug_hb1.gff\n</code></pre> <p>This will take about 15 minutes to run, so while it is running set up the following exercise as well:</p> <p>Exercise</p> <p>Select a second model organism, preferably a non-insect model, and begin a second round of prediction using <code>AUGUSTUS</code>.</p> Solution <p>code</p> <pre><code>augustus --genemodel=partial --protein=on --codingseq=on \\\n    --species=E_coli_K12 \\\n    input/NW_020110202.fna \\\n    &gt; outputs/NW_020110202.aug_ecoli.gff\n</code></pre>"},{"location":"level2/43_annotation_augustus/#extracting-sequences-from-prediction-files","title":"Extracting sequences from prediction files","text":"<p>Once your jobs have finished (about 15 minutes per attempt), we need to run a helper script that comes with <code>AUGUSTUS</code> to extract the gene and coding sequence predicitons from the output file.</p> <p>code</p> <pre><code>getAnnoFasta.pl outputs/NW_020110202.aug_hb1.gff\n</code></pre> <p>This will return no output to the console, but creates two new files, with names dervied from the file name above.</p> <ol> <li><code>*.codingseq</code> - Nucleotide sequences for each predicted coding region</li> <li><code>*.aa</code> - Amino acid residue translation from each predicted coding region</li> </ol> <p>Exercise</p> <p>Use <code>grep</code> to count the number of protein coding sequences in your predicted files (both models) and the official annotation for this H. halys genome.</p> Solution <p>code</p> <pre><code>grep -c \"&gt;\" outputs/NW_020110202.aug_hb1.aa outputs/NW_020110202.aug_ecoli.aa outputs/NW_020110202.canonical.faa \n</code></pre> Output <pre><code>outputs/NW_020110202.aug_hb1.aa:170\noutputs/NW_020110202.aug_ecoli.aa:17\noutputs/NW_020110202.canonical.faa:110\n</code></pre> <p>Similar to when running predictions with <code>prodigal</code>, you can see that neither of the models identified the correct number of protein coding sequences. This is because neither of these gene models are accurate for the organism we are trying to characterise. However, the more closely related model was much closer in its prediction that the non-insect model.</p>"},{"location":"level2/43_annotation_augustus/#creating-a-custom-species-profile-for-gene-prediciton-optional","title":"Creating a custom species profile for gene prediciton (optional)","text":"<p>Creating a new model is a slow process so we will not be running through it today, but if this is something you need to do in your own work then use the steps below to get started.</p> <p>There are a few steps we need to perform in advance of the new model training. The first is to do with file permissions - the location of the prediction databases that <code>AUGUSTUS</code> uses for gene prediction are not writable to us, so we cannot add new data into them. We must create our own copy of the configuration information and point <code>AUGUSTUS</code> towards this new location in order to create new models</p> <p>code</p> <pre><code>cp -r ${AUGUSTUS_CONFIG_PATH} /your/location/to/store/data/\n\n# Update the `AUGUSTUS_CONFIG_PATH` variable to point to the new location\nAUGUSTUS_CONFIG_PATH=\"/your/location/to/store/data\"\n</code></pre> <p>Now we just need to obtain a reference genome to train against. For H. halys, this can found on the NCBI website and downloaded from the command line:</p> <p>code</p> <pre><code>wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/696/795/GCA_000696795.3_Hhal_1.1/GCA_000696795.3_Hhal_1.1_genomic.gbff.gz\ngunzip GCA_000696795.3_Hhal_1.1_genomic.gbff.gz\n</code></pre> <p>Once these steps are completed, training is a single command:</p> <p>code</p> <pre><code>autoAugTrain.pl --trainingset=GCA_000696795.3_Hhal_1.1_genomic.gbff --species=hhalys\n</code></pre> <p>We could then perform prediction using the new model as usual. Note that if this was performed in a new session, we would need to set the <code>AUGUSTUS_CONFIG_PATH</code> variable again.</p> <p>code</p> <pre><code>augustus --AUGUSTUS_CONFIG_PATH=/your/location/to/store/data --species=hhalys --genemodel=partial ...\n</code></pre>"},{"location":"level2/44_annotation_protein/","title":"4.4 - Protein annotation with BLAST-like methods","text":""},{"location":"level2/44_annotation_protein/#44-protein-annotation-with-blast-like-methods","title":"4.4 - Protein annotation with <code>BLAST</code>-like methods","text":"<p>time</p> <ul> <li>Teaching: 20 minutes</li> <li>Teaching: 30 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/44_annotation_protein/#objectives","title":"Objectives","text":"<ul> <li>Know how to run a protein sequence classification using <code>BLASTp</code></li> <li>Know how to run a protein sequence classification using <code>diamond</code></li> <li>Understand the differences between these tools and when one might be more appropriate than the other.</li> </ul>"},{"location":"level2/44_annotation_protein/#refresher-on-the-blast-algorithm","title":"Refresher on the <code>BLAST</code> algorithm","text":"<p>We have covered the major concepts behind the <code>BLAST</code> approach to sequence classification in level 1 training (materials here) so will not be covering this in detail today.</p> <p>However, as a general refresher, remember that the <code>BLAST</code> algorithm approaches sequence classification by comparing small sections of a query sequence to a reference database, and extending the matches where they are found.</p> Review of <code>BLAST</code> steps <p></p> <p>Where matches are found, BLAST then extends the ends of the seed one position at a time and assesses how well the seed continues to match the targets.  </p> <p></p> <p>Matches and mismatches are recorded and the seed extenstion continues.    </p> <p> </p> <p>BLAST is also able to introduce insertions to preserve a match between query and target sequences.  </p> <p> </p> <p>In the level 1 training we applied this to nucleotide sequence data, and in practice there is very little difference <code>BLAST</code>-ing against nucleotide or protein sequences.</p> What's the difference? <p>Behind the scenes, <code>BLAST</code> uses different seed sizes for its initial matching, as protein sequences are typically shorter than nucleotide sequences and are more information-rich.</p> <p>There is also a different weighting system applied when evaluating the match between query and target positions to account for whether or not the amino acid residues being compared are identical, or if their chemistry is similar. Similar to the case with <code>BLASTn</code> the scoring system is based on real-world observations of amino acid substitution rates, but is more complex to allow for the differing charge, size, and polarity of the amino acids.</p>"},{"location":"level2/44_annotation_protein/#annotating-proteins-using-blastp","title":"Annotating proteins using <code>BLASTp</code>","text":"<p>As already mentioned, performing <code>BLAST</code> for protein sequences is very similar to the case for nucleotide sequences. The two differences are that we need to substitute the <code>blastn</code> exectuable with the <code>blastp</code> version, and point out command towards a reference database of protein sequences. We still load the same modules are for a nucleotide <code>BLAST</code>, there are just a few parts of the <code>slurm</code> script which need to change from the last time we ran this job.</p> <p>Using BLAST databases</p> <p>If you recall the level 1 traning, you will remember that NeSI periodically download and store NCBI <code>BLAST</code> databases in a common location, accessed through the <code>BLASTDB/YYYY-MM</code> module series.</p> <p>Protein <code>BLAST</code> searches are significantly slower than nucleotide searches, so we will not be using this resource today as the queue and run times for the jobs we would need to run are not practical for the workshop today. We will instead use a local version of the UniProt <code>SwissProt</code> database which is much smaller.</p> <p>Navigate to the <code>/nesi/project/nesi03181/phel/&lt;username&gt;/level2/annotation_protein/</code> directory and prepare the following <code>slurm</code> script:</p> <p>level2_blast.sl</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi03181\n#SBATCH --job-name      level2_blast\n#SBATCH --time          00:20:00\n#SBATCH --cpus-per-task 10\n#SBATCH --mem           20G\n#SBATCH --error         level2_blast.%j.err\n#SBATCH --output        level2_blast.%j.out\n#SBATCH --mail-type     END\n#SBATCH --mail-user     YOUR_EMAIL\n\nmodule purge\nmodule load BLAST/2.13.0-GCC-11.3.0\n\ncd /nesi/project/nesi03181/phel/&lt;username&gt;/level2/annotation_protein/\n\nblastp -num_threads ${SLURM_CPUS_PER_TASK} -max_target_seqs 10 -evalue 1e-3 -outfmt 6 \\\n    -db /nesi/project/nesi03181/phel/databases/swissprot_blastp/uniprot_sprot \\\n    -query input/input_seqs.faa -out outputs/blastp.txt\n</code></pre> <p>Submit this job to <code>slurm</code>:</p> <p>code</p> <pre><code>sbatch level2_blast.sl\n</code></pre> Output <pre><code>Submitted batch job XXXXXXXX\n</code></pre>"},{"location":"level2/44_annotation_protein/#accelerating-protein-searches-using-diamond","title":"Accelerating protein searches using <code>diamond</code>","text":"<p>Given the time required to produce results with the <code>BLASTp</code> approach, a lot of work has been conducted to speed up the results of protein alignment searches. A very useful alternative to <code>BLAST</code> when working with protein data, or when trying to perform translated (nucleotide queries, protein targets) searches is <code>diamond</code> (Buchfink et al., 2021)</p> <p>Differences in <code>diamond</code> versions</p> <p>In previous releases, <code>diamond</code> gave faster results at the cost of sensitivity but the more recent release of the tool purports comparable sensitivity with <code>BLASTp</code> at a greatly reduced run time. See Figure 1 in the manuscript above for comparisons of sensitivity and speed between more recent releases of <code>diamond</code> and <code>BLASTp</code>.</p> Producing a <code>diamond</code> database <p>Unfortunately, <code>diamond</code> requires us to create our own classification database - there is no NCBI-supported release. However, <code>diamond</code> has a built-in method to modify an existing <code>BLAST</code> database to be compatible with <code>diamond</code>.</p> <p>You will probably not ever need to perform this operation yourself, but the database we are using today was produced using the following commands:</p> <p>code</p> <pre><code>module purge\nmodule load BLASTDB/2023-10 DIAMOND/2.1.6-GCC-11.3.0\n\nmkdir /nesi/project/nesi03181/phel/databases/diamond_nr/\ncd /nesi/project/nesi03181/phel/databases/diamond_nr/\n\ncp ${BLASTDB}/nr* diamond_nr/\ndiamond prepdb -d diamond_nr/nr\n</code></pre> <p>With our database in place, the command for running <code>diamond</code> is very similar (by design) to <code>BLASTp</code>:</p> <p>level2_blast.sl</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi03181\n#SBATCH --job-name      level2_diamond\n#SBATCH --time          00:10:00\n#SBATCH --cpus-per-task 16\n#SBATCH --mem           20G\n#SBATCH --error         level2_diamond.%j.err\n#SBATCH --output        level2_diamond.%j.out\n#SBATCH --mail-type     END\n#SBATCH --mail-user     YOUR_EMAIL\n\nmodule purge\nmodule load DIAMOND/2.1.6-GCC-11.3.0\n\ncd /nesi/project/nesi03181/phel/&lt;username&gt;/level2/annotation_protein/\n\ndiamond blastp --threads ${SLURM_CPUS_PER_TASK} --ultra-sensitive --max-target-seqs 10 --evalue 1e-3 --outfmt 6 \\\n    --db /nesi/project/nesi03181/phel/databases/swissprot_dmnd/uniprot_sprot.dmnd \\\n    --query input/input_seqs.faa --out outputs/diamond.txt\n</code></pre> <p>Submit this job to <code>slurm</code>:</p> <p>code</p> <pre><code>sbatch level2_blast.sl\n</code></pre> Output <pre><code>Submitted batch job XXXXXXXX\n</code></pre>"},{"location":"level2/44_annotation_protein/#comparing-the-outputs","title":"Comparing the outputs","text":"<p>In both of the jbos above, we specified the tab-delimited <code>BLAST6</code> output format, which is a table of the form;</p> Column header Meaning qseqid Sequence ID of the query sequence (input file) sseqid Sequence ID of the target sequence (reference database) pident Percentage of identical positions between query and target length Alignment length (sequence overlap) of the common region between query and target mismatch Number of mismatches between query and target gapopen Number of gap openings in the alignment qstart Position in the query sequence where alignment begins qend Position in the query sequence where alignment ends sstart Position in the target sequence where alignment begins send Position in the target sequence where alignment ends evalue The E-value for the query/target match, as described above bitscore The bit score for the query/target match, as described above <p>As there are around 3,000 proteins in our input file which were classified, we are just going to focus on a single sequence and how it's annotation differs between <code>BLASTp</code> and <code>diamond</code>.</p> <p>Exercise</p> <p>Use your knowledge the command line to print all results for the query sequence <code>46ee54c6-57d2-2d22-f454-9a5737db31e7_1</code> in both output files. You can either keep the results on the command line, or redirect them into files for analysis.</p> Solution <pre><code>grep \"46ee54c6-57d2-2d22-f454-9a5737db31e7_1\" outputs/blastp.txt \ngrep \"46ee54c6-57d2-2d22-f454-9a5737db31e7_1\" outputs/diamond.txt\n</code></pre> outputs/blastp.txt qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P17779|RDRP_PVXX3 100.000 719 0 0 1 719 240 958 0.0 1499 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P09395|RDRP_PVX 97.218 719 20 0 1 719 240 958 0.0 1462 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|Q07630|RDRP_PVXHB 83.449 719 119 0 1 719 240 958 0.0 1256 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P22591|RDRP_PVXCP 82.476 719 126 0 1 719 240 958 0.0 1237 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P20951|RDRP_PMV 48.563 348 174 4 370 717 699 1041 2.79e-97 332 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P20951|RDRP_PMV 41.489 188 110 0 1 188 230 417 7.28e-39 159 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P09498|RDRP_WCMVM 50.140 357 168 6 364 719 446 793 3.73e-96 327 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P09498|RDRP_WCMVM 40.556 180 104 2 14 193 235 411 4.56e-30 131 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P15402|RDRP_WCMVO 49.860 357 169 6 364 719 446 793 2.67e-94 322 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P15402|RDRP_WCMVO 38.776 196 116 3 15 210 236 427 1.67e-29 129 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|Q07518|RDRP_P1AMV 47.238 362 180 8 363 719 531 886 1.71e-88 306 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|Q07518|RDRP_P1AMV 41.765 170 96 3 14 182 236 403 7.82e-31 134 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|Q918W3|RDRP_ICRSV 48.986 345 171 5 375 719 821 1160 1.13e-87 305 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|Q918W3|RDRP_ICRSV 26.070 257 175 4 14 268 242 485 1.60e-20 100 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P28897|RDRP_SMYEA 48.387 341 170 6 380 719 493 828 1.08e-85 298 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P28897|RDRP_SMYEA 32.524 206 129 3 15 211 254 458 1.05e-24 114 outputs/diamond.txt qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P17779|RDRP_PVXX3 100 719 0 0 1 719 240 958 0.0 1425 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P09395|RDRP_PVX 97.2 719 20 0 1 719 240 958 0.0 1390 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|Q07630|RDRP_PVXHB 83.4 719 119 0 1 719 240 958 0.0 1197 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P22591|RDRP_PVXCP 82.5 719 126 0 1 719 240 958 0.0 1179 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P20951|RDRP_PMV 34.8 819 425 15 1 717 230 1041 8.77e-125 409 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P09498|RDRP_WCMVM 37.3 707 294 12 14 719 235 793 5.69e-117 384 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P15402|RDRP_WCMVO 36.7 706 298 12 15 719 236 793 7.69e-115 378 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|Q07518|RDRP_P1AMV 38.5 716 365 23 14 719 236 886 4.72e-114 377 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P28897|RDRP_SMYEA 34.2 710 323 13 15 717 254 826 5.27e-103 346 46ee54c6-57d2-2d22-f454-9a5737db31e7_1 sp|P15095|RDRP_NMV 32.2 860 419 23 15 719 243 1093 5.58e-100 340 <p>Comparing the outputs, you can see that <code>diamond</code> produced slightly fewer hits than <code>BLASTp</code>, but the top four results for both methods are the same:</p> Target <code>BLASTp</code>Identity E-value Bitscore <code>diamond</code>Identity E-value Bitscore sp|P17779|RDRP_PVXX3 100.000 0 1,499 100.0 0 1,425 sp|P09395|RDRP_PVX 97.218 0 1,462 97.2 0 1,390 sp|Q07630|RDRP_PVXHB 83.449 0 1,256 83.4 0 1,197 sp|P22591|RDRP_PVXCP 82.476 0 1,237 82.5 0 1,179 <p>We can check the complete annotation of this protein to determine its function and organism from which the reference sequence originated.</p> <p>Exercise</p> <p>Search the top result(s) through the UniProt search tool and report the most likely function of this sequence, and the organism that it was obtained from.</p> Solution Sequence Function Organism sp|P17779|RDRP_PVXX3 RNA-directed RNA polymerase Potato virus X (strain X3) sp|P09395|RDRP_PVX RNA-directed RNA polymerase Potato virus X sp|Q07630|RDRP_PVXHB RNA-directed RNA polymerase Potato virus X sp|P22591|RDRP_PVXCP RNA-directed RNA polymerase Potato virus X (strain CP)"},{"location":"level2/45_annotation_kraken2/","title":"4.5 - Classification of sequences with k-mer composition profiles","text":""},{"location":"level2/45_annotation_kraken2/#45-classification-of-sequences-with-k-mer-composition-profiles","title":"4.5 - Classification of sequences with k-mer composition profiles","text":"<p>time</p> <ul> <li>Teaching: 15 minutes</li> <li>Teaching: 15 minutes</li> </ul> <p>Objectives and Key points</p>"},{"location":"level2/45_annotation_kraken2/#objectives","title":"Objectives","text":"<ul> <li>Understand when we may need to use something other than <code>BLAST</code> for getting quick classification results.</li> <li>Know how to run <code>kraken2</code> using a standard database, and interpret the output files.</li> </ul>"},{"location":"level2/45_annotation_kraken2/#keypoints","title":"Keypoints","text":"<ul> <li>k-mer based classification tools provide a rapid means of assigning taxonomy to sequence data.</li> <li>The quality of the classification is tool and database dependent, results are also harder to interpret than a BLAST output</li> <li>For example, there are no identity/coverage statistics from which judgement calls can be made.</li> </ul>"},{"location":"level2/45_annotation_kraken2/#why-not-blast","title":"Why not <code>BLAST</code>?","text":"<p>As we saw in the level 1 training, running <code>BLASTn</code> to classify sequences is a good way to get annotation information about a sequence, and from this we can infer taxonomic origin of the sequence. However, obtaining classification information is not the primary purpose of <code>BLASTn</code> so we need to perform some manual inspection of the output to make a call on the classification of each sequence.</p> <p>When we do not care about functional information, and only want classification of sequences, there are a suite of tools designed specifically with this purpose in mind. Rather than perform pairwise sequence alignment between query and target sequences, these tools process their reference database into a compressed form containing sequence fingerprints.</p> <p>Query sequences are fingerprinted and the results compared to the database providing the most likely, lowest common ancestor for each query sequence.</p> <p>Options for k-mer classification</p> <p>How this fingerprinting is done is tool specific, and goes outside the realm of biology. There are a number of tools developed for this kind of work, including;</p> <ol> <li>kraken2 (Wood et al., 2019)</li> <li>CLARK (Ounit et al., 2015)</li> <li>MetaCache (M\u00fcller et al., 2017)</li> <li>ganon (Piro et al., 2020)</li> </ol> <p>We will be using <code>kraken2</code> today, as it is a tool which is used often within our operations.</p>"},{"location":"level2/45_annotation_kraken2/#getting-started-with-kraken2","title":"Getting started with <code>kraken2</code>","text":"<p>Navigate to <code>/nesi/project/nesi03181/phel/USERNAME/level2/annotation_kraken2/</code> and prepare to run <code>kraken2</code>.</p> <p>Exercise</p> <p>Find and load the most current version of <code>kraken2</code>. Once loaded, run with the help paramter (<code>-h</code>) to confirm that the module has successfully loaded.</p> Solution <p>code</p> <pre><code>module spider kraken2\n\nmodule purge\nmodule load Kraken2/2.1.3-GCC-11.3.0\n\nkraken2 -h\n</code></pre> Output <pre><code>Usage: kraken2 [options] &lt;filename(s)&gt;\n\nOptions:\n--db NAME               Name for Kraken 2 DB\n                        (default: \"/opt/nesi/db/Kraken2/standard-2018-09\")\n--threads NUM           Number of threads (default: 1)\n--quick                 Quick operation (use first hit or hits)\n--unclassified-out FILENAME\n                        Print unclassified sequences to filename\n--classified-out FILENAME\n                        Print classified sequences to filename\n--output FILENAME       Print output to filename (default: stdout); \"-\" will\n                        suppress normal output\n--confidence FLOAT      Confidence score threshold (default: 0.0); must be\n                        in [0, 1].\n--minimum-base-quality NUM\n                        Minimum base quality used in classification (def: 0,\n                        only effective with FASTQ input).\n--report FILENAME       Print a report with aggregrate counts/clade to file\n--use-mpa-style         With --report, format report output like Kraken 1's\n                        kraken-mpa-report\n--report-zero-counts    With --report, report counts for ALL taxa, even if\n                        counts are zero\n--report-minimizer-data With --report, report minimizer and distinct minimizer\n                        count information in addition to normal Kraken report\n--memory-mapping        Avoids loading database into RAM\n--paired                The filenames provided have paired-end reads\n--use-names             Print scientific names instead of just taxids\n--gzip-compressed       Input files are compressed with gzip\n--bzip2-compressed      Input files are compressed with bzip2\n--minimum-hit-groups NUM\n                        Minimum number of hit groups (overlapping k-mers\n                        sharing the same minimizer) needed to make a call\n                        (default: 2)\n--help                  Print this message\n--version               Print version information\n\nIf none of the *-compressed flags are specified, and the filename provided\nis a regular file, automatic format detection is attempted.\n</code></pre> <p>One of the nice perks of working with <code>kraken2</code> is that the maintainers of the tool provide pre-computed, up-to-date databases for classification. These are released in many different variants - some for use of HPCs and some for local machines, and databases which cover a range of target organisms.</p> <p>We use <code>PlusPFP</code> database, which contains the following lineages:</p> <ol> <li>Archaea</li> <li>Bacteria</li> <li>Fungi</li> <li>Human</li> <li>Plant</li> <li>Protozoa</li> <li>Virus</li> <li>Plasmid sequences</li> <li>Sequencing and transformation vectors (UniVec)</li> </ol> <p>For training, we are using a reduced version of this database (<code>PlusPFP-16</code>) so that our jobs do not need to queue as long, but the same lineages are represented in this smaller database.</p> <p>This makes it a comprehensive database for most purposes, but if you are tring to annotate insect sequences this is not the right database for you. For training purposes, a copy of this database can be found at <code>/nesi/project/nesi03181/phel/databases/</code> and we also have a copy in our diagnostic workspace.</p> <p>Make sure you have the right database for your work</p> <p>It is critical to know what is in your classification database, as a failure to include the correct reference material in the database will result in sequences being either misclassified, or not classified at all.</p>"},{"location":"level2/45_annotation_kraken2/#performing-basic-classification-with-kraken2","title":"Performing basic classification with <code>kraken2</code>","text":"<p>To begin, we are going to write a <code>slurm</code> script to run <code>kraken2</code> with a fairly minimal set in input parameters just to see the tool in action. Although <code>kraken2</code> is very fast to run, it requires a lot of memory (RAM) to run so we cannot run it from the command line in most cases.</p> <p>Complete the following script, then submit your <code>slurm</code> job.</p> <p>kraken2_basic.sl</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi03181\n#SBATCH --job-name      kraken2_basic\n#SBATCH --time          00:01:00\n#SBATCH --cpus-per-task 16\n#SBATCH --mem           20G\n#SBATCH --error         kraken2_basic.%j.err\n#SBATCH --output        kraken2_basic.%j.out\n\nmodule purge\nmodule load Kraken2/2.1.3-GCC-11.3.0\n\ncd /nesi/project/nesi03181/phel/USERNAME/level2/annotation_kraken2/\n\nPFP_DB=\"/nesi/project/nesi03181/phel/databases/k2_pluspfp_16gb_20231009\"\nkraken2 --db ${PFP_DB} --threads ${SLURM_CPUS_PER_TASK} --use-names \\\n    --output outputs/input_seqs.out \\\n    input/input_seqs.fna\n</code></pre> <p>No email block in the <code>slurm</code> script</p> <p>This job will take less than 30 seconds to complete, so we're not going to bother with the email notifications for job status. The only reason we need to push this job through <code>slurm</code> at all is because of the memory required to run the tool.</p> <p>code</p> <pre><code>sbatch kraken2_basic.sl\n</code></pre> Output <pre><code>Submitted batch job XXXXXXXX\n</code></pre> <p>This will not take long to run.</p>"},{"location":"level2/45_annotation_kraken2/#the-kraken2-output-format","title":"The <code>kraken2</code> output format","text":"<p>Once the job is complete, use <code>less</code> or <code>head</code> to take a look at the output file. The output file is a tab-delimited table with the following columns:</p> <p>kraken2_basic.sl</p> Column Content 1 A single letter (C or U) designating whether the sequence was classified or not 2 Sequence name 3 The NCBI taxonomy ID applied to the sequence (0 if unclassified)Because we used the <code>--use-names</code> flag, this takes the form <code>TAXONOMY NAME (TAXONOMY ID)</code>By default this is just the numeric value 4 Sequence lengthFor paired-end data, this will be in the form <code>FORWARD|REVERSE</code> 5 A list of how many k-mers mapped to each taxonomy IDResults are reported as colon-delimited pairings of <code>TAXID:NUMBER</code> <p>It is also possible to generate a report file that summarises the number of sequences classified to each taxonomy/rank, using the modified command:</p> <p>code</p> <pre><code>kraken2 --db ${PFP_DB} --threads ${SLURM_CPUS_PER_TASK} --use-names \\\n    --output outputs/input_seqs.out \\\n    --report outputs/input_seqs.report.txt \\\n    input/input_seqs.fna\n</code></pre> <p>Depending on your situation, this may be more useful.</p> <p>Classification output versus report</p> <p>When would you need the per-sequence output</p> <p>If you are screening a sequence set for the specific reads obtained from a particular organism, or are trying to remove contamination from a sample, having the per-sequence results are invaluable.</p> <p>The content of this file can be easily parsed with the <code>cut</code> and <code>grep</code> tools to extract lists of sequences match or failing to match different search criteria.</p> <p>When would you need the high-level report</p> <p>If you trust that your data is uncontaminated, you might be trying ot achive one of the following:</p> <ol> <li>Detecting the presence of a particular organism in a mixed population (i.e. eDNA surveillance).</li> <li>Confirming the identity of a pure culture.</li> <li>In this case you might be comfortable taking a majority-rules approach to your data - if most of the sequences are classified as a single species, this is likely the identity of your culture.</li> </ol>"},{"location":"level2/45_annotation_kraken2/#refining-the-kraken2-classification","title":"Refining the <code>kraken2</code> classification","text":"<p>By default <code>kraken2</code> does not apply any filtering to the classification data. Reads are classified based on the most commonly reported taxonomy for a sequence, with no regard for how good the evidence for this classification can be. There is a built-in parameter to increase the stringency of classification. The filter is applied during classification and cannot be run after the fact.</p> <p>This means that if you want to change your filtering criteria you must perform classification again. This differs from tools like <code>BLAST</code>, where a more stringent filter can be applied after running classification.</p> <p>For example, if we run a <code>BLAST</code> search looking to report all hits with greater than 30% identity, we can later filter the table to keep results with greater than 50% identity by filtering the appropriate column.</p> <p>Limitations with the <code>kraken2</code> filtering</p> <p>It is also difficult to interpret the numeric values of filters.</p> <p>According to the <code>kraken2</code> manual, the confidence scoring does not have a simple interpretation.</p> <p>For example, with a <code>BLAST</code> identitiy of 90%, we can infer that 90% of positions are identical between query and target match, or a bootstrap support of 90% would mean that 90% of the resamplings agree with the full result.</p> <p>However, there is no such interpretation of what a <code>0.9</code> confidence value in <code>kraken2</code> means, other than it is more strict that a value of <code>0.5</code>.</p> <p>We generally don't need to apply much of a cutoff to <code>kraken2</code> - the tool is already good at identifying sequences which can't be reliably classified.</p> <p>kraken2_refined.sl</p> <pre><code>#!/bin/bash -e\n#SBATCH --account       nesi03181\n#SBATCH --job-name      kraken2_refined\n#SBATCH --time          00:01:00\n#SBATCH --cpus-per-task 16\n#SBATCH --mem           20G\n#SBATCH --error         kraken2_refined.%j.err\n#SBATCH --output        kraken2_refined.%j.out\n\nmodule purge\nmodule load Kraken2/2.1.3-GCC-11.3.0\n\ncd /nesi/project/nesi03181/phel/USERNAME/level2/annotation_kraken2/\n\nPFP_DB=\"/nesi/project/nesi03181/phel/databases/k2_pluspfp_16gb_20231009\"\nkraken2 --db ${PFP_DB} --threads ${SLURM_CPUS_PER_TASK} --confidence 0.1 --use-names \\\n    --output outputs/input_seqs.conf_0.1.out \\\n    input/input_seqs.fna\n</code></pre> <p>code</p> <pre><code>sbatch kraken2_refined.sl\n</code></pre> Output <pre><code>Submitted batch job XXXXXXXX\n</code></pre>"},{"location":"level2/45_annotation_kraken2/#comparing-the-outputs","title":"Comparing the outputs","text":"<p>Without going into too much detail, we can briefly compare the contents of both output files. For starters, we can use some of the basic <code>bash</code> command line tools to work out how many sequences were in our input and output files.</p> <p>code</p> <pre><code>grep -c \"&gt;\" input/input_seqs.fna \n</code></pre> Output <pre><code>5614\n</code></pre> <p>code</p> <pre><code>wc -l outputs/input_seqs.* \n</code></pre> Output <pre><code> 5614 outputs/input_seqs.conf_0.1.out\n 5614 outputs/input_seqs.out\n11228 total\n</code></pre> <p>The number of results in each output file is the same, and also the same as the number of sequences in the input file. This is different to <code>BLAST</code>, where an unclassified or unannotated sequence is not reported in the output.</p> <p>A quick way to look for differences in the classification rate of the unfiltered and filtered <code>kraken2</code> output is to apply the following commands:</p> <p>code</p> <pre><code>cut -f ${n} outputs/input_seqs.out | sort | uniq -c\n</code></pre> <p>If we point this towards the correct column, we can count how many sequences were successfully classified or not.</p> <p>Exercise</p> <p>Look through these tutorial notes, or the <code>kraken2</code> documentation, to find the appropriate column index for the command above. Once you have determined the correct value for <code>${n}</code>, complete the command above and run it for both <code>kraken2</code> outputs.</p> <p>How many sequences were classified for each run?</p> Solution <p>The first column (C or U values) is the easiest one to work with, although you could also get the same result from column 3.</p> <pre><code>n=1\ncut -f ${n} outputs/input_seqs.out | sort | uniq -c\ncut -f ${n} outputs/input_seqs.conf_0.1.out | sort | uniq -c\n</code></pre> Output <pre><code># outputs/input_seqs.out\n5205 C\n 409 U\n\n# outputs/input_seqs.conf_0.1.out\n  10 C\n5604 U\n</code></pre> <p>As you can see, even a small degree of filtering has a drastic impact on the classification rate. If you are applying a filter value, using a low threshold is recommended to avoid discarding too much usable data.</p>"},{"location":"level2/45_annotation_kraken2/#how-reliable-are-these-results","title":"How reliable are these results?","text":"<p>The input sequences in this exercise are simulated Oxford Nanopore reads from the following organisms;</p> <ol> <li>Tomato (Solanum lycopersicum) (5,447 sequences)</li> <li>Xyllela fastidiosa (166 sequences)</li> <li>Potato virus X (1 sequence)</li> </ol> <p>If you skim through the results of both output files, you should note the following:</p> outputs/input_seqs.out Lineage Observations Solanum lycopersicum 4,976 unclassified 409 Xylella fastidiosa 162 Homo sapiens 37 Solanum subgen. Lycopersicon 6 Solanum pennellii 2 Triticum aestivum 2 Asparagus officinalis 1 Flavobacterium sp. 1 Hordeum vulgare 1 Lolium rigidum 1 Oryza brachyantha 1 Pandoraea thiooxydans 1 Potato virus X 1 Setaria viridis 1 Streptomyces cinnabarinus 1 outputs/input_seqs.conf_0.1.out Lineage Observations unclassified 5,604 Homo sapiens 6 Solanum lycopersicum 2 Potato virus X 1 <p>What should be apparent here is that while filtering did remove a lot of the noise from our sample (we lost species we know are not truly present) we also lost a signal that was in the sample.</p> <p>Recommendation for <code>kraken2</code></p> <p>Don't let this put you off using <code>kraken2</code> - it is an excellent tool for rapidly sifting through HTS data for quick summaries. However, results should only be considered indicative in nature, not definitive.</p> <p>Use <code>kraken2</code> to make a quick check for names of interest (regulated organisms, host, contamination) but follow up with more thorough investigation.</p>"},{"location":"proficiency/2023_proficiency/","title":"2023 Level 2 proficiency testing (version 1)","text":""},{"location":"proficiency/2023_proficiency/#2023-level-2-proficiency-testing-version-1","title":"2023 Level 2 proficiency testing (version 1)","text":"<p>time</p> <ul> <li>Exercises: 150 minutes</li> </ul> <p>Key information</p> <ul> <li>The aim is to complete each of the exercises before the end of the session.</li> <li>This testing is open book - you may use any materials from the level 1 or level 2 training and anything online.</li> <li>This testing is individual - please do not discuss your solutions with other trainees.</li> <li>The tutors are here to help with significant technical issues such as loss of connection. We are not here to help you identify tools or how to solve specific questions.</li> </ul> <p>All test folders are located in the following location:</p> <p>code</p> <pre><code>/nesi/project/nesi03181/phel/proficiency/USERNAME/\n</code></pre> <p>Output files</p> <p>For each exercise, you are free to name your output folders/files whatever you wish.</p> <p>Please ensure that your results are in the appropriate folder for each exercise, however.</p> <p>Reporting findings</p> <p>Whenever an exercise asks your to in some way interpret an output file and report your results, just create a small text file to store your notes.</p> <p>Name the file according to the exercise that asked for the information.</p>"},{"location":"proficiency/2023_proficiency/#exercise-1","title":"Exercise 1","text":"<p>Navigate to your instance of the <code>01_illumina_assembly/</code> folder. You have been provided with the following files:</p> <p>code</p> <pre><code>reads/illumina_R1.fq.gz\nreads/illumina_R2.fq.gz\nreference/reference.fna\nresults/provided_assembly.fna\n</code></pre> <p>Part 1</p> <p>You are to produce a <code>slurm</code> script that uses an approriate assembly tool to assemble the Illumina MiSeq sequences in the <code>reads/</code> folder into a draft genome.</p> <p>Your script will only need about 10 minutes to complete with 16 CPUs.</p> <p>Part 2</p> <p>Use an appropriate tool to determine assembly statistics for your assembly file and use the file <code>reference/reference.fna</code> as your reference genome.</p> <p>If your job does not complete in a reasonable amount of time, you can use the provided <code>results/provided_assembly.fna</code> file instead of your own output.</p> <p>Report the folllowing metrics from your assembly:</p> <ol> <li>Number of contigs with length greater than or equal than 50 kbp.</li> <li>The N50 value for the assembly.</li> <li>Number of indels relative to the reference genome.</li> </ol>"},{"location":"proficiency/2023_proficiency/#exercise-2","title":"Exercise 2","text":"<p>Navigate to your instance of the <code>02_mapping/</code> folder. You have been provided with the following files:</p> <p>code</p> <pre><code>reads/nanopore.fq.gz\nreference/reference.fna\nresults/provided_mapping.sam\n</code></pre> <p>Part 1</p> <p>Use whatever tool is appropriate to map the sequences in the <code>reads/</code> folder against the reference file, creating a sam file with the mapping results.</p> <p>Part 2</p> <ol> <li>Once you have completed mapping your reads, sort and compress the mapping information from your output sam file.</li> </ol> <p>If you are unable to complete your mapping job for any reason, use the file supplied at `results/provided_mapping.sam.</p> <ol> <li> <p>Produce a depth report from the compressed mapping file.</p> </li> <li> <p>When you have produced your compressed file, filter the results to keep only the mapped reads in the mapping output.</p> </li> </ol>"},{"location":"proficiency/2023_proficiency/#exercise-3","title":"Exercise 3","text":"<p>Navigate to your instance of the <code>03_gene_calling/</code> folder. You have been provided with the following file:</p> <p>code</p> <pre><code>input_sequences.fna\n</code></pre> <p>Question</p> <p>You have been provided with a draft assembly of a prokaryotic genome. Create predictions of the protein coding regions of the assembly using whichever tool you believe is appropriate.</p>"},{"location":"proficiency/2023_proficiency/#exercise-4","title":"Exercise 4","text":"<p>Navigate to your instance of the <code>04_classification_kraken2/</code> folder. You have been provided with the following files:</p> <p>code</p> <pre><code>inputs/input_sequences.fna\nresults/provided_kraken2.txt\n</code></pre> <p>Part 1</p> <p>Write a <code>slurm</code> script to execute a <code>kraken2</code> classification of the sequences provided in <code>inputs/input_sequences.fna</code>.</p> <p>When selecting a database, use the <code>PlusPFP</code> database located at:</p> <p>code</p> <pre><code>/nesi/project/nesi03181/phel/databases/k2_pluspfp_16gb_20231009/\n</code></pre> <p>Check your notes carefully to make sure you understand how to direct <code>kraken2</code> towards this database.</p> <p>Part 2</p> <p>You have been provided with an output file of the above classification job - <code>results/provided_kraken2.txt</code>. Use your knowledge of the command line to search through the output file and identify the most likely species to be in this sample.</p> <p>Create a text file containing your findings. You do not need to report every species hit in the <code>provided_kraken2.txt</code> file - just pick the candidates you believe are most likely and give a quick 1-sentence justification for each choice.</p>"},{"location":"proficiency/2023_proficiency/#exercise-5","title":"Exercise 5","text":"<p>Navigate to your instance of the <code>05_classification_diamond/</code> folder. You have been provided with the following file:</p> <p>code</p> <pre><code>inputs/input_sequences.faa\nresults/provided_diamond.txt\n</code></pre> <p>Part 1</p> <p>Write a <code>slurm</code> script to execute a <code>diamond</code> classification of the sequences provided in <code>inputs/input_sequences.faa</code>.</p> <p>When selecting a database, use the <code>uniprot_sprot.dmnd</code> database located at:</p> <p>code</p> <pre><code>/nesi/project/nesi03181/phel/databases/swissprot_dmnd/uniprot_sprot.dmnd\n</code></pre> <p>Part 2</p> <p>Examine the outputs of the file <code>results/provided_diamond.txt</code>, remembering the BLAST6 format for column meaning.</p> <p>Based on the top hit results, determine the most likely genus or species of the organism from which these sequences were obtained. Create a text file containing your conclusion.</p>"},{"location":"proficiency/2023_proficiency/#closing-out-the-test","title":"Closing out the test","text":"<p>When you have completed all of the exercises, and you are happy with your results, please run the following command to log the history of your command line to a file.</p> <p>code</p> <pre><code>history &gt; /nesi/project/nesi03181/phel/proficiency/USERNAME/history_log.txt\n</code></pre>"},{"location":"proficiency/2023_proficiency_alternate/","title":"2023 Level 2 proficiency testing (version 2)","text":""},{"location":"proficiency/2023_proficiency_alternate/#2023-level-2-proficiency-testing-version-2","title":"2023 Level 2 proficiency testing (version 2)","text":"<p>time</p> <ul> <li>Exercises: 150 minutes</li> </ul> <p>Key information</p> <ul> <li>The aim is to complete each of the exercises before the end of the session.</li> <li>This testing is open book - you may use any materials from the level 1 or level 2 training and anything online.</li> <li>This testing is individual - please do not discuss your solutions with other trainees.</li> <li>The tutors are here to help with significant technical issues such as loss of connection. We are not here to help you identify tools or how to solve specific questions.</li> </ul> <p>All test folders are located in the following location:</p> <p>code</p> <pre><code>/nesi/project/nesi03181/phel/proficiency/USERNAME/\n</code></pre> <p>Output files</p> <p>For each exercise, you are free to name your output folders/files whatever you wish.</p> <p>Please ensure that your results are in the appropriate folder for each exercise, however.</p> <p>Reporting findings</p> <p>Whenever an exercise asks your to in some way interpret an output file and report your results, just create a small text file to store your notes.</p> <p>Name the file according to the exercise that asked for the information.</p>"},{"location":"proficiency/2023_proficiency_alternate/#exercise-1","title":"Exercise 1","text":"<p>Navigate to your instance of the <code>01_nanopore_assembly/</code> folder. You have been provided with the following files:</p> <p>code</p> <pre><code>reads/nanopore.fq.gz\nreference/reference.fna\nresults/provided_assembly.fna\n</code></pre> <p>Part 1</p> <p>You are to produce a <code>slurm</code> script that uses an approriate assembly tool to assemble the Oxford Nanopore sequences <code>reads/nanopore.fq.gz</code> into a draft genome.</p> <p>Your script will only need about 10 minutes to complete.</p> <p>Part 2</p> <p>Use an appropriate tool to determine assembly statistics for your assembly file and use the file <code>reference/reference.fna</code> as your reference genome.</p> <p>If your job does not complete in a reasonable amount of time, you can use the provided <code>results/provided_assembly.fna</code> file instead of your own output.</p> <p>Report the folllowing metrics from your assembly:</p> <ol> <li>Number of contigs with length greater than or equal than 50 kbp.</li> <li>The N50 value for the assembly.</li> <li>Number of indels relative to the reference genome.</li> </ol>"},{"location":"proficiency/2023_proficiency_alternate/#exercise-2","title":"Exercise 2","text":"<p>Navigate to your instance of the <code>02_mapping/</code> folder. You have been provided with the following files:</p> <p>code</p> <pre><code>reads/illumina_R1.fq.gz\nreads/illumina_R2.fq.gz\nreference/reference.fna\nresults/provided_mapping.sam\n</code></pre> <p>Part 1</p> <p>Use whatever tool is appropriate to map the sequences in the <code>reads/</code> folder against the reference file, creating a sam file with the mapping results.</p> <p>Part 2</p> <ol> <li>Once you have completed mapping your reads, sort and compress the mapping information from your output sam file.</li> </ol> <p>If you are unable to complete your mapping job for any reason, use the file supplied at `results/provided_mapping.sam.</p> <ol> <li> <p>Produce a depth report from the compressed mapping file.</p> </li> <li> <p>When you have produced your compressed file, filter the results to keep only the unmapped reads in the mapping output.</p> </li> </ol>"},{"location":"proficiency/2023_proficiency_alternate/#exercise-3","title":"Exercise 3","text":"<p>Navigate to your instance of the <code>03_gene_calling/</code> folder. You have been provided with the following file:</p> <p>code</p> <pre><code>input_sequences.fna\n</code></pre> <p>Question</p> <p>You have been provided with a draft assembly of a prokaryotic genome. Create predictions of the protein coding regions of the assembly using whichever tool you believe is appropriate.</p>"},{"location":"proficiency/2023_proficiency_alternate/#exercise-4","title":"Exercise 4","text":"<p>Navigate to your instance of the <code>04_classification_kraken2/</code> folder. You have been provided with the following files:</p> <p>code</p> <pre><code>inputs/input_sequences.fna\nresults/provided_kraken2.txt\n</code></pre> <p>Part 1</p> <p>Write a <code>slurm</code> script to execute a <code>kraken2</code> classification of the sequences provided in <code>inputs/input_sequences.fna</code>.</p> <p>When selecting a database, use the <code>PlusPFP</code> database located at:</p> <p>code</p> <pre><code>/nesi/project/nesi03181/phel/databases/k2_pluspfp_16gb_20231009/\n</code></pre> <p>Check your notes carefully to make sure you understand how to direct <code>kraken2</code> towards this database.</p> <p>Part 2</p> <p>You have been provided with an output file of the above classification job - <code>results/provided_kraken2.txt</code>. Use your knowledge of the command line to search through the output file and identify the most likely species to be in this sample.</p> <p>Create a text file containing your findings. You do not need to report every species hit in the <code>provided_kraken2.txt</code> file - just pick the candidates you believe are most likely and give a quick 1-sentence justification for each choice.</p>"},{"location":"proficiency/2023_proficiency_alternate/#exercise-5","title":"Exercise 5","text":"<p>Navigate to your instance of the <code>05_classification_diamond/</code> folder. You have been provided with the following file:</p> <p>code</p> <pre><code>inputs/input_sequences.faa\nresults/provided_diamond.txt\n</code></pre> <p>Part 1</p> <p>Write a <code>slurm</code> script to execute a <code>diamond</code> classification of the sequences provided in <code>inputs/input_sequences.faa</code>.</p> <p>When selecting a database, use the <code>uniprot_sprot.dmnd</code> database located at:</p> <p>code</p> <pre><code>/nesi/project/nesi03181/phel/databases/swissprot_dmnd/uniprot_sprot.dmnd\n</code></pre> <p>Part 2</p> <p>Examine the outputs of the file <code>results/provided_diamond.txt</code>, remembering the BLAST6 format for column meaning.</p> <p>Based on the top hit results, determine the most likely genus or species of the organism from which these sequences were obtained. Create a text file containing your conclusion.</p>"},{"location":"proficiency/2023_proficiency_alternate/#closing-out-the-test","title":"Closing out the test","text":"<p>When you have completed all of the exercises, and you are happy with your results, please run the following command to log the history of your command line to a file.</p> <p>code</p> <pre><code>history &gt; /nesi/project/nesi03181/phel/proficiency/USERNAME/history_log.txt\n</code></pre>"},{"location":"supplementary/common_terms/","title":"Common terms","text":""},{"location":"supplementary/common_terms/#glossary","title":"Glossary","text":""},{"location":"supplementary/common_terms/#absolute-path","title":"Absolute path","text":"<ul> <li>A path that refers to a particular location in a file system.</li> <li>Absolute paths are usually written with respect to the file system's root directory, and begin with <code>/</code> (<code>\\\\</code> on Microsoft Windows).</li> <li>See also: relative path</li> </ul>"},{"location":"supplementary/common_terms/#argument","title":"Argument","text":"<ul> <li>A value given to a function or program when it runs.</li> <li>The term is often used interchangeably (and inconsistently) with parameter.</li> </ul>"},{"location":"supplementary/common_terms/#command-shell","title":"Command shell","text":"<ul> <li>See shell</li> </ul>"},{"location":"supplementary/common_terms/#command-line-interface","title":"Command-line interface","text":"<ul> <li>A user interface based on typing commands, usually at a REPL.</li> <li>See also: graphical user interface</li> </ul>"},{"location":"supplementary/common_terms/#comment","title":"Comment","text":"<ul> <li>A remark in a program that is intended to help human readers understand what is going on, but is ignored by the computer.</li> <li>Comments in <code>Python</code>, <code>R</code>, and the Unix shell start with a <code>#</code> character and run to the end of the line, other languages have other conventions.</li> </ul>"},{"location":"supplementary/common_terms/#current-working-directory","title":"Current working directory","text":"<ul> <li>The directory that relative paths are calculated from.</li> <li>Equivalently, the place where files referenced by name only are searched for.</li> <li>Every process has a current working directory.</li> <li>The current working directory is usually referred to using the shorthand notation <code>.</code> (pronounced \"dot\").</li> </ul>"},{"location":"supplementary/common_terms/#file-system","title":"File system","text":"<ul> <li>A set of files, directories, and I/O devices (such as keyboards and screens).</li> <li>A file system may be spread across many physical devices, or many file systems may be stored on a single physical device; the operating system manages access.</li> </ul>"},{"location":"supplementary/common_terms/#filename-extension","title":"Filename extension","text":"<ul> <li>The portion of a file's name that comes after the final \".\" character.</li> <li>By convention this identifies the file's type.</li> <li>For example, <code>.txt</code> means \"text file\", <code>.png</code> means \"Portable Network Graphics file\"</li> <li>These conventions are not enforced by most operating systems and it is perfectly possible (but confusing!) to name an MP3 sound file <code>homepage.html</code>.</li> <li>Many applications use filename extensions to identify the MIME type of the file, so misnaming files may cause those applications to fail.</li> </ul>"},{"location":"supplementary/common_terms/#filter","title":"Filter","text":"<ul> <li>A program that transforms a stream of data.</li> <li>Many Unix command-line tools are written as filters - they read data from standard input, process it, and write the result to standard output.</li> </ul>"},{"location":"supplementary/common_terms/#flag","title":"Flag","text":"<ul> <li>A terse way to specify an option or setting to a command-line program.</li> <li>Conventions for flags vary between operating systems.</li> <li>Unix applications use a dash followed by a single letter, such as <code>-v</code>, or two dashes followed by a word, such as <code>--verbose</code>.</li> <li>DOS applications use a slash, such as <code>/V</code>.</li> <li>Depending on the application, a flag may be followed by a single argument, as in <code>-o /tmp/output.txt</code>.</li> </ul>"},{"location":"supplementary/common_terms/#for-loop","title":"For loop","text":"<ul> <li>A loop that is executed once for each value in some kind of set, list, or range.</li> <li>See also: loop, while loop</li> </ul>"},{"location":"supplementary/common_terms/#graphical-user-interface","title":"Graphical user interface","text":"<ul> <li>A user interface based on selecting items and actions from a graphical display, usually controlled by using a mouse.</li> <li>See also: command-line interface</li> </ul>"},{"location":"supplementary/common_terms/#home-directory","title":"Home directory","text":"<ul> <li>The default directory associated with an account on a computer system.</li> <li>By convention, all of a user's files are stored in or below her home directory.</li> </ul>"},{"location":"supplementary/common_terms/#loop","title":"Loop","text":"<ul> <li>A set of instructions to be executed multiple times. Consists of a loop body and (usually) a condition for exiting the loop.</li> <li>See also for loop and while loop</li> </ul>"},{"location":"supplementary/common_terms/#loop-body","title":"Loop body","text":"<ul> <li>The set of statements or commands that are repeated inside a for loop or while loop.</li> </ul>"},{"location":"supplementary/common_terms/#mime-type","title":"MIME type","text":"<p>*MIME (Multi-Purpose Internet Mail Extensions) types describe different file types for exchange on the Internet, for example images, audio, and documents.</p>"},{"location":"supplementary/common_terms/#operating-system","title":"Operating system","text":"<ul> <li>Software that manages interactions between users, hardware, and software processes. Common examples are Linux, OS X, and Windows.</li> </ul>"},{"location":"supplementary/common_terms/#parameter","title":"Parameter","text":"<ul> <li>A variable named in a function's declaration that is used to hold a value passed into the call.</li> <li>The term is often used interchangeably (and inconsistently) with argument.</li> </ul>"},{"location":"supplementary/common_terms/#parent-directory","title":"Parent directory","text":"<ul> <li>The directory that \"contains\" the one in question.</li> <li>Every directory in a file system except the root directory has a parent.</li> <li>A directory's parent is usually referred to using the shorthand notation <code>..</code> (pronounced \"dot dot\").</li> </ul>"},{"location":"supplementary/common_terms/#path","title":"Path","text":"<ul> <li>A description that specifies the location of a file or directory within a file system.</li> <li>See also: absolute path, relative path</li> </ul>"},{"location":"supplementary/common_terms/#pipe","title":"Pipe","text":"<ul> <li>A connection from the output of one program to the input of another.</li> <li>When two or more programs are connected in this way, they are called a \"pipeline\".</li> </ul>"},{"location":"supplementary/common_terms/#process","title":"Process","text":"<ul> <li>A running instance of a program, containing code, variable values, open files and network connections, and so on.</li> <li>Processes are the \"actors\" that the operating system manages.</li> <li>The operating system typically runs many process at once, allowing each to run for a few milliseconds at a time to give the impression that they are executing simultaneously.</li> </ul>"},{"location":"supplementary/common_terms/#prompt","title":"Prompt","text":"<ul> <li>A character or characters display by a REPL to show that it is waiting for its next command.</li> </ul>"},{"location":"supplementary/common_terms/#quoting","title":"Quoting","text":"<ul> <li>Using quotation marks of various kinds to prevent the shell from interpreting special characters.</li> <li>For example, to pass the string <code>*.txt</code> to a program, it is usually necessary to write it as <code>'*.txt'</code> so that the shell will not try to expand the <code>*</code> wildcard.</li> </ul>"},{"location":"supplementary/common_terms/#read-evaluate-print-loop","title":"Read-evaluate-print loop","text":"<ul> <li>(REPL): A command-line interface that reads a command from the user, executes it, prints the result, and waits for another command.</li> </ul>"},{"location":"supplementary/common_terms/#redirect","title":"Redirect","text":"<ul> <li>To send a command's output to a file rather than to the screen or another command, or equivalently to read a command's input from a file.</li> </ul>"},{"location":"supplementary/common_terms/#regular-expression","title":"Regular expression","text":"<ul> <li>A pattern that specifies a set of character strings.</li> <li>REs are most often used to find sequences of characters in strings.</li> </ul>"},{"location":"supplementary/common_terms/#relative-path","title":"Relative path","text":"<ul> <li>A path that specifies the location of a file or directory with respect to the current working directory.</li> <li>Any path that does not begin with a separator character (<code>/</code> or <code>\\\\</code>) is a relative path.</li> <li>See also: absolute path</li> </ul>"},{"location":"supplementary/common_terms/#root-directory","title":"Root directory","text":"<ul> <li>The top-most directory in a file system.</li> <li>Its name is <code>/</code> on Unix (including Linux and Mac OS X) and <code>\\\\</code> on Microsoft Windows.</li> </ul>"},{"location":"supplementary/common_terms/#shell","title":"Shell","text":"<ul> <li>A command-line interface such as Bash (the Bourne-Again Shell) or the Microsoft Windows DOS shell that allows a user to interact with the operating system.</li> </ul>"},{"location":"supplementary/common_terms/#shell-script","title":"Shell script","text":"<ul> <li>A set of shell commands stored in a file for re-use.</li> <li>A shell script is a program executed by the shell; the name \"script\" is used for historical reasons.</li> </ul>"},{"location":"supplementary/common_terms/#standard-input","title":"Standard input","text":"<ul> <li>A process's default input stream.</li> <li>In interactive command-line applications, it is typically connected to the keyboard.</li> <li>In a pipe, it receives data from the standard output of the preceding process.</li> </ul>"},{"location":"supplementary/common_terms/#standard-output","title":"Standard output","text":"<ul> <li>A process's default output stream.</li> <li>In interactive command-line applications, data sent to standard output is displayed on the screen.</li> <li>In a pipe, it is passed to the standard input of the next process.</li> </ul>"},{"location":"supplementary/common_terms/#sub-directory","title":"Sub-directory","text":"<ul> <li>A directory contained within another directory.</li> </ul>"},{"location":"supplementary/common_terms/#tab-completion","title":"Tab completion","text":"<ul> <li>A feature provided by many interactive systems in which pressing the Tab key triggers automatic completion of the current word or command.</li> </ul>"},{"location":"supplementary/common_terms/#variable","title":"Variable","text":"<ul> <li>A name in a program that is associated with a value or a collection of values.</li> </ul>"},{"location":"supplementary/common_terms/#while-loop","title":"While loop","text":"<ul> <li>A loop that keeps executing as long as some condition is true.</li> <li>See also: loop, for loop</li> </ul>"},{"location":"supplementary/common_terms/#wildcard","title":"Wildcard","text":"<ul> <li>A character used in pattern matching.</li> <li>In the Unix shell, the wildcard <code>*</code> matches zero or more characters, so that <code>*.txt</code> matches all files whose names end in <code>.txt</code>.</li> </ul>"},{"location":"supplementary/common_terms/#external-references","title":"External references","text":""},{"location":"supplementary/common_terms/#opening-a-terminal","title":"Opening a terminal","text":"<ul> <li>Using a UNIX/Linux emulator (Cygwin) or Secure Shell (SSH) client (Putty)</li> <li>Addressing the digital divide in contemporary biology: Lessons from teaching UNIX</li> <li>Unix cheat sheet</li> </ul>"},{"location":"supplementary/common_terms/#manuals","title":"Manuals","text":"<ul> <li>GNU BASH reference</li> <li>GNU manual</li> <li>Core GNU utilities</li> </ul>"},{"location":"supplementary/common_terms/#fastq-files","title":"FASTQ files","text":"<ul> <li>Phred quality score</li> <li>FASTQ format</li> </ul>"},{"location":"supplementary/fastq_format/","title":"The FASTQ file format","text":""},{"location":"supplementary/fastq_format/#the-fastq-file-format","title":"The FASTQ file format","text":"<p>In several fo the exercises in this training course we are dealing with fastq files, although we never actually discuss what they contain. This is a file format inteded to convery nucleotide sequence information, similar to a fasta file, as well as per-nucleotide quality information which is expressed as the confidence that a position in the sequence was assigned correctly by the sequencing platform. A comprehensive description of the fastq format and its variatiosn can be found in this article but for the purposes of these workshops there are only a few key points you need to know.</p> <p>Although it looks complicated, the format is actually pretty easy to understand once you know the pattern. Each fastq file is comprised of repeating blocks of 4 lines, where each line describes a single sequence read. Within these four lines:</p> Line Description 1 Is the name of the sequence, and optionally some metadata about the read.This line always begins with a <code>@</code> symbol. 2 The nucleic acid sequence. 3 A placeholder line which divides the nucleic acid sequence and quality information.In some old files this line will contain a <code>+</code> symbol followed by the sequence name.Newer tools tend to just use the <code>+</code> alone to keep the file smaller. 4 A string of characters which represent the quality scores.This string must be the same length as Line 2. <p>Using one of the example files from the training exercises (for example, Level 1, module 3), if we run the <code>head</code> command with a <code>-n</code> parameter of 4 we can view the first complete read in one of the files.</p> <pre><code>$ head -n4 SRR097977.fastq\n@SRR097977.1 209DTAAXX_Lenski2_1_7:8:3:710:178 length=36\nTATTCTGCCATAATGAAATTCGCCACTTGTTAGTGT\n+SRR097977.1 209DTAAXX_Lenski2_1_7:8:3:710:178 length=36\nCCCCCCCCCCCCCCC&gt;CCCCC7CCCCCCACA?5A5&lt;\n</code></pre> <p>Line 4 shows the quality for each nucleotide in the read. Quality is interpreted as the probability of an incorrect base call (e.g. 1 in 10) or, equivalently, the base call accuracy (e.g. 90%). These probability values are the results from the base calling algorithm and dependent on how much signal was captured for the base incorporation.  To make it possible to line up each individual nucleotide with its quality score, the numerical score is converted into a code where each individual character represents the numerical quality score for an individual nucleotide. In the line above, the quality score line is <code>CCCCCCCCCCCCCCC&gt;CCCCC7CCCCCCACA?5A5&lt;</code>.</p> <p>The numerical value assigned to each of these characters depends on the sequencing platform that generated the reads. The sequencing machine used to generate our data uses <code>PHRED+33</code> encoding, which is what is used in all modern Illumina and Nanopore sequencing, as well as Sanger sequencing. Each character is assigned a quality score between 0 and 42 as shown in the chart below.</p> <pre><code>Quality encoding: !\"#$%&amp;'()*+,-./0123456789:;&lt;=&gt;?@ABCDEFGHIJK\n                  |         |         |         |         |\nQuality score:    0........10........20........30........40..\n</code></pre> <p>If you line it up, you can see that the <code>C</code> character has a qualtiy score of 34, which is the most common value in this sequence. The probability of a position with a Q of 34 can be calculated as:</p> \\[ P_{incorrect} = 10^{\\frac{Q}{-10}} = 0.000398107 \\] <p>Which can be converted to the probabilty that the read is correct by</p> \\[ P_{correct} = 1 - P_{incorrect} = 0.9996 \\] <p>Therefore, for any of these characters the base call accuracy is 99.96%. You will almost never need to calculate the per-position score, but it is handy to know the following numbers for when we are quality filtering:</p> Q Accuracy 10 90.00% 20 99.00% 30 99.90% 40 99.99%"},{"location":"supplementary/slurm_module_guide/","title":"Slurm module guide","text":""},{"location":"supplementary/slurm_module_guide/#common-commands-slurm","title":"Common commands - slurm","text":""},{"location":"supplementary/slurm_module_guide/#submit-a-slurm-batch-job","title":"Submit a <code>slurm</code> batch job","text":"<pre><code>$ sbatch job_script.sh\n</code></pre>"},{"location":"supplementary/slurm_module_guide/#monitor-your-current-jobs","title":"Monitor your current jobs","text":"<pre><code>$ squeue -u USERNAME\n</code></pre>"},{"location":"supplementary/slurm_module_guide/#cancel-a-queued-or-running-job","title":"Cancel a queued or running job","text":"<pre><code>$ scancel JOB_ID\n</code></pre>"},{"location":"supplementary/slurm_module_guide/#view-the-status-of-completed-jobs","title":"View the status of completed jobs","text":"<pre><code># A particular job\n$ sacct -j JOB_ID\n\n# All jobs run since the date YYYY-MM-DD\n$ sacct -S YYYY-MM-DD\n\n# All jobs run before the date YYYY-MM-DD\n$ sacct -E YYYY-MM-DD\n</code></pre> <p>Note: There are many more ways to fine tune the output of <code>sacct</code>. Refer to the documentation for more detailed information.</p>"},{"location":"supplementary/slurm_module_guide/#view-the-efficiency-statistics-for-a-completed-job","title":"View the efficiency statistics for a completed job","text":"<p>Note: This command can be run for a job that is currently in progress, but the values will not be accurate for the full run (i.e. they are the values of the job to date, not the full job).</p> <pre><code>$ seff JOB_ID\n\n$ nn_seff JOB_ID\n</code></pre>"},{"location":"supplementary/slurm_module_guide/#common-commands-module","title":"Common commands - module","text":""},{"location":"supplementary/slurm_module_guide/#load-a-specific-program","title":"Load a specific program","text":"<p>Note: All modules on NeSI have version and toolchain/environment suffixes. If none is specified, the default version for the tool is loaded. The default version can be seen with the <code>module avail</code> command.</p> <pre><code>$ module load MY_TOOL\n</code></pre>"},{"location":"supplementary/slurm_module_guide/#view-available-modules","title":"View available modules","text":"<pre><code># View all modules\n$ module avail\n\n# View all modules which match the keyword in their name\n$ module avail KEYWORD\n\n# View all modules which match the keyword in their name or description\n$ module spider KEYWORD\n</code></pre>"},{"location":"supplementary/slurm_module_guide/#unload-all-current-modules","title":"Unload all current modules","text":"<pre><code>$ module purge\n</code></pre>"},{"location":"supplementary/slurm_module_guide/#swap-a-currently-loaded-module-for-a-different-one","title":"Swap a currently loaded module for a different one","text":"<pre><code>$ module switch CURRENT_MODULE DESIRED_MODULE\n</code></pre>"}]}